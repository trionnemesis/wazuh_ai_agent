import os
import logging
import traceback
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
from fastapi import FastAPI
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import uuid
import json
import re

# LangChain Áõ∏ÈóúÂ•ó‰ª∂ÂºïÂÖ•
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# OpenSearch ÂÆ¢Êà∂Á´Ø
from opensearchpy import AsyncOpenSearch, AsyncHttpConnection

# Neo4j ÂúñÂΩ¢Ë≥áÊñôÂ∫´ÂÆ¢Êà∂Á´Ø
try:
    from neo4j import AsyncGraphDatabase, AsyncDriver
    NEO4J_AVAILABLE = True
except ImportError:
    logger.warning("Neo4j driver not available. Graph persistence will be disabled.")
    NEO4J_AVAILABLE = False
    AsyncGraphDatabase = None
    AsyncDriver = None

# ÂºïÂÖ•Ëá™ÂÆöÁæ©ÁöÑÂµåÂÖ•ÊúçÂãôÊ®°ÁµÑ
from embedding_service import GeminiEmbeddingService

# ÈÖçÁΩÆÊó•Ë™åÁ≥ªÁµ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Áí∞Â¢ÉËÆäÊï∏ÈÖçÁΩÆ
OPENSEARCH_URL = os.getenv("OPENSEARCH_URL", "https://wazuh.indexer:9200")
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER", "admin")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD", "SecretPassword")

# Neo4j ÂúñÂΩ¢Ë≥áÊñôÂ∫´ÈÖçÁΩÆ
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "wazuh-graph-2024")

# Â§ßÂûãË™ûË®ÄÊ®°ÂûãÈÖçÁΩÆ
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "anthropic").lower()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# ÂàùÂßãÂåñ OpenSearch ÈùûÂêåÊ≠•ÂÆ¢Êà∂Á´Ø
client = AsyncOpenSearch(
    hosts=[OPENSEARCH_URL],
    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),
    use_ssl=True,
    verify_certs=False,
    ssl_show_warn=False,
    connection_class=AsyncHttpConnection
)

# ÂàùÂßãÂåñ Neo4j ÂúñÂΩ¢Ë≥áÊñôÂ∫´ÂÆ¢Êà∂Á´Ø
neo4j_driver = None
if NEO4J_AVAILABLE:
    try:
        neo4j_driver = AsyncGraphDatabase.driver(
            NEO4J_URI,
            auth=(NEO4J_USER, NEO4J_PASSWORD)
        )
        logger.info(f"Neo4j driver initialized: {NEO4J_URI}")
    except Exception as e:
        logger.warning(f"Failed to initialize Neo4j driver: {str(e)}")
        neo4j_driver = None
else:
    logger.warning("Neo4j driver not available - graph persistence disabled")

def get_llm():
    """
    Ê†πÊìöÁí∞Â¢ÉÈÖçÁΩÆÂàùÂßãÂåñÂ§ßÂûãË™ûË®ÄÊ®°Âûã
    
    ÊîØÊè¥ÁöÑÊèê‰æõÂïÜÔºö
    - gemini: Google Gemini 1.5 Flash Ê®°Âûã
    - anthropic: Anthropic Claude 3 Haiku Ê®°Âûã
    
    Returns:
        ChatModel: ÈÖçÁΩÆÂÆåÊàêÁöÑË™ûË®ÄÊ®°ÂûãÂØ¶‰æã
        
    Raises:
        ValueError: Áï∂Êèê‰æõÂïÜ‰∏çÊîØÊè¥Êàñ API ÈáëÈë∞Êú™Ë®≠ÂÆöÊôÇ
    """
    logger.info(f"Ê≠£Âú®ÂàùÂßãÂåñ LLM Êèê‰æõÂïÜ: {LLM_PROVIDER}")
    
    if LLM_PROVIDER == 'gemini':
        if not GEMINI_API_KEY:
            raise ValueError("LLM_PROVIDER Ë®≠ÁÇ∫ 'gemini' ‰ΩÜ GEMINI_API_KEY Êú™Ë®≠ÂÆö")
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GEMINI_API_KEY)
    
    elif LLM_PROVIDER == 'anthropic':
        if not ANTHROPIC_API_KEY:
            raise ValueError("LLM_PROVIDER Ë®≠ÁÇ∫ 'anthropic' ‰ΩÜ ANTHROPIC_API_KEY Êú™Ë®≠ÂÆö")
        return ChatAnthropic(model="claude-3-haiku-20240307", anthropic_api_key=ANTHROPIC_API_KEY)
    
    else:
        raise ValueError(f"‰∏çÊîØÊè¥ÁöÑ LLM_PROVIDER: {LLM_PROVIDER}„ÄÇË´ãÈÅ∏Êìá 'gemini' Êàñ 'anthropic'")

# ÂàùÂßãÂåñ LangChain ÁµÑ‰ª∂
llm = get_llm()

# Stage 4: GraphRAG prompt template for graph-native security analysis
graphrag_prompt_template = ChatPromptTemplate.from_template(
    """You are a senior cyber security analyst with expertise in graph-based threat hunting and advanced persistent threat (APT) analysis. Analyze the new Wazuh alert below using the comprehensive graph-native intelligence gathered from the security knowledge graph.

**üîó ÊîªÊìäË∑ØÂæëÂàÜÊûê (Attack Path Analysis):**
{attack_path_analysis}

**üîÑ Ê©´ÂêëÁßªÂãïÊ™¢Ê∏¨ (Lateral Movement Detection):**
{lateral_movement_analysis}

**‚è∞ ÊôÇÈñìÂ∫èÂàóÈóúËÅØ (Temporal Correlation):**
{temporal_correlation}

**üåç IP ‰ø°Ë≠ΩÂàÜÊûê (IP Reputation Analysis):**
{ip_reputation_analysis}

**üë§ ‰ΩøÁî®ËÄÖË°åÁÇ∫ÂàÜÊûê (User Behavior Analysis):**
{user_behavior_analysis}

**‚öôÔ∏è Á®ãÂ∫èÂü∑Ë°åÈèàÂàÜÊûê (Process Chain Analysis):**
{process_chain_analysis}

**üìÅ Ê™îÊ°à‰∫§‰∫íÂàÜÊûê (File Interaction Analysis):**
{file_interaction_analysis}

**üåê Á∂≤Ë∑ØÊãìÊí≤ÂàÜÊûê (Network Topology Analysis):**
{network_topology_analysis}

**‚ö†Ô∏è Â®ÅËÑÖÂÖ®ÊôØÂàÜÊûê (Threat Landscape Analysis):**
{threat_landscape_analysis}

**üìä ÂÇ≥Áµ±Ê™¢Á¥¢Ë£úÂÖÖ (Traditional Retrieval Supplement):**
{traditional_supplement}

**üö® Áï∂ÂâçÂàÜÊûêÁöÑÊñ∞Ë≠¶Â†±Ôºö**
{alert_summary}

**ÊÇ®ÁöÑÂúñÂΩ¢ÂåñÂ®ÅËÑÖÂàÜÊûê‰ªªÂãôÔºö**
1. **‰∫ã‰ª∂ÊëòË¶ÅËàáÂàÜÈ°ûÔºö** Á∞°Ë¶ÅÁ∏ΩÁµêÊñ∞‰∫ã‰ª∂Ôºå‰∏¶Ê†πÊìöÂúñÂΩ¢‰∏ä‰∏ãÊñáÈÄ≤Ë°åÂ®ÅËÑÖÂàÜÈ°û
2. **ÊîªÊìäÈèàÈáçÂª∫Ôºö** Âü∫ÊñºÂúñÂΩ¢ÈóúËÅØË≥áÊñôÈáçÂª∫ÂÆåÊï¥ÁöÑÊîªÊìäÊôÇÈñìÁ∑öÂíåË∑ØÂæë
3. **Ê©´ÂêëÁßªÂãïË©ï‰º∞Ôºö** Ë©ï‰º∞ÊîªÊìäËÄÖÁöÑÊ©´ÂêëÁßªÂãïËÉΩÂäõÂíåÂ∑≤Êª≤ÈÄèÁöÑÁ≥ªÁµ±ÁØÑÂúç
4. **Â®ÅËÑÖË°åÁÇ∫ËÄÖÁï´ÂÉèÔºö** Âü∫ÊñºÊîªÊìäÊ®°Âºè„ÄÅIP‰ø°Ë≠Ω„ÄÅÊôÇÈñìÊ®°ÂºèÂàÜÊûêÂ®ÅËÑÖË°åÁÇ∫ËÄÖÁâπÂæµ
5. **È¢®Èö™Á≠âÁ¥öË©ï‰º∞Ôºö** Á∂úÂêàÊâÄÊúâÂúñÂΩ¢Êô∫ËÉΩÔºåË©ï‰º∞È¢®Èö™Á≠âÁ¥öÔºàCritical, High, Medium, Low, InformationalÔºâ
6. **ÂΩ±ÈüøÁØÑÂúçÂàÜÊûêÔºö** Á¢∫ÂÆöÂèóÂΩ±ÈüøÁöÑÁ≥ªÁµ±„ÄÅ‰ΩøÁî®ËÄÖ„ÄÅÊ™îÊ°àÂíåÁ∂≤Ë∑ØË≥áÊ∫ê
7. **Á∑©Ëß£Âª∫Ë≠∞Ôºö** Êèê‰æõÂü∫ÊñºÂúñÂΩ¢ÂàÜÊûêÁöÑÁ≤æÁ¢∫Á∑©Ëß£ÂíåÊáâÊÄ•ÈüøÊáâÂª∫Ë≠∞
8. **ÊåÅÁ∫åÂ®ÅËÑÖÊåáÊ®ôÔºö** Ë≠òÂà•ÈúÄË¶ÅÊåÅÁ∫åÁõ£ÊéßÁöÑÂ®ÅËÑÖÊåáÊ®ôÔºàIOCs/IOAsÔºâ

**ÊÇ®ÁöÑ GraphRAG Â®ÅËÑÖÂàÜÊûêÂ†±ÂëäÔºö**
"""
)

# Legacy prompt template for fallback scenarios
traditional_prompt_template = ChatPromptTemplate.from_template(
    """You are a senior security analyst with expertise in correlating security events with system performance data. Analyze the new Wazuh alert below using the provided multi-source contextual information.

**Historical Similar Alerts:**
{similar_alerts_context}

**Correlated System Metrics:**
{system_metrics_context}

**Process Information:**
{process_context}

**Network Data:**
{network_context}

**Additional Context:**
{additional_context}

**ÂæÖÂàÜÊûêÁöÑÊñ∞ Wazuh Ë≠¶Â†±Ôºö**
{alert_summary}

**Your Analysis Task:**
1. Briefly summarize the new event.
2. Correlate the alert with system performance data and other contextual information.
3. Assess its risk level (Critical, High, Medium, Low, Informational) considering all available context.
4. Identify any patterns or anomalies by cross-referencing different data sources.
5. Provide actionable recommendations based on the comprehensive analysis.

**Your Comprehensive Triage Report:**
"""
)

def get_analysis_chain(context_data: Dict[str, Any]):
    """
    Ê†πÊìö‰∏ä‰∏ãÊñáË≥áÊñôÈ°ûÂûãÈÅ∏ÊìáÈÅ©Áï∂ÁöÑÂàÜÊûêÈèà
    """
    # Ê™¢Ê∏¨ÊòØÂê¶ÁÇ∫ÂúñÂΩ¢Ê™¢Á¥¢ÁµêÊûú
    graph_indicators = ['attack_paths', 'lateral_movement', 'temporal_sequences']
    has_graph_data = any(context_data.get(indicator) for indicator in graph_indicators)
    
    if has_graph_data:
        logger.info("üîó Using GraphRAG analysis chain")
        return graphrag_prompt_template | llm | StrOutputParser()
    else:
        logger.info("üìä Using traditional analysis chain")
        return traditional_prompt_template | llm | StrOutputParser()

# Remove legacy static chain - now using dynamic chain selection

# ÂàùÂßãÂåñÂµåÂÖ•ÊúçÂãô
embedding_service = GeminiEmbeddingService()

# === Stage 3: Enhanced Agentic Context Correlation Implementation ===

def determine_contextual_queries(alert: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Stage 3: Enhanced decision engine that determines what contextual information is needed
    based on the alert type and content using more sophisticated, human-like reasoning.
    
    Args:
        alert: The new alert document from OpenSearch
        
    Returns:
        List of query specifications for different types of contextual data
    """
    queries = []
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    data = alert_source.get('data', {})
    timestamp = alert_source.get('timestamp')
    
    rule_description = rule.get('description', '').lower()
    rule_groups = rule.get('groups', [])
    rule_level = rule.get('level', 0)
    host_name = agent.get('name', '')
    
    logger.info(f"ü§ñ AGENTIC DECISION ENGINE: Analyzing alert for contextual needs")
    logger.info(f"   Alert: {rule_description}")
    logger.info(f"   Level: {rule_level}, Host: {host_name}")
    logger.info(f"   Groups: {', '.join(rule_groups)}")
    
    # Default: Always perform k-NN search for similar historical alerts
    queries.append({
        'type': 'vector_similarity',
        'description': 'Similar historical alerts',
        'priority': 'high',
        'parameters': {
            'k': 7,  # Increased for better context
            'include_ai_analysis': True
        }
    })
    logger.info("‚úÖ DECISION: Adding vector similarity search (always required)")
    
    # Enhanced Resource monitoring correlation rules
    resource_keywords = [
        'high cpu usage', 'excessive ram consumption', 'memory usage', 'memory leak',
        'disk space', 'cpu utilization', 'system overload', 'performance', 
        'resource exhaustion', 'out of memory', 'cpu spike', 'high load'
    ]
    
    if any(keyword in rule_description for keyword in resource_keywords) or 'system' in rule_groups:
        logger.info("üîç DECISION: Resource-related alert detected - correlating with system data")
        
        # Process information query
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Process information from same host',
            'priority': 'high',
            'parameters': {
                'keywords': ['process list', 'top processes', 'running processes', 'ps aux', 'htop'],
                'host': host_name,
                'time_window_minutes': 10,  # Wider window for resource issues
                'timestamp': timestamp
            }
        })
        
        # Memory usage correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Memory usage metrics',
            'priority': 'medium',
            'parameters': {
                'keywords': ['memory usage', 'ram utilization', 'swap usage', 'free memory'],
                'host': host_name,
                'time_window_minutes': 15,
                'timestamp': timestamp
            }
        })
        
        logger.info("   ‚úÖ Added process and memory correlation queries")
    
    # Enhanced Security event correlation rules
    security_keywords = [
        'ssh brute-force', 'web attack', 'authentication failed', 'login attempt',
        'intrusion', 'malware', 'suspicious activity', 'unauthorized access',
        'privilege escalation', 'command injection', 'sql injection',
        'cross-site scripting', 'buffer overflow', 'trojan', 'backdoor'
    ]
    
    security_groups = ['authentication', 'attack', 'malware', 'intrusion_detection', 'web']
    
    if (any(keyword in rule_description for keyword in security_keywords) or 
        any(group in rule_groups for group in security_groups) or 
        rule_level >= 7):  # High-level alerts likely security-related
        
        logger.info("üõ°Ô∏è DECISION: Security event detected - adding comprehensive correlation")
        
        # CPU metrics for detecting resource-intensive attacks
        queries.append({
            'type': 'keyword_time_range',
            'description': 'CPU metrics during security event',
            'priority': 'high',
            'parameters': {
                'keywords': ['cpu usage', 'cpu utilization', 'processor load', 'high cpu'],
                'host': host_name,
                'time_window_minutes': 2,  # Tight window for security correlation
                'timestamp': timestamp
            }
        })
        
        # Network activity correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Network activity during security event',
            'priority': 'high',
            'parameters': {
                'keywords': ['network traffic', 'network io', 'bandwidth', 'packets', 'connections'],
                'host': host_name,
                'time_window_minutes': 3,
                'timestamp': timestamp
            }
        })
        
        # User activity correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'User activity correlation',
            'priority': 'medium',
            'parameters': {
                'keywords': ['user login', 'user activity', 'session', 'authentication'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        logger.info("   ‚úÖ Added security event correlation queries (CPU, Network, User)")
    
    # SSH-specific enhanced correlation
    if 'ssh' in rule_description or 'sshd' in rule_description:
        logger.info("üîë DECISION: SSH-related alert - adding SSH-specific correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'SSH connection patterns',
            'priority': 'high',
            'parameters': {
                'keywords': ['ssh connection', 'port 22', 'sshd', 'ssh login', 'ssh session'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        # Look for brute force patterns
        if 'brute' in rule_description or 'failed' in rule_description:
            queries.append({
                'type': 'keyword_time_range',
                'description': 'SSH failure patterns',
                'priority': 'high',
                'parameters': {
                    'keywords': ['ssh failed', 'authentication failure', 'invalid user', 'connection refused'],
                    'host': host_name,
                    'time_window_minutes': 10,  # Wider window for brute force detection
                    'timestamp': timestamp
                }
            })
            logger.info("   ‚úÖ Added SSH brute force correlation")
    
    # Web-related enhanced correlation
    web_indicators = ['web', 'http', 'apache', 'nginx', 'php', 'sql injection', 'xss']
    if any(indicator in rule_description for indicator in web_indicators):
        logger.info("üåê DECISION: Web-related alert - adding web server correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Web server performance',
            'priority': 'medium',
            'parameters': {
                'keywords': ['apache', 'nginx', 'web server', 'http requests', 'response time'],
                'host': host_name,
                'time_window_minutes': 3,
                'timestamp': timestamp
            }
        })
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Web access logs',
            'priority': 'high',
            'parameters': {
                'keywords': ['access log', 'http status', 'user agent', 'request uri'],
                'host': host_name,
                'time_window_minutes': 2,
                'timestamp': timestamp
            }
        })
        
        logger.info("   ‚úÖ Added web server correlation queries")
    
    # File system correlation for critical alerts
    if rule_level >= 10 or 'file' in rule_description:
        logger.info("üìÅ DECISION: High-level/file-related alert - adding filesystem correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'File system activity',
            'priority': 'medium',
            'parameters': {
                'keywords': ['file created', 'file modified', 'file deleted', 'disk usage', 'inode'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        logger.info("   ‚úÖ Added filesystem correlation")
    
    # Summary logging
    total_queries = len(queries)
    high_priority = len([q for q in queries if q.get('priority') == 'high'])
    logger.info(f"üéØ AGENTIC DECISION COMPLETE: Generated {total_queries} contextual queries")
    logger.info(f"   High priority: {high_priority}, Total sources: {total_queries}")
    logger.info(f"   Query types: {', '.join(set(q['type'] for q in queries))}")
    
    return queries

async def execute_retrieval(queries: List[Dict[str, Any]], alert_vector: List[float]) -> Dict[str, Any]:
    """
    Stage 3: Enhanced retrieval function that executes multiple types of queries
    and aggregates results into a structured context object.
    
    Args:
        queries: List of query specifications from determine_contextual_queries
        alert_vector: Vector representation of the current alert
        
    Returns:
        Dictionary containing aggregated results from all queries
    """
    context_data = {
        'similar_alerts': [],
        'cpu_metrics': [],
        'network_logs': [],
        'process_data': [],
        'ssh_logs': [],
        'web_metrics': [],
        'user_activity': [],
        'memory_metrics': [],
        'filesystem_data': [],
        'additional_context': []
    }
    
    logger.info(f"üîÑ EXECUTING RETRIEVAL: Processing {len(queries)} contextual queries")
    
    # Sort queries by priority for optimal execution order
    sorted_queries = sorted(queries, key=lambda x: {'high': 0, 'medium': 1, 'low': 2}.get(x.get('priority', 'medium'), 1))
    
    for i, query in enumerate(sorted_queries, 1):
        query_type = query['type']
        description = query['description']
        priority = query.get('priority', 'medium')
        parameters = query['parameters']
        
        try:
            logger.info(f"   [{i}/{len(queries)}] üîç {priority.upper()}: {description}")
            
            if query_type == 'vector_similarity':
                # K-NN vector search for similar alerts
                results = await execute_vector_search(alert_vector, parameters)
                context_data['similar_alerts'].extend(results)
                logger.info(f"      ‚úÖ Found {len(results)} similar alerts")
                
            elif query_type == 'keyword_time_range':
                # Keyword and time-based search
                results = await execute_keyword_time_search(parameters)
                
                # Enhanced categorization based on description
                if 'cpu' in description.lower():
                    context_data['cpu_metrics'].extend(results)
                elif 'network' in description.lower():
                    context_data['network_logs'].extend(results)
                elif 'process' in description.lower():
                    context_data['process_data'].extend(results)
                elif 'ssh' in description.lower():
                    context_data['ssh_logs'].extend(results)
                elif 'web' in description.lower():
                    context_data['web_metrics'].extend(results)
                elif 'user' in description.lower():
                    context_data['user_activity'].extend(results)
                elif 'memory' in description.lower():
                    context_data['memory_metrics'].extend(results)
                elif 'file' in description.lower():
                    context_data['filesystem_data'].extend(results)
                else:
                    context_data['additional_context'].extend(results)
                
                logger.info(f"      ‚úÖ Found {len(results)} contextual records")
                    
        except Exception as e:
            logger.error(f"      ‚ùå Query failed: {str(e)}")
            continue
    
    # Enhanced retrieval summary
    total_results = sum(len(results) for results in context_data.values())
    logger.info(f"üìä RETRIEVAL SUMMARY: {total_results} total contextual records")
    for category, results in context_data.items():
        if results:
            logger.info(f"   {category}: {len(results)} records")
    
    return context_data

async def execute_vector_search(alert_vector: List[float], parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Execute k-NN vector similarity search for historical alerts.
    
    Args:
        alert_vector: Vector representation of the current alert
        parameters: Search parameters including k and filters
        
    Returns:
        List of similar alert documents
    """
    try:
        k = parameters.get('k', 5)
        include_ai_analysis = parameters.get('include_ai_analysis', True)
        
        # Build k-NN search query
        knn_search_body = {
            "size": k,
            "query": {
                "bool": {
                    "must": [
                        {
                            "knn": {
                                "alert_vector": {
                                    "vector": alert_vector,
                                    "k": k
                                }
                            }
                        }
                    ]
                }
            },
            "_source": ["rule", "agent", "ai_analysis", "timestamp", "data"]
        }
        
        # Add filter for alerts with AI analysis if requested
        if include_ai_analysis:
            if "filter" not in knn_search_body["query"]["bool"]:
                knn_search_body["query"]["bool"]["filter"] = []
            knn_search_body["query"]["bool"]["filter"].append(
                {"exists": {"field": "ai_analysis"}}
            )
        
        response = await client.search(
            index="wazuh-alerts-*",
            body=knn_search_body
        )
        
        similar_alerts = response.get('hits', {}).get('hits', [])
        return similar_alerts
        
    except Exception as e:
        logger.error(f"Vector search failed: {str(e)}")
        return []

async def execute_keyword_time_search(parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Enhanced keyword and time-range search for system metrics and logs.
    
    Args:
        parameters: Search parameters including keywords, host, and time window
        
    Returns:
        List of matching documents
    """
    try:
        keywords = parameters.get('keywords', [])
        host = parameters.get('host', '')
        time_window_minutes = parameters.get('time_window_minutes', 5)
        timestamp = parameters.get('timestamp')
        
        if not timestamp:
            logger.warning("No timestamp provided for time-range search")
            return []
        
        # Parse timestamp and calculate time range
        if isinstance(timestamp, str):
            alert_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        else:
            alert_time = datetime.utcnow()
        
        start_time = alert_time - timedelta(minutes=time_window_minutes)
        end_time = alert_time + timedelta(minutes=time_window_minutes)
        
        # Build enhanced keyword and time-range query
        search_body = {
            "size": 15,  # Increased for better context
            "query": {
                "bool": {
                    "should": [  # Using should for better matching flexibility
                        {
                            "multi_match": {
                                "query": " ".join(keywords),
                                "fields": ["rule.description^2", "data.*", "full_log", "location"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        {
                            "terms": {
                                "rule.description.keyword": keywords
                            }
                        }
                    ],
                    "filter": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_time.isoformat(),
                                    "lte": end_time.isoformat()
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "sort": [
                {"timestamp": {"order": "desc"}},
                {"_score": {"order": "desc"}}
            ]
        }
        
        # Add host filter if specified
        if host:
            search_body["query"]["bool"]["filter"].append({
                "term": {"agent.name.keyword": host}
            })
        
        response = await client.search(
            index="wazuh-alerts-*",
            body=search_body
        )
        
        results = response.get('hits', {}).get('hits', [])
        return results
        
    except Exception as e:
        logger.error(f"Keyword/time search failed: {str(e)}")
        return []

def format_multi_source_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    Stage 3: Enhanced formatting of multi-source context data for LLM consumption.
    
    Args:
        context_data: Aggregated context from execute_retrieval
        
    Returns:
        Dictionary with formatted context strings for each category
    """
    formatted_context = {}
    
    # Format similar alerts with enhanced details
    similar_alerts = context_data.get('similar_alerts', [])
    if similar_alerts:
        context_parts = []
        for i, alert in enumerate(similar_alerts, 1):
            source = alert.get('_source', {})
            rule = source.get('rule', {})
            agent = source.get('agent', {})
            ai_analysis = source.get('ai_analysis', {})
            
            # Extract risk level from previous analysis if available
            prev_analysis = ai_analysis.get('triage_report', '')
            risk_level = "Unknown"
            for level in ['Critical', 'High', 'Medium', 'Low']:
                if level.lower() in prev_analysis.lower():
                    risk_level = level
                    break
            
            context_part = f"""
{i}. **Alert:** {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')})
   **Host:** {agent.get('name', 'Unknown')} | **Time:** {source.get('timestamp', 'Unknown')}
   **Previous Risk Assessment:** {risk_level}
   **Similarity Score:** {alert.get('_score', 'N/A'):.3f}
   **Analysis Preview:** {prev_analysis[:200]}..."""
            context_parts.append(context_part)
        formatted_context['similar_alerts_context'] = "\n".join(context_parts)
    else:
        formatted_context['similar_alerts_context'] = "No similar historical alerts found."
    
    # Format system metrics with enhanced correlation info
    cpu_metrics = context_data.get('cpu_metrics', [])
    memory_metrics = context_data.get('memory_metrics', [])
    all_system_metrics = cpu_metrics + memory_metrics
    
    if all_system_metrics:
        metric_parts = []
        for metric in all_system_metrics[:10]:  # Limit for readability
            source = metric.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            metric_parts.append(f"- {timestamp}: {rule.get('description', 'System metric')}")
        formatted_context['system_metrics_context'] = "\n".join(metric_parts)
    else:
        formatted_context['system_metrics_context'] = "No correlated system metrics found."
    
    # Format process data
    process_data = context_data.get('process_data', [])
    if process_data:
        process_parts = []
        for proc in process_data[:8]:
            source = proc.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            process_parts.append(f"- {timestamp}: {rule.get('description', 'Process info')}")
        formatted_context['process_context'] = "\n".join(process_parts)
    else:
        formatted_context['process_context'] = "No process information found."
    
    # Format network data with enhanced details
    network_logs = context_data.get('network_logs', [])
    ssh_logs = context_data.get('ssh_logs', [])
    all_network_data = network_logs + ssh_logs
    
    if all_network_data:
        network_parts = []
        for net in all_network_data[:10]:
            source = net.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            data = source.get('data', {})
            
            # Extract relevant network details
            details = []
            if data.get('srcip'):
                details.append(f"SRC:{data['srcip']}")
            if data.get('dstip'):
                details.append(f"DST:{data['dstip']}")
            if data.get('srcport'):
                details.append(f"PORT:{data['srcport']}")
            
            detail_str = f" ({', '.join(details)})" if details else ""
            network_parts.append(f"- {timestamp}: {rule.get('description', 'Network activity')}{detail_str}")
        formatted_context['network_context'] = "\n".join(network_parts)
    else:
        formatted_context['network_context'] = "No correlated network data found."
    
    # Format additional context from various sources
    additional_sources = []
    for category in ['web_metrics', 'user_activity', 'filesystem_data', 'additional_context']:
        category_data = context_data.get(category, [])
        if category_data:
            additional_sources.extend(category_data[:5])  # Limit each category
    
    if additional_sources:
        additional_parts = []
        for item in additional_sources:
            source = item.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            additional_parts.append(f"- {timestamp}: {rule.get('description', 'Additional context')}")
        formatted_context['additional_context'] = "\n".join(additional_parts)
    else:
        formatted_context['additional_context'] = "No additional contextual data found."
    
    return formatted_context

async def query_new_alerts(limit: int = 10) -> List[Dict[str, Any]]:
    """Query OpenSearch for new unanalyzed alerts"""
    try:
        response = await client.search(
            index="wazuh-alerts-*",
            body={
                "query": {
                    "bool": {
                        "must_not": [{"exists": {"field": "ai_analysis"}}]
                    }
                },
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
        )
        
        alerts = response.get('hits', {}).get('hits', [])
        logger.info(f"Found {len(alerts)} new alerts to process")
        return alerts
        
    except Exception as e:
        logger.error(f"Failed to query new alerts: {str(e)}")
        raise

async def process_single_alert(alert: Dict[str, Any]) -> None:
    """
    Stage 3: Enhanced single alert processing with agentic context correlation.
    
    Processing workflow:
    1. Fetch new alert
    2. Vectorize alert
    3. Decide: Call determine_contextual_queries to get required contextual queries
    4. Retrieve: Call execute_retrieval with query list to fetch all required data
    5. Format: Update context formatting to handle multi-source context
    6. Analyze: Send comprehensive context to LLM
    7. Update: Store results
    8. Graph Persistence: Extract entities and build relationships in graph database (NEW)
    """
    alert_id = alert['_id']
    alert_index = alert['_index']
    alert_source = alert['_source']
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    
    # Step 1: Prepare alert summary
    alert_summary = f"Rule: {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')}) on Host: {agent.get('name', 'N/A')}"
    logger.info(f"Processing alert {alert_id}: {alert_summary}")

    try:
        # Step 2: Vectorize new alert
        logger.info(f"üîÆ STEP 2: Vectorizing alert {alert_id}")
        alert_vector = await embedding_service.embed_alert_content(alert_source)
        logger.info(f"   ‚úÖ Alert vectorized (dimension: {len(alert_vector)})")
        
        # Step 3: Decide - Determine graph queries for GraphRAG
        logger.info(f"üîó STEP 3: GRAPH-NATIVE DECISION - Determining Cypher queries for alert {alert_id}")
        graph_queries = determine_graph_queries(alert)
        
        # Step 4: Execute Graph-Native Retrieval
        logger.info(f"üìä STEP 4: GRAPH-NATIVE RETRIEVAL - Executing {len(graph_queries)} Cypher queries for alert {alert_id}")
        context_data = await execute_hybrid_retrieval(alert)
        
        # Step 5: Format - Prepare graph-native context for LLM
        logger.info(f"üìã STEP 5: GRAPH CONTEXT FORMATTING - Preparing graph-native context for alert {alert_id}")
        formatted_context = format_hybrid_context(context_data)
        
        # Log context summary for verification
        total_context_items = sum(len(ctx.split('\n')) for ctx in formatted_context.values() if ctx and "No " not in ctx)
        logger.info(f"   üìä Context summary: {total_context_items} total contextual items prepared")
        
        # Step 6: Analyze - Send comprehensive context to LLM using appropriate chain
        logger.info(f"ü§ñ STEP 6: GRAPHRAG ANALYSIS - Generating graph-native AI analysis for alert {alert_id}")
        analysis_chain = get_analysis_chain(context_data)
        analysis_result = await analysis_chain.ainvoke({
            "alert_summary": alert_summary,
            **formatted_context
        })
        
        # Extract risk level for logging
        risk_level = "Unknown"
        for level in ['Critical', 'High', 'Medium', 'Low', 'Informational']:
            if level.lower() in analysis_result.lower():
                risk_level = level
                break
        
        logger.info(f"   ‚úÖ AI Analysis generated (Risk: {risk_level}): {analysis_result[:150]}...")
        
        # Step 7: Update - Store results in OpenSearch
        logger.info(f"üíæ STEP 7: STORING RESULTS - Updating alert {alert_id} with GraphRAG analysis")
        
        # Enhanced metadata for Stage 4 GraphRAG
        context_metadata = {
            # Graph-native metrics
            "attack_paths_count": len(context_data.get('attack_paths', [])),
            "lateral_movement_count": len(context_data.get('lateral_movement', [])),
            "temporal_sequences_count": len(context_data.get('temporal_sequences', [])),
            "ip_reputation_count": len(context_data.get('ip_reputation', [])),
            "user_behavior_count": len(context_data.get('user_behavior', [])),
            "process_chains_count": len(context_data.get('process_chains', [])),
            "file_interactions_count": len(context_data.get('file_interactions', [])),
            "network_topology_count": len(context_data.get('network_topology', [])),
            "threat_landscape_count": len(context_data.get('threat_landscape', [])),
            "correlation_graph_count": len(context_data.get('correlation_graph', [])),
            
            # Traditional supplement metrics (when used)
            "traditional_similar_alerts_count": len(context_data.get('traditional_similar_alerts', [])),
            "traditional_metrics_count": len(context_data.get('traditional_metrics', [])),
            "traditional_logs_count": len(context_data.get('traditional_logs', [])),
            
            # Legacy compatibility
            "similar_alerts_count": len(context_data.get('similar_alerts', [])),
            "cpu_metrics_count": len(context_data.get('cpu_metrics', [])),
            "memory_metrics_count": len(context_data.get('memory_metrics', [])),
            "network_logs_count": len(context_data.get('network_logs', [])),
            "ssh_logs_count": len(context_data.get('ssh_logs', [])),
            "process_data_count": len(context_data.get('process_data', [])),
            "web_metrics_count": len(context_data.get('web_metrics', [])),
            "user_activity_count": len(context_data.get('user_activity', [])),
            "filesystem_data_count": len(context_data.get('filesystem_data', [])),
            "additional_context_count": len(context_data.get('additional_context', []))
        }
        
        update_body = {
            "doc": {
                "ai_analysis": {
                    "triage_report": analysis_result,
                    "provider": LLM_PROVIDER,
                    "timestamp": alert_source.get('timestamp'),
                    "context_sources": len(graph_queries),
                    "extracted_risk_level": risk_level,
                    "stage": "Stage 4 - GraphRAG Analysis",
                    "analysis_method": "Graph-Native Retrieval" if any(context_data.get(k) for k in ['attack_paths', 'lateral_movement']) else "Hybrid Retrieval",
                    **context_metadata
                },
                "alert_vector": alert_vector
            }
        }
        
        await client.update(index=alert_index, id=alert_id, body=update_body)
        
        logger.info(f"üéâ GRAPHRAG PROCESSING COMPLETE: Alert {alert_id} successfully updated")
        logger.info(f"   üìà Graph-native correlation metadata stored for future analysis")
        
        # Step 8: Graph Persistence - Extract entities and build relationships (NEW)
        logger.info(f"üîó STEP 8: GRAPH PERSISTENCE - Building knowledge graph for alert {alert_id}")
        
        try:
            # Extract graph entities from alert and context
            graph_entities = await extract_graph_entities(alert, context_data, analysis_result)
            logger.info(f"   üîç Extracted {len(graph_entities)} entities for graph database")
            
            # Build relationships between entities
            graph_relationships = await build_graph_relationships(graph_entities, alert, context_data)
            logger.info(f"   üîó Built {len(graph_relationships)} relationships for graph database")
            
            # Persist to graph database (Neo4j)
            graph_persistence_result = await persist_to_graph_database(graph_entities, graph_relationships, alert_id)
            
            if graph_persistence_result['success']:
                logger.info(f"   ‚úÖ Graph persistence successful: {graph_persistence_result['nodes_created']} nodes, {graph_persistence_result['relationships_created']} relationships")
                
                # Update alert with graph metadata
                graph_metadata = {
                    "graph_entities_count": len(graph_entities),
                    "graph_relationships_count": len(graph_relationships),
                    "graph_persistence_timestamp": graph_persistence_result['timestamp'],
                    "graph_node_ids": graph_persistence_result.get('node_ids', [])
                }
                
                # Add graph metadata to the alert
                graph_update_body = {
                    "doc": {
                        "ai_analysis.graph_metadata": graph_metadata
                    }
                }
                
                await client.update(index=alert_index, id=alert_id, body=graph_update_body)
                logger.info(f"   üìä Graph metadata added to alert {alert_id}")
                
            else:
                logger.warning(f"   ‚ö†Ô∏è Graph persistence failed: {graph_persistence_result.get('error', 'Unknown error')}")
                
        except Exception as graph_error:
            logger.error(f"   ‚ùå Graph persistence error for alert {alert_id}: {str(graph_error)}")
            # Graph persistence failure should not break the main pipeline
            logger.info(f"   üîÑ Main processing pipeline continues despite graph persistence failure")
        
    except Exception as e:
        logger.error(f"‚ùå PROCESSING FAILED for alert {alert_id}: {str(e)}")
        logger.error(f"   Stack trace: {traceback.format_exc()}")
        raise

async def triage_new_alerts():
    """Main alert triage task with Stage 3 agentic context correlation"""
    print("üöÄ === STAGE 3 AGENTIC CONTEXT CORRELATION TRIAGE JOB EXECUTING ===")
    logger.info(f"üî¨ Analyzing alerts with {LLM_PROVIDER} model and enhanced agentic context correlation...")
    
    try:
        # Query new alerts
        alerts = await query_new_alerts(limit=10)
        
        if not alerts:
            print("üì≠ --- No new alerts found ---")
            logger.info("No new alerts requiring agentic analysis")
            return
            
        logger.info(f"üéØ Found {len(alerts)} new alerts to process with agentic context correlation")
        
        # Process each alert with enhanced agentic workflow
        successful_processing = 0
        failed_processing = 0
        
        for i, alert in enumerate(alerts, 1):
            alert_id = alert['_id']
            rule_desc = alert.get('_source', {}).get('rule', {}).get('description', 'Unknown')
            
            try:
                logger.info(f"üîÑ [{i}/{len(alerts)}] Processing alert: {alert_id}")
                logger.info(f"   Rule: {rule_desc}")
                
                await process_single_alert(alert)
                
                successful_processing += 1
                print(f"‚úÖ [{i}/{len(alerts)}] Successfully processed alert {alert_id}")
                logger.info(f"‚úÖ Alert {alert_id} processing completed successfully")
                
            except Exception as e:
                failed_processing += 1
                print(f"‚ùå [{i}/{len(alerts)}] Failed to process alert {alert_id}: {str(e)}")
                logger.error(f"‚ùå Alert {alert_id} processing failed: {str(e)}")
                continue
        
        # Summary logging
        print(f"üìä === AGENTIC TRIAGE BATCH SUMMARY ===")
        print(f"   ‚úÖ Successful: {successful_processing}")
        print(f"   ‚ùå Failed: {failed_processing}")
        print(f"   üìà Success Rate: {(successful_processing/len(alerts)*100):.1f}%")
        
        logger.info(f"üéØ Agentic triage batch completed: {successful_processing}/{len(alerts)} successful")
            
    except Exception as e:
        print(f"üí• !!! CRITICAL ERROR IN AGENTIC TRIAGE JOB !!!")
        logger.error(f"Critical error during agentic triage: {e}", exc_info=True)
        traceback.print_exc()

# === FastAPI ÊáâÁî®Á®ãÂºèËàáÊéíÁ®ãÂô® ===

app = FastAPI(title="Wazuh AI Triage Agent - Stage 3 Agentic Context Correlation")

scheduler = AsyncIOScheduler()

@app.on_event("startup")
async def startup_event():
    logging.info("AI Agent with Stage 3 Agentic Context Correlation starting up...")
    scheduler.add_job(triage_new_alerts, 'interval', seconds=60, id='agentic_triage_job', misfire_grace_time=30)
    scheduler.start()
    logging.info("Scheduler started. Agentic Context Correlation Triage job scheduled.")

@app.get("/")
def read_root():
    """Ê†πÁ´ØÈªû - ËøîÂõûÊúçÂãôÁãÄÊÖãË≥áË®ä"""
    return {
        "status": "AI Triage Agent with Agentic Context Correlation is running", 
        "scheduler_status": str(scheduler.get_jobs()),
        "stage": "Stage 3 - Agentic Context Correlation",
        "features": [
            "Dynamic contextual query generation",
            "Multi-source data retrieval",
            "Cross-referential analysis",
            "Enhanced decision engine"
        ]
    }

@app.get("/health")
async def health_check():
    """
    Ë©≥Á¥∞ÂÅ•Â∫∑Ê™¢Êü•Á´ØÈªû
    
    Êèê‰æõÂÆåÊï¥ÁöÑÁ≥ªÁµ±ÁãÄÊÖãË≥áË®äÔºåÂåÖÊã¨Ôºö
    - OpenSearch ÈÄ£Á∑öÁãÄÊÖã
    - ÂµåÂÖ•ÊúçÂãôÂèØÁî®ÊÄß
    - ÂêëÈáèÂåñÁµ±Ë®àË≥áÊñô
    - Á≥ªÁµ±ÈÖçÁΩÆË≥áË®ä
    
    Returns:
        Dict: Ë©≥Á¥∞ÁöÑÂÅ•Â∫∑Ê™¢Êü•Â†±Âëä
    """
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "3.0", # Updated version
        "stage": "Stage 3 - Agentic Context Correlation" # Updated stage
    }
    
    try:
        # Ê™¢Êü• OpenSearch ÈÄ£Á∑öÁãÄÊÖã
        cluster_health = await client.cluster.health()
        health_status["opensearch"] = {
            "status": "connected",
            "cluster_name": cluster_health.get("cluster_name", "unknown"),
            "cluster_status": cluster_health.get("status", "unknown"),
            "number_of_nodes": cluster_health.get("number_of_nodes", 0)
        }
        
        # Ê™¢Êü•ÂµåÂÖ•ÊúçÂãôÁãÄÊÖã
        embedding_test = await embedding_service.test_connection()
        health_status["embedding_service"] = {
            "status": "working" if embedding_test else "failed",
            "model": embedding_service.model_name,
            "dimension": embedding_service.get_vector_dimension()
        }
        
        # Ê™¢Êü•ÂêëÈáèÂåñË≠¶Â†±Áµ±Ë®à
        vectorized_count_response = await client.count(
            index="wazuh-alerts-*",
            body={"query": {"exists": {"field": "alert_vector"}}}
        )
        
        # Ê™¢Êü• Stage 3 ÂàÜÊûêÁµ±Ë®à
        stage3_analysis_response = await client.count(
            index="wazuh-alerts-*",
            body={"query": {"bool": {"must": [
                {"exists": {"field": "ai_analysis"}},
                {"term": {"ai_analysis.stage.keyword": "Stage 3 - Agentic Context Correlation"}}
            ]}}}
        )
        
        total_alerts_response = await client.count(index="wazuh-alerts-*")
        
        health_status["processing_stats"] = {
            "vectorized_alerts": vectorized_count_response.get("count", 0),
            "stage3_analyzed_alerts": stage3_analysis_response.get("count", 0),
            "total_alerts": total_alerts_response.get("count", 0),
            "vectorization_rate": round(
                (vectorized_count_response.get("count", 0) / max(total_alerts_response.get("count", 1), 1)) * 100, 2
            ),
            "stage3_analysis_rate": round(
                (stage3_analysis_response.get("count", 0) / max(total_alerts_response.get("count", 1), 1)) * 100, 2
            )
        }
        
        # LLM ÈÖçÁΩÆË≥áË®ä
        health_status["llm_config"] = {
            "provider": LLM_PROVIDER,
            "model_configured": True
        }
        
        # ÊéíÁ®ãÂô®ÁãÄÊÖã
        jobs = scheduler.get_jobs()
        health_status["scheduler"] = {
            "status": "running" if jobs else "no_jobs",
            "active_jobs": len(jobs),
            "next_run": str(jobs[0].next_run_time) if jobs else None
        }
        
    except Exception as e:
        health_status["status"] = "unhealthy"
        health_status["error"] = str(e)
        logger.error(f"ÂÅ•Â∫∑Ê™¢Êü•Â§±Êïó: {str(e)}")
    
    return health_status

# ==================== ÂúñÂΩ¢ÂåñÊåÅ‰πÖÂ±§ÂáΩÊï∏ (GraphRAG Stage 4 Ê∫ñÂÇô) ====================

async def extract_graph_entities(alert: Dict[str, Any], context_data: Dict[str, Any], analysis_result: str) -> List[Dict[str, Any]]:
    """
    ÂæûË≠¶Â†±„ÄÅ‰∏ä‰∏ãÊñáË≥áÊñôÂíåÂàÜÊûêÁµêÊûú‰∏≠ÊèêÂèñÂúñÂΩ¢ÂØ¶È´î
    
    Args:
        alert: ÂéüÂßãË≠¶Â†±Ë≥áÊñô
        context_data: ‰∏ä‰∏ãÊñáÈóúËÅØË≥áÊñô
        analysis_result: LLM ÂàÜÊûêÁµêÊûú
    
    Returns:
        ÊèêÂèñÁöÑÂúñÂΩ¢ÂØ¶È´îÂàóË°®
    """
    entities = []
    alert_source = alert.get('_source', {})
    
    # 1. Ë≠¶Â†±ÂØ¶È´î (Alert Entity)
    alert_entity = {
        'type': 'Alert',
        'id': alert['_id'],
        'properties': {
            'alert_id': alert['_id'],
            'timestamp': alert_source.get('timestamp'),
            'rule_id': alert_source.get('rule', {}).get('id'),
            'rule_description': alert_source.get('rule', {}).get('description'),
            'rule_level': alert_source.get('rule', {}).get('level'),
            'rule_groups': alert_source.get('rule', {}).get('groups', []),
            'risk_level': _extract_risk_level_from_analysis(analysis_result),
            'triage_score': _calculate_triage_score(alert_source, analysis_result)
        }
    }
    entities.append(alert_entity)
    
    # 2. ‰∏ªÊ©üÂØ¶È´î (Host Entity)
    agent = alert_source.get('agent', {})
    if agent.get('id') or agent.get('name'):
        host_entity = {
            'type': 'Host',
            'id': f"host_{agent.get('id', agent.get('name', 'unknown'))}",
            'properties': {
                'agent_id': agent.get('id'),
                'agent_name': agent.get('name'),
                'agent_ip': agent.get('ip'),
                'operating_system': _extract_os_info(alert_source)
            }
        }
        entities.append(host_entity)
    
    # 3. IP ‰ΩçÂùÄÂØ¶È´î (IP Address Entities)
    ip_addresses = _extract_ip_addresses(alert_source)
    for ip_info in ip_addresses:
        ip_entity = {
            'type': 'IPAddress',
            'id': f"ip_{ip_info['address']}",
            'properties': {
                'address': ip_info['address'],
                'type': ip_info['type'],  # source, destination, internal
                'geolocation': ip_info.get('geo'),
                'is_private': _is_private_ip(ip_info['address'])
            }
        }
        entities.append(ip_entity)
    
    # 4. ‰ΩøÁî®ËÄÖÂØ¶È´î (User Entities)
    users = _extract_user_info(alert_source)
    for user_info in users:
        user_entity = {
            'type': 'User',
            'id': f"user_{user_info['name']}",
            'properties': {
                'username': user_info['name'],
                'user_type': user_info.get('type', 'unknown'),
                'authentication_method': user_info.get('auth_method')
            }
        }
        entities.append(user_entity)
    
    # 5. Á®ãÂ∫èÂØ¶È´î (Process Entities)
    processes = _extract_process_info(alert_source, context_data)
    for process_info in processes:
        process_entity = {
            'type': 'Process',
            'id': f"process_{process_info.get('pid', 'unknown')}_{process_info.get('name', 'unknown')}",
            'properties': {
                'process_name': process_info.get('name'),
                'process_id': process_info.get('pid'),
                'command_line': process_info.get('cmdline'),
                'parent_process': process_info.get('ppid'),
                'hash': process_info.get('hash')
            }
        }
        entities.append(process_entity)
    
    # 6. Ê™îÊ°àÂØ¶È´î (File Entities)
    files = _extract_file_info(alert_source)
    for file_info in files:
        file_entity = {
            'type': 'File',
            'id': f"file_{hash(file_info['path'])}",
            'properties': {
                'file_path': file_info['path'],
                'file_name': file_info.get('name'),
                'file_size': file_info.get('size'),
                'file_hash': file_info.get('hash'),
                'file_permissions': file_info.get('permissions')
            }
        }
        entities.append(file_entity)
    
    # 7. Â®ÅËÑÖÂØ¶È´î (ÂæûÂàÜÊûêÁµêÊûúÊèêÂèñ)
    threat_indicators = _extract_threat_indicators(analysis_result)
    for threat in threat_indicators:
        threat_entity = {
            'type': 'ThreatIndicator',
            'id': f"threat_{uuid.uuid4().hex[:8]}",
            'properties': {
                'indicator_type': threat['type'],
                'indicator_value': threat['value'],
                'confidence': threat.get('confidence', 0.5),
                'mitre_technique': threat.get('mitre_technique')
            }
        }
        entities.append(threat_entity)
    
    logger.info(f"Extracted {len(entities)} entities: {dict(zip(*zip(*[(e['type'], 1) for e in entities])))}")
    return entities

async def build_graph_relationships(entities: List[Dict[str, Any]], alert: Dict[str, Any], context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Ê†πÊìöÂØ¶È´îÂíå‰∏ä‰∏ãÊñáË≥áÊñôÂª∫Á´ãÂúñÂΩ¢Èóú‰øÇ
    
    Args:
        entities: ÊèêÂèñÁöÑÂØ¶È´îÂàóË°®
        alert: ÂéüÂßãË≠¶Â†±Ë≥áÊñô
        context_data: ‰∏ä‰∏ãÊñáÈóúËÅØË≥áÊñô
    
    Returns:
        ÂØ¶È´îÈñìÁöÑÈóú‰øÇÂàóË°®
    """
    relationships = []
    entity_by_id = {entity['id']: entity for entity in entities}
    entity_by_type = {}
    
    # ÊåâÈ°ûÂûãÁµÑÁπîÂØ¶È´î
    for entity in entities:
        entity_type = entity['type']
        if entity_type not in entity_by_type:
            entity_by_type[entity_type] = []
        entity_by_type[entity_type].append(entity)
    
    # 1. Ë≠¶Â†±Ëß∏ÁôºÈóú‰øÇ (Alert -> Host)
    alert_entities = entity_by_type.get('Alert', [])
    host_entities = entity_by_type.get('Host', [])
    
    for alert_entity in alert_entities:
        for host_entity in host_entities:
            relationship = {
                'type': 'TRIGGERED_ON',
                'source_id': alert_entity['id'],
                'target_id': host_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'severity': alert.get('_source', {}).get('rule', {}).get('level')
                }
            }
            relationships.append(relationship)
    
    # 2. ‰æÜÊ∫ê IP Èóú‰øÇ (Alert -> IPAddress)
    ip_entities = entity_by_type.get('IPAddress', [])
    for alert_entity in alert_entities:
        for ip_entity in ip_entities:
            if ip_entity['properties'].get('type') == 'source':
                relationship = {
                    'type': 'HAS_SOURCE_IP',
                    'source_id': alert_entity['id'],
                    'target_id': ip_entity['id'],
                    'properties': {
                        'timestamp': alert.get('_source', {}).get('timestamp')
                    }
                }
                relationships.append(relationship)
    
    # 3. ‰ΩøÁî®ËÄÖÂèÉËàáÈóú‰øÇ (Alert -> User)
    user_entities = entity_by_type.get('User', [])
    for alert_entity in alert_entities:
        for user_entity in user_entities:
            relationship = {
                'type': 'INVOLVES_USER',
                'source_id': alert_entity['id'],
                'target_id': user_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'action_type': _determine_user_action_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 4. Á®ãÂ∫èÂü∑Ë°åÈóú‰øÇ (Alert -> Process)
    process_entities = entity_by_type.get('Process', [])
    for alert_entity in alert_entities:
        for process_entity in process_entities:
            relationship = {
                'type': 'INVOLVES_PROCESS',
                'source_id': alert_entity['id'],
                'target_id': process_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp')
                }
            }
            relationships.append(relationship)
    
    # 5. Ê™îÊ°àÂ≠òÂèñÈóú‰øÇ (Alert -> File)
    file_entities = entity_by_type.get('File', [])
    for alert_entity in alert_entities:
        for file_entity in file_entities:
            relationship = {
                'type': 'ACCESSES_FILE',
                'source_id': alert_entity['id'],
                'target_id': file_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'access_type': _determine_file_access_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 6. È°û‰ººË≠¶Â†±Èóú‰øÇ (Âü∫Êñº‰∏ä‰∏ãÊñáË≥áÊñô)
    similar_alerts = context_data.get('similar_alerts', [])
    for similar_alert in similar_alerts[:5]:  # ÈôêÂà∂Èóú‰øÇÊï∏Èáè
        similar_alert_id = similar_alert.get('_id')
        if similar_alert_id:
            for alert_entity in alert_entities:
                relationship = {
                    'type': 'SIMILAR_TO',
                    'source_id': alert_entity['id'],
                    'target_id': f"alert_{similar_alert_id}",  # ÂÅáË®≠Ë©≤Ë≠¶Â†±Â∑≤Âú®Âúñ‰∏≠
                    'properties': {
                        'similarity_score': similar_alert.get('_score', 0.0),
                        'correlation_type': 'vector_similarity'
                    }
                }
                relationships.append(relationship)
    
    # 7. ÊôÇÈñìÂ∫èÂàóÈóú‰øÇ (Temporal Relationships)
    # Ê†πÊìöÊôÇÈñìÊà≥Âª∫Á´ã PRECEDES Èóú‰øÇ
    if len(alert_entities) > 1:
        sorted_alerts = sorted(alert_entities, key=lambda x: x['properties'].get('timestamp', ''))
        for i in range(len(sorted_alerts) - 1):
            relationship = {
                'type': 'PRECEDES',
                'source_id': sorted_alerts[i]['id'],
                'target_id': sorted_alerts[i + 1]['id'],
                'properties': {
                    'time_difference': _calculate_time_difference(
                        sorted_alerts[i]['properties'].get('timestamp'),
                        sorted_alerts[i + 1]['properties'].get('timestamp')
                    )
                }
            }
            relationships.append(relationship)
    
    logger.info(f"Built {len(relationships)} relationships")
    return relationships

async def persist_to_graph_database(entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]], alert_id: str) -> Dict[str, Any]:
    """
    Â∞áÂØ¶È´îÂíåÈóú‰øÇÊåÅ‰πÖÂåñÂà∞ Neo4j ÂúñÂΩ¢Ë≥áÊñôÂ∫´
    
    Args:
        entities: Ë¶ÅÂ≠òÂÑ≤ÁöÑÂØ¶È´îÂàóË°®
        relationships: Ë¶ÅÂ≠òÂÑ≤ÁöÑÈóú‰øÇÂàóË°®
        alert_id: Ë≠¶Â†± ID
    
    Returns:
        ÊåÅ‰πÖÂåñÁµêÊûúÔºåÂåÖÂê´ÊàêÂäüÁãÄÊÖãÂíåÁµ±Ë®àË≥áË®ä
    """
    if not neo4j_driver:
        return {
            'success': False,
            'error': 'Neo4j driver not available',
            'nodes_created': 0,
            'relationships_created': 0
        }
    
    try:
        async with neo4j_driver.session() as session:
            # Â≠òÂÑ≤ÁØÄÈªû
            nodes_created = 0
            node_ids = []
            
            for entity in entities:
                # ‰ΩøÁî® MERGE ‰æÜÈÅøÂÖçÈáçË§áÁØÄÈªû
                cypher_query = f"""
                MERGE (n:{entity['type']} {{id: $entity_id}})
                SET n += $properties
                RETURN n.id as node_id
                """
                
                result = await session.run(
                    cypher_query,
                    entity_id=entity['id'],
                    properties=entity['properties']
                )
                
                record = await result.single()
                if record:
                    node_ids.append(record['node_id'])
                    nodes_created += 1
            
            # Â≠òÂÑ≤Èóú‰øÇ
            relationships_created = 0
            
            for relationship in relationships:
                # ‰ΩøÁî® MERGE ‰æÜÈÅøÂÖçÈáçË§áÈóú‰øÇ
                cypher_query = f"""
                MATCH (source {{id: $source_id}})
                MATCH (target {{id: $target_id}})
                MERGE (source)-[r:{relationship['type']}]->(target)
                SET r += $properties
                RETURN r
                """
                
                result = await session.run(
                    cypher_query,
                    source_id=relationship['source_id'],
                    target_id=relationship['target_id'],
                    properties=relationship.get('properties', {})
                )
                
                if await result.peek():
                    relationships_created += 1
            
            # Âª∫Á´ãÁ¥¢Âºï (Â¶ÇÊûú‰∏çÂ≠òÂú®)
            index_queries = [
                "CREATE INDEX alert_timestamp_idx IF NOT EXISTS FOR (a:Alert) ON (a.timestamp)",
                "CREATE INDEX host_agent_id_idx IF NOT EXISTS FOR (h:Host) ON (h.agent_id)",
                "CREATE INDEX ip_address_idx IF NOT EXISTS FOR (i:IPAddress) ON (i.address)",
                "CREATE INDEX user_name_idx IF NOT EXISTS FOR (u:User) ON (u.username)"
            ]
            
            for index_query in index_queries:
                await session.run(index_query)
            
            return {
                'success': True,
                'nodes_created': nodes_created,
                'relationships_created': relationships_created,
                'node_ids': node_ids,
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Graph persistence error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'nodes_created': 0,
            'relationships_created': 0
        }

# ==================== ËºîÂä©ÂáΩÊï∏ ====================

def _extract_risk_level_from_analysis(analysis_result: str) -> str:
    """ÂæûÂàÜÊûêÁµêÊûú‰∏≠ÊèêÂèñÈ¢®Èö™Á≠âÁ¥ö"""
    risk_levels = ['Critical', 'High', 'Medium', 'Low', 'Informational']
    for level in risk_levels:
        if level.lower() in analysis_result.lower():
            return level
    return 'Unknown'

def _calculate_triage_score(alert_source: Dict, analysis_result: str) -> float:
    """Ë®àÁÆóË≠¶Â†±ÂàÜÁ¥öÂàÜÊï∏"""
    base_score = alert_source.get('rule', {}).get('level', 1) * 10
    
    # Ê†πÊìöÂàÜÊûêÁµêÊûúË™øÊï¥ÂàÜÊï∏
    if 'critical' in analysis_result.lower():
        return min(base_score * 1.5, 100)
    elif 'high' in analysis_result.lower():
        return min(base_score * 1.2, 100)
    elif 'low' in analysis_result.lower():
        return max(base_score * 0.8, 0)
    
    return base_score

def _extract_os_info(alert_source: Dict) -> str:
    """ÊèêÂèñ‰ΩúÊ•≠Á≥ªÁµ±Ë≥áË®ä"""
    agent = alert_source.get('agent', {})
    return agent.get('labels', {}).get('os', 'unknown')

def _extract_ip_addresses(alert_source: Dict) -> List[Dict]:
    """ÊèêÂèñ IP ‰ΩçÂùÄË≥áË®ä"""
    ips = []
    data = alert_source.get('data', {})
    
    # ‰æÜÊ∫ê IP
    if data.get('srcip'):
        ips.append({
            'address': data['srcip'],
            'type': 'source',
            'geo': data.get('srcgeoip', {})
        })
    
    # ÁõÆÁöÑ IP
    if data.get('dstip'):
        ips.append({
            'address': data['dstip'],
            'type': 'destination',
            'geo': data.get('dstgeoip', {})
        })
    
    # Agent IP
    agent = alert_source.get('agent', {})
    if agent.get('ip'):
        ips.append({
            'address': agent['ip'],
            'type': 'internal'
        })
    
    return ips

def _is_private_ip(ip_address: str) -> bool:
    """Ê™¢Êü•ÊòØÂê¶ÁÇ∫ÁßÅÊúâ IP ‰ΩçÂùÄ"""
    import ipaddress
    try:
        ip = ipaddress.ip_address(ip_address)
        return ip.is_private
    except:
        return False

def _extract_user_info(alert_source: Dict) -> List[Dict]:
    """ÊèêÂèñ‰ΩøÁî®ËÄÖË≥áË®ä"""
    users = []
    data = alert_source.get('data', {})
    
    # ‰∏ªË¶Å‰ΩøÁî®ËÄÖ
    if data.get('user'):
        users.append({
            'name': data['user'],
            'type': 'primary'
        })
    
    # ‰æÜÊ∫ê‰ΩøÁî®ËÄÖ
    if data.get('srcuser'):
        users.append({
            'name': data['srcuser'],
            'type': 'source'
        })
    
    return users

def _extract_process_info(alert_source: Dict, context_data: Dict) -> List[Dict]:
    """ÊèêÂèñÁ®ãÂ∫èË≥áË®ä"""
    processes = []
    data = alert_source.get('data', {})
    
    # ‰æÜËá™Ë≠¶Â†±ÁöÑÁ®ãÂ∫èË≥áË®ä
    if data.get('process'):
        processes.append({
            'name': data['process'].get('name'),
            'pid': data['process'].get('pid'),
            'cmdline': data['process'].get('cmdline'),
            'ppid': data['process'].get('ppid')
        })
    
    # ‰æÜËá™‰∏ä‰∏ãÊñáÁöÑÁ®ãÂ∫èË≥áË®ä
    process_data = context_data.get('process_data', [])
    for proc in process_data[:5]:  # ÈôêÂà∂Êï∏Èáè
        if isinstance(proc, dict) and proc.get('_source'):
            proc_source = proc['_source']
            processes.append({
                'name': proc_source.get('data', {}).get('process', {}).get('name'),
                'pid': proc_source.get('data', {}).get('process', {}).get('pid'),
                'cmdline': proc_source.get('data', {}).get('process', {}).get('cmdline')
            })
    
    return [p for p in processes if p.get('name')]  # ÈÅéÊøæÁ©∫ÁöÑÁ®ãÂ∫è

def _extract_file_info(alert_source: Dict) -> List[Dict]:
    """ÊèêÂèñÊ™îÊ°àË≥áË®ä"""
    files = []
    data = alert_source.get('data', {})
    
    # Ê™îÊ°àË∑ØÂæë
    if data.get('file'):
        files.append({
            'path': data['file'],
            'name': data['file'].split('/')[-1] if '/' in data['file'] else data['file']
        })
    
    # È°çÂ§ñÁöÑÊ™îÊ°àÊ¨Ñ‰Ωç
    if data.get('path'):
        files.append({
            'path': data['path'],
            'name': data['path'].split('/')[-1] if '/' in data['path'] else data['path']
        })
    
    return files

def _extract_threat_indicators(analysis_result: str) -> List[Dict]:
    """ÂæûÂàÜÊûêÁµêÊûú‰∏≠ÊèêÂèñÂ®ÅËÑÖÊåáÊ®ô"""
    indicators = []
    
    # Á∞°ÂñÆÁöÑÊ≠£ÂâáË°®ÈÅîÂºèÂåπÈÖç
    ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
    domain_pattern = r'\b[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*\b'
    
    # ÊèêÂèñ IP ‰ΩçÂùÄ
    ips = re.findall(ip_pattern, analysis_result)
    for ip in ips[:3]:  # ÈôêÂà∂Êï∏Èáè
        indicators.append({
            'type': 'ip_address',
            'value': ip,
            'confidence': 0.7
        })
    
    # ÊèêÂèñÂüüÂêçÔºàÁ∞°ÂåñÁâàÔºâ
    words = analysis_result.split()
    for word in words:
        if '.' in word and len(word) > 4 and not word.startswith('.'):
            indicators.append({
                'type': 'domain',
                'value': word,
                'confidence': 0.5
            })
            if len(indicators) >= 5:  # ÈôêÂà∂Êï∏Èáè
                break
    
    return indicators

def _determine_user_action_type(alert: Dict) -> str:
    """Á¢∫ÂÆö‰ΩøÁî®ËÄÖÂãï‰ΩúÈ°ûÂûã"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'login' in rule_desc or 'authentication' in rule_desc:
        return 'authentication'
    elif 'ssh' in rule_desc:
        return 'remote_access'
    elif 'file' in rule_desc:
        return 'file_access'
    else:
        return 'unknown'

def _determine_file_access_type(alert: Dict) -> str:
    """Á¢∫ÂÆöÊ™îÊ°àÂ≠òÂèñÈ°ûÂûã"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'write' in rule_desc or 'modify' in rule_desc:
        return 'write'
    elif 'read' in rule_desc:
        return 'read'
    elif 'delete' in rule_desc:
        return 'delete'
    else:
        return 'access'

def _calculate_time_difference(timestamp1: str, timestamp2: str) -> int:
    """Ë®àÁÆóÂÖ©ÂÄãÊôÇÈñìÊà≥‰πãÈñìÁöÑÂ∑ÆÁï∞ÔºàÁßíÔºâ"""
    try:
        from dateutil import parser
        dt1 = parser.parse(timestamp1)
        dt2 = parser.parse(timestamp2)
        return int(abs((dt2 - dt1).total_seconds()))
    except:
        return 0

@app.on_event("shutdown")
def shutdown_event():
    """ÊáâÁî®Á®ãÂºèÈóúÈñâ‰∫ã‰ª∂ËôïÁêÜÂô®"""
    scheduler.shutdown()
    if neo4j_driver:
        neo4j_driver.close()
        logger.info("Neo4j ÈÄ£Êé•Â∑≤ÈóúÈñâ")
    logger.info("ÊéíÁ®ãÂô®Â∑≤ÈóúÈñâ")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

# ==================== Graph-Native Ê™¢Á¥¢Âô® (Stage 4 Step 3) ====================

async def execute_graph_retrieval(cypher_queries: List[Dict[str, Any]], alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    Graph-Native Ê™¢Á¥¢Âô®ÔºöÂü∑Ë°å Cypher Êü•Ë©¢‰æÜÊ™¢Á¥¢Áõ∏ÈóúÁöÑÂúñÂΩ¢Â≠êÁ∂≤
    ÈÄôÊòØ GraphRAG ÁöÑÊ†∏ÂøÉÊ™¢Á¥¢ÂºïÊìéÔºåÂèñ‰ª£ÂÇ≥Áµ±ÁöÑÂêëÈáèËàáÈóúÈçµÂ≠óÊêúÂ∞ã
    
    Args:
        cypher_queries: Âæû Decision Engine ÁîüÊàêÁöÑ Cypher Êü•Ë©¢‰ªªÂãôÂàóË°®
        alert: Áï∂ÂâçË≠¶Â†±Ë≥áÊñô
        
    Returns:
        Dictionary ÂåÖÂê´Ê™¢Á¥¢Âà∞ÁöÑÂúñÂΩ¢Â≠êÁ∂≤ÂíåÁµêÊßãÂåñ‰∏ä‰∏ãÊñá
    """
    logger.info(f"üîó GRAPH-NATIVE RETRIEVAL: Processing {len(cypher_queries)} Cypher queries")
    
    context_data = {
        'attack_paths': [],           # ÊîªÊìäË∑ØÂæëÂ≠êÂúñ
        'lateral_movement': [],       # Ê©´ÂêëÁßªÂãïÊ®°Âºè
        'temporal_sequences': [],     # ÊôÇÈñìÂ∫èÂàóÈóúËÅØ
        'ip_reputation': [],          # IP ‰ø°Ë≠ΩÂúñ
        'user_behavior': [],          # ‰ΩøÁî®ËÄÖË°åÁÇ∫Âúñ
        'process_chains': [],         # Á®ãÂ∫èÂü∑Ë°åÈèà
        'file_interactions': [],      # Ê™îÊ°à‰∫§‰∫íÂúñ
        'network_topology': [],       # Á∂≤Ë∑ØÊãìÊí≤
        'threat_landscape': [],       # Â®ÅËÑÖÂÖ®ÊôØ
        'correlation_graph': []       # Áõ∏ÈóúÊÄßÂúñ
    }
    
    if not neo4j_driver:
        logger.warning("Neo4j driver not available - falling back to traditional retrieval")
        # ÈôçÁ¥öÂà∞ÂÇ≥Áµ±Ê™¢Á¥¢
        return await _fallback_to_traditional_retrieval(alert)
    
    # ÊéíÂ∫èÊü•Ë©¢‰ª•ÂÑ™ÂåñÂü∑Ë°åÈ†ÜÂ∫è
    sorted_queries = sorted(cypher_queries, key=lambda x: {
        'critical': 0, 'high': 1, 'medium': 2, 'low': 3
    }.get(x.get('priority', 'medium'), 2))
    
    alert_id = alert.get('_id')
    
    async with neo4j_driver.session() as session:
        for i, query_spec in enumerate(sorted_queries, 1):
            query_type = query_spec['type']
            description = query_spec['description']
            priority = query_spec.get('priority', 'medium')
            cypher_query = query_spec['cypher_query']
            parameters = query_spec.get('parameters', {})
            
            # Ê≥®ÂÖ•Áï∂ÂâçË≠¶Â†± ID Âà∞ÂèÉÊï∏‰∏≠
            parameters['alert_id'] = alert_id
            
            try:
                logger.info(f"   [{i}/{len(sorted_queries)}] üîç {priority.upper()}: {description}")
                
                # Âü∑Ë°å Cypher Êü•Ë©¢
                result = await session.run(cypher_query, parameters)
                records = await result.data()
                
                # Ê†πÊìöÊü•Ë©¢È°ûÂûãÂàÜÈ°ûÁµêÊûú
                await _categorize_graph_results(query_type, records, context_data)
                
                logger.info(f"      ‚úÖ Graph query returned {len(records)} subgraph components")
                
            except Exception as e:
                logger.error(f"      ‚ùå Cypher query failed: {str(e)}")
                # Ë®òÈåÑÂ§±ÊïóÁöÑÊü•Ë©¢‰ª•‰æøÂæåÁ∫åÂàÜÊûê
                logger.error(f"      Query: {cypher_query[:200]}...")
                continue
    
    # ÁîüÊàêÊ™¢Á¥¢ÊëòË¶Å
    total_components = sum(len(results) for results in context_data.values())
    logger.info(f"üìä GRAPH RETRIEVAL SUMMARY: {total_components} total graph components")
    for category, results in context_data.items():
        if results:
            logger.info(f"   {category}: {len(results)} components")
    
    return context_data

async def _categorize_graph_results(query_type: str, records: List[Dict], context_data: Dict[str, Any]):
    """
    Ê†πÊìöÊü•Ë©¢È°ûÂûãÂ∞áÂúñÂΩ¢ÁµêÊûúÂàÜÈ°ûÂà∞ÈÅ©Áï∂ÁöÑ‰∏ä‰∏ãÊñáÈ°ûÂà•‰∏≠
    
    Args:
        query_type: Êü•Ë©¢È°ûÂûãÔºàÊîªÊìäË∑ØÂæë„ÄÅÊ©´ÂêëÁßªÂãïÁ≠âÔºâ
        records: Cypher Êü•Ë©¢ËøîÂõûÁöÑË®òÈåÑ
        context_data: Ë¶ÅÊõ¥Êñ∞ÁöÑ‰∏ä‰∏ãÊñáË≥áÊñôÂ≠óÂÖ∏
    """
    if query_type == 'attack_path_analysis':
        context_data['attack_paths'].extend(records)
    elif query_type == 'lateral_movement_detection':
        context_data['lateral_movement'].extend(records)
    elif query_type == 'temporal_correlation':
        context_data['temporal_sequences'].extend(records)
    elif query_type == 'ip_reputation_analysis':
        context_data['ip_reputation'].extend(records)
    elif query_type == 'user_behavior_analysis':
        context_data['user_behavior'].extend(records)
    elif query_type == 'process_chain_analysis':
        context_data['process_chains'].extend(records)
    elif query_type == 'file_interaction_analysis':
        context_data['file_interactions'].extend(records)
    elif query_type == 'network_topology_analysis':
        context_data['network_topology'].extend(records)
    elif query_type == 'threat_landscape_analysis':
        context_data['threat_landscape'].extend(records)
    else:
        # È†êË®≠ÂàÜÈ°û
        context_data['correlation_graph'].extend(records)

async def _fallback_to_traditional_retrieval(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    Áï∂ Neo4j ‰∏çÂèØÁî®ÊôÇÔºåÈôçÁ¥öÂà∞ÂÇ≥Áµ±ÁöÑÂêëÈáèÂíåÈóúÈçµÂ≠óÊ™¢Á¥¢
    
    Args:
        alert: Áï∂ÂâçË≠¶Â†±Ë≥áÊñô
        
    Returns:
        ÂÇ≥Áµ±Ê™¢Á¥¢ÁöÑÁµêÊûú
    """
    logger.info("üîÑ Falling back to traditional vector + keyword retrieval")
    
    # ÁîüÊàêÂÇ≥Áµ±Ê™¢Á¥¢Êü•Ë©¢
    traditional_queries = determine_contextual_queries(alert)
    
    # ÂêëÈáèÂåñË≠¶Â†±ÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
    alert_vector = []
    embedding_service = GeminiEmbeddingService()
    try:
        alert_text = _extract_alert_text_for_embedding(alert)
        alert_vector = await embedding_service.embed_text(alert_text)
    except Exception as e:
        logger.warning(f"Alert vectorization failed: {str(e)}")
    
    # Âü∑Ë°åÂÇ≥Áµ±Ê™¢Á¥¢
    return await execute_retrieval(traditional_queries, alert_vector)

def _extract_alert_text_for_embedding(alert: Dict[str, Any]) -> str:
    """
    ÂæûË≠¶Â†±‰∏≠ÊèêÂèñÊñáÊú¨Áî®ÊñºÂêëÈáèÂåñ
    """
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    
    text_parts = [
        rule.get('description', ''),
        ' '.join(rule.get('groups', [])),
        str(alert_source.get('data', {}))
    ]
    
    return ' '.join(filter(None, text_parts))

# ==================== Graph-Native Ê±∫Á≠ñÂºïÊìé ====================

def determine_graph_queries(alert: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Graph-Native Ê±∫Á≠ñÂºïÊìéÔºöÊ†πÊìöË≠¶Â†±ÂÖßÂÆπÊ±∫ÂÆöË¶ÅÂü∑Ë°åÁöÑ Cypher Êü•Ë©¢
    Âèñ‰ª£ÂéüÊúâÁöÑ determine_contextual_queriesÔºåÂ∞àÊ≥®ÊñºÂúñÂΩ¢Êü•Ë©¢Á≠ñÁï•
    
    Args:
        alert: Êñ∞ÁöÑË≠¶Â†±ÊñáÊ™î
        
    Returns:
        Cypher Êü•Ë©¢Ë¶èÊ†ºÂàóË°®
    """
    queries = []
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    data = alert_source.get('data', {})
    timestamp = alert_source.get('timestamp')
    
    rule_description = rule.get('description', '').lower()
    rule_groups = rule.get('groups', [])
    rule_level = rule.get('level', 0)
    agent_id = agent.get('id', '')
    
    logger.info(f"üîó GRAPH-NATIVE DECISION ENGINE: Analyzing alert for graph queries")
    logger.info(f"   Alert: {rule_description}")
    logger.info(f"   Level: {rule_level}, Agent: {agent_id}")
    logger.info(f"   Groups: {', '.join(rule_groups)}")
    
    # 1. SSH Êö¥ÂäõÁ†¥Ëß£Â†¥ÊôØ - ÊîªÊìä‰æÜÊ∫êÂÖ®Ë≤åÂàÜÊûê
    if 'ssh' in rule_description and ('brute' in rule_description or 'failed' in rule_description):
        logger.info("üîë DECISION: SSH brute force detected - analyzing attacker profile")
        
        queries.append({
            'type': 'attack_path_analysis',
            'description': 'SSH attacker complete activity profile',
            'priority': 'critical',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                CALL {
                    WITH attacker
                    MATCH (related_alert:Alert)-[:HAS_SOURCE_IP]->(attacker)
                    WHERE related_alert.timestamp > datetime() - duration({hours: 1})
                    MATCH (related_alert)-[r]->(entity)
                    WHERE type(r) <> 'MATCHED_RULE'
                    RETURN related_alert, r, entity
                }
                RETURN *
            ''',
            'parameters': {}
        })
        
        # Ê©´ÂêëÁßªÂãïÊ™¢Ê∏¨
        queries.append({
            'type': 'lateral_movement_detection',
            'description': 'Lateral movement patterns from attacker IP',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                MATCH (attacker)<-[:HAS_SOURCE_IP]-(other_alerts:Alert)-[:TRIGGERED_ON]->(hosts:Host)
                WITH attacker, collect(DISTINCT hosts) as target_hosts
                WHERE size(target_hosts) > 1
                MATCH path = (attacker)-[*1..3]-(hosts:Host)
                RETURN path, target_hosts, attacker
            ''',
            'parameters': {}
        })
    
    # 2. ÊÉ°ÊÑèËªüÈ´î/Á®ãÂ∫èÂàÜÊûêÂ†¥ÊôØ
    malware_keywords = ['malware', 'trojan', 'virus', 'suspicious', 'backdoor', 'rootkit']
    if any(keyword in rule_description for keyword in malware_keywords):
        logger.info("ü¶† DECISION: Malware detected - analyzing process execution chains")
        
        queries.append({
            'type': 'process_chain_analysis',
            'description': 'Malicious process execution chains',
            'priority': 'critical',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_PROCESS]->(process:Process)
                MATCH path = (process)-[:SPAWNED_BY*0..5]->(parent:Process)
                MATCH (parent)<-[:INVOLVES_PROCESS]-(related_alerts:Alert)
                WHERE related_alerts.timestamp > datetime() - duration({hours: 2})
                RETURN path, collect(related_alerts) as timeline
            ''',
            'parameters': {}
        })
        
        # Ê™îÊ°àÁ≥ªÁµ±ÂΩ±ÈüøÂàÜÊûê
        queries.append({
            'type': 'file_interaction_analysis',
            'description': 'File system impact analysis',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_PROCESS]->(process:Process)
                MATCH (process)-[:ACCESSED_FILE|MODIFIED_FILE|CREATED_FILE]->(files:File)
                MATCH (files)<-[r]-(other_processes:Process)<-[:INVOLVES_PROCESS]-(other_alerts:Alert)
                WHERE other_alerts.timestamp > alert.timestamp - duration({minutes: 30})
                RETURN files, collect(other_processes) as interacting_processes, 
                       collect(other_alerts) as related_alerts
            ''',
            'parameters': {}
        })
    
    # 3. Á∂≤Ë∑ØÊîªÊìäÂ†¥ÊôØ - Web ÊîªÊìäÂàÜÊûê
    web_keywords = ['web attack', 'sql injection', 'xss', 'command injection', 'http']
    if any(keyword in rule_description for keyword in web_keywords) or 'web' in rule_groups:
        logger.info("üåê DECISION: Web attack detected - analyzing network attack patterns")
        
        queries.append({
            'type': 'network_topology_analysis',
            'description': 'Web attack network topology',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                MATCH (alert)-[:TRIGGERED_ON]->(target:Host)
                MATCH (attacker)-[:CONNECTED_TO*1..3]-(related_ips:IPAddress)
                MATCH (related_ips)<-[:HAS_SOURCE_IP]-(attack_alerts:Alert)
                WHERE attack_alerts.timestamp > datetime() - duration({hours: 6})
                RETURN attacker, target, related_ips, collect(attack_alerts) as attack_sequence
            ''',
            'parameters': {}
        })
    
    # 4. ‰ΩøÁî®ËÄÖË°åÁÇ∫Áï∞Â∏∏ÂàÜÊûê
    auth_keywords = ['authentication', 'login', 'failed', 'privilege', 'escalation']
    if any(keyword in rule_description for keyword in auth_keywords) or 'authentication' in rule_groups:
        logger.info("üë§ DECISION: Authentication anomaly - analyzing user behavior patterns")
        
        queries.append({
            'type': 'user_behavior_analysis',
            'description': 'User behavior anomaly analysis',
            'priority': 'medium',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_USER]->(user:User)
                MATCH (user)<-[:INVOLVES_USER]-(user_alerts:Alert)
                WHERE user_alerts.timestamp > datetime() - duration({days: 7})
                WITH user, collect(user_alerts) as user_history
                MATCH (user)<-[:INVOLVES_USER]-(recent_alerts:Alert)
                WHERE recent_alerts.timestamp > datetime() - duration({hours: 2})
                RETURN user, user_history, collect(recent_alerts) as recent_activity
            ''',
            'parameters': {}
        })
    
    # 5. ÊôÇÈñìÂ∫èÂàóÈóúËÅØÂàÜÊûê (Á∏ΩÊòØÂü∑Ë°å)
    queries.append({
        'type': 'temporal_correlation',
        'description': 'Temporal sequence analysis',
        'priority': 'medium',
        'cypher_query': '''
            MATCH (alert:Alert {id: $alert_id})-[:TRIGGERED_ON]->(host:Host)
            MATCH (host)<-[:TRIGGERED_ON]-(related_alerts:Alert)
            WHERE related_alerts.timestamp > alert.timestamp - duration({minutes: 30})
              AND related_alerts.timestamp < alert.timestamp + duration({minutes: 30})
              AND related_alerts.id <> alert.id
            WITH alert, related_alerts
            ORDER BY related_alerts.timestamp
            RETURN alert, collect(related_alerts) as temporal_sequence
        ''',
        'parameters': {}
    })
    
    # 6. IP ‰ø°Ë≠ΩËàáÂú∞ÁêÜ‰ΩçÁΩÆÂàÜÊûê (ÈáùÂ∞çÂ§ñÈÉ® IP)
    if _has_external_ip(alert_source):
        logger.info("üåç DECISION: External IP detected - analyzing IP reputation")
        
        queries.append({
            'type': 'ip_reputation_analysis',
            'description': 'IP reputation and geolocation analysis',
            'priority': 'medium',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(ip:IPAddress)
                WHERE ip.is_private = false
                MATCH (ip)<-[:HAS_SOURCE_IP]-(historical_alerts:Alert)
                WHERE historical_alerts.timestamp > datetime() - duration({days: 30})
                WITH ip, collect(historical_alerts) as ip_history
                MATCH (ip)-[:GEOLOCATED_IN]->(geo:GeoLocation)
                RETURN ip, ip_history, geo
            ''',
            'parameters': {}
        })
    
    # 7. Â®ÅËÑÖÂÖ®ÊôØÂàÜÊûê (È´òÁ¥öÂà•Ë≠¶Â†±)
    if rule_level >= 8:
        logger.info("‚ö†Ô∏è DECISION: High-severity alert - comprehensive threat landscape analysis")
        
        queries.append({
            'type': 'threat_landscape_analysis',
            'description': 'Comprehensive threat landscape',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})
                MATCH (alert)-[r1]->(entity1)
                MATCH (entity1)-[r2]->(entity2)
                MATCH (entity2)<-[r3]-(other_alerts:Alert)
                WHERE other_alerts.timestamp > datetime() - duration({hours: 24})
                  AND other_alerts.rule_level >= 6
                RETURN alert, entity1, entity2, other_alerts, r1, r2, r3
                LIMIT 50
            ''',
            'parameters': {}
        })
    
    logger.info(f"‚úÖ Generated {len(queries)} graph queries for alert analysis")
    return queries

def _has_external_ip(alert_source: Dict[str, Any]) -> bool:
    """
    Ê™¢Êü•Ë≠¶Â†±ÊòØÂê¶ÂåÖÂê´Â§ñÈÉ® IP Âú∞ÂùÄ
    """
    data = alert_source.get('data', {})
    
    # Ê™¢Êü•Â∏∏Ë¶ãÁöÑ IP Ê¨Ñ‰Ωç
    ip_fields = ['srcip', 'dstip', 'src_ip', 'dst_ip', 'remote_ip']
    
    for field in ip_fields:
        ip = data.get(field)
        if ip and not _is_private_ip(ip):
            return True
    
    return False

def _is_private_ip(ip_address: str) -> bool:
    """
    Ê™¢Êü• IP Âú∞ÂùÄÊòØÂê¶ÁÇ∫ÁßÅÊúâÂú∞ÂùÄ
    """
    try:
        import ipaddress
        ip = ipaddress.ip_address(ip_address)
        return ip.is_private
    except:
        return False

# ==================== Ê∑∑ÂêàÊ™¢Á¥¢Êï¥Âêà ====================

async def execute_hybrid_retrieval(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    Ê∑∑ÂêàÊ™¢Á¥¢Á≥ªÁµ±ÔºöÁµêÂêàÂúñÂΩ¢Êü•Ë©¢ÂíåÂÇ≥Áµ±Ê™¢Á¥¢ÊñπÊ≥ï
    ÁÇ∫ GraphRAG Êèê‰æõÊúÄ‰Ω≥ÁöÑ‰∏ä‰∏ãÊñáÊ™¢Á¥¢Á≠ñÁï•
    
    Args:
        alert: Áï∂ÂâçË≠¶Â†±Ë≥áÊñô
        
    Returns:
        ÁµêÂêàÁöÑÊ™¢Á¥¢ÁµêÊûú
    """
    logger.info("üîóüîç HYBRID RETRIEVAL: Combining graph and traditional methods")
    
    # 1. Âü∑Ë°åÂúñÂΩ¢Êü•Ë©¢
    graph_queries = determine_graph_queries(alert)
    graph_context = await execute_graph_retrieval(graph_queries, alert)
    
    # 2. Â¶ÇÊûúÂúñÂΩ¢Êü•Ë©¢ÁµêÊûú‰∏çË∂≥ÔºåË£úÂÖÖÂÇ≥Áµ±Ê™¢Á¥¢
    total_graph_results = sum(len(results) for results in graph_context.values())
    
    if total_graph_results < 10:  # Ë®≠ÂÆöÈñæÂÄº
        logger.info("üìä Graph results insufficient - supplementing with traditional retrieval")
        
        # ÁîüÊàêË£úÂÖÖÊü•Ë©¢
        traditional_queries = determine_contextual_queries(alert)
        
        # ÂêëÈáèÂåñË≠¶Â†±
        embedding_service = GeminiEmbeddingService()
        try:
            alert_text = _extract_alert_text_for_embedding(alert)
            alert_vector = await embedding_service.embed_text(alert_text)
            traditional_context = await execute_retrieval(traditional_queries, alert_vector)
            
            # Âêà‰ΩµÁµêÊûú
            return _merge_retrieval_contexts(graph_context, traditional_context)
        except Exception as e:
            logger.warning(f"Traditional retrieval failed: {str(e)}")
            return graph_context
    
    return graph_context

def _merge_retrieval_contexts(graph_context: Dict[str, Any], traditional_context: Dict[str, Any]) -> Dict[str, Any]:
    """
    Âêà‰ΩµÂúñÂΩ¢Ê™¢Á¥¢ÂíåÂÇ≥Áµ±Ê™¢Á¥¢ÁöÑÁµêÊûú
    """
    merged_context = graph_context.copy()
    
    # Ê∑ªÂä†ÂÇ≥Áµ±Ê™¢Á¥¢ÁöÑÁµêÊûú‰ΩúÁÇ∫Ë£úÂÖÖ‰∏ä‰∏ãÊñá
    merged_context['traditional_similar_alerts'] = traditional_context.get('similar_alerts', [])
    merged_context['traditional_metrics'] = traditional_context.get('cpu_metrics', []) + \
                                          traditional_context.get('memory_metrics', [])
    merged_context['traditional_logs'] = traditional_context.get('network_logs', []) + \
                                       traditional_context.get('ssh_logs', [])
    
    return merged_context

def format_graph_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    Graph-Native ‰∏ä‰∏ãÊñáÊ†ºÂºèÂåñÔºöÂ∞áÂúñÂΩ¢Ê™¢Á¥¢ÁµêÊûúÊ†ºÂºèÂåñÁÇ∫ LLM ÂèØÁêÜËß£ÁöÑÁµêÊßãÂåñÊñáÊú¨
    
    Args:
        context_data: Âæû execute_graph_retrieval Áç≤ÂæóÁöÑÂúñÂΩ¢‰∏ä‰∏ãÊñáË≥áÊñô
        
    Returns:
        Ê†ºÂºèÂåñÁöÑ‰∏ä‰∏ãÊñáÂ≠óÂÖ∏ÔºåÊ∫ñÂÇôÊèê‰æõÁµ¶ LLM ÂàÜÊûê
    """
    formatted_context = {}
    
    # 1. ÊîªÊìäË∑ØÂæëÂàÜÊûê
    attack_paths = context_data.get('attack_paths', [])
    if attack_paths:
        path_parts = []
        for i, path_data in enumerate(attack_paths[:5], 1):
            # Ëß£ÊûêÂúñÂΩ¢Ë∑ØÂæëË≥áÊñô
            attacker = path_data.get('attacker', {})
            related_alerts = path_data.get('related_alert', [])
            entities = path_data.get('entity', [])
            
            path_part = f"""
{i}. **ÊîªÊìä‰æÜÊ∫ê:** {attacker.get('address', 'Unknown IP')}
   **Áõ∏ÈóúË≠¶Â†±Êï∏Èáè:** {len(related_alerts) if isinstance(related_alerts, list) else 1}
   **ÂΩ±ÈüøÂØ¶È´î:** {len(entities) if isinstance(entities, list) else 1} ÂÄãÁ≥ªÁµ±ÁµÑ‰ª∂
   **ÊîªÊìäÊôÇÈñìÁØÑÂúç:** ÈÅéÂéª1Â∞èÊôÇÂÖßÁöÑÊåÅÁ∫åÊ¥ªÂãï"""
            path_parts.append(path_part)
        formatted_context['attack_path_analysis'] = "\n".join(path_parts)
    else:
        formatted_context['attack_path_analysis'] = "Êú™ÁôºÁèæÊòéÁ¢∫ÁöÑÊîªÊìäË∑ØÂæëÊ®°Âºè„ÄÇ"
    
    # 2. Ê©´ÂêëÁßªÂãïÊ™¢Ê∏¨
    lateral_movement = context_data.get('lateral_movement', [])
    if lateral_movement:
        movement_parts = []
        for i, movement_data in enumerate(lateral_movement[:3], 1):
            attacker = movement_data.get('attacker', {})
            target_hosts = movement_data.get('target_hosts', [])
            
            movement_part = f"""
{i}. **Ê©´ÂêëÁßªÂãï‰æÜÊ∫ê:** {attacker.get('address', 'Unknown')}
   **ÁõÆÊ®ô‰∏ªÊ©üÊï∏Èáè:** {len(target_hosts)} Âè∞‰∏ªÊ©ü
   **ÁßªÂãïÊ®°Âºè:** Â§ö‰∏ªÊ©üÊª≤ÈÄèÊ™¢Ê∏¨Âà∞"""
            movement_parts.append(movement_part)
        formatted_context['lateral_movement_analysis'] = "\n".join(movement_parts)
    else:
        formatted_context['lateral_movement_analysis'] = "Êú™Ê™¢Ê∏¨Âà∞Ê©´ÂêëÁßªÂãïÊ¥ªÂãï„ÄÇ"
    
    # 3. ÊôÇÈñìÂ∫èÂàóÈóúËÅØ
    temporal_sequences = context_data.get('temporal_sequences', [])
    if temporal_sequences:
        temporal_parts = []
        for seq_data in temporal_sequences[:3]:
            sequence = seq_data.get('temporal_sequence', [])
            if sequence:
                temporal_part = f"**ÊôÇÈñìÂ∫èÂàóÁõ∏ÈóúË≠¶Â†±:** {len(sequence)} ÂÄãÁõ∏Èóú‰∫ã‰ª∂Âú®¬±30ÂàÜÈêòÊôÇÈñìÁ™óÂè£ÂÖß"
                temporal_parts.append(temporal_part)
        formatted_context['temporal_correlation'] = "\n".join(temporal_parts)
    else:
        formatted_context['temporal_correlation'] = "Êú™ÁôºÁèæÊôÇÈñìÂ∫èÂàóÁõ∏Èóú‰∫ã‰ª∂„ÄÇ"
    
    # 4. IP ‰ø°Ë≠ΩÂàÜÊûê
    ip_reputation = context_data.get('ip_reputation', [])
    if ip_reputation:
        ip_parts = []
        for ip_data in ip_reputation[:3]:
            ip = ip_data.get('ip', {})
            ip_history = ip_data.get('ip_history', [])
            geo = ip_data.get('geo', {})
            
            ip_part = f"""
**IP Âú∞ÂùÄ:** {ip.get('address', 'Unknown')}
**Ê≠∑Âè≤Ê¥ªÂãï:** ÈÅéÂéª30Â§©ÂÖß {len(ip_history)} Ê¨°Ë≠¶Â†±Ë®òÈåÑ
**Âú∞ÁêÜ‰ΩçÁΩÆ:** {geo.get('country', 'Unknown')} - {geo.get('city', 'Unknown')}
**ÁßÅÊúâÂú∞ÂùÄ:** {'Âê¶' if not ip.get('is_private', True) else 'ÊòØ'}"""
            ip_parts.append(ip_part)
        formatted_context['ip_reputation_analysis'] = "\n".join(ip_parts)
    else:
        formatted_context['ip_reputation_analysis'] = "ÁÑ°Â§ñÈÉ®IP‰ø°Ë≠ΩË≥áÊñôÂèØ‰æõÂàÜÊûê„ÄÇ"
    
    # 5. ‰ΩøÁî®ËÄÖË°åÁÇ∫ÂàÜÊûê
    user_behavior = context_data.get('user_behavior', [])
    if user_behavior:
        user_parts = []
        for user_data in user_behavior[:3]:
            user = user_data.get('user', {})
            user_history = user_data.get('user_history', [])
            recent_activity = user_data.get('recent_activity', [])
            
            user_part = f"""
**‰ΩøÁî®ËÄÖ:** {user.get('username', 'Unknown')}
**Ê≠∑Âè≤Ë°åÁÇ∫:** ÈÅéÂéª7Â§©ÂÖß {len(user_history)} Ê¨°Ê¥ªÂãïË®òÈåÑ
**ËøëÊúüÁï∞Â∏∏:** ÈÅéÂéª2Â∞èÊôÇÂÖß {len(recent_activity)} Ê¨°Ê¥ªÂãï"""
            user_parts.append(user_part)
        formatted_context['user_behavior_analysis'] = "\n".join(user_parts)
    else:
        formatted_context['user_behavior_analysis'] = "Êú™ÁôºÁèæÁõ∏Èóú‰ΩøÁî®ËÄÖË°åÁÇ∫Áï∞Â∏∏„ÄÇ"
    
    # 6. Á®ãÂ∫èÂü∑Ë°åÈèàÂàÜÊûê
    process_chains = context_data.get('process_chains', [])
    if process_chains:
        process_parts = []
        for process_data in process_chains[:3]:
            timeline = process_data.get('timeline', [])
            if timeline:
                process_part = f"**Á®ãÂ∫èÂü∑Ë°åÈèà:** Ê™¢Ê∏¨Âà∞ {len(timeline)} ÂÄãÁõ∏ÈóúÁ®ãÂ∫èÂü∑Ë°å‰∫ã‰ª∂"
                process_parts.append(process_part)
        formatted_context['process_chain_analysis'] = "\n".join(process_parts)
    else:
        formatted_context['process_chain_analysis'] = "Êú™ÁôºÁèæÂèØÁñëÁöÑÁ®ãÂ∫èÂü∑Ë°åÈèà„ÄÇ"
    
    # 7. Ê™îÊ°à‰∫§‰∫íÂàÜÊûê
    file_interactions = context_data.get('file_interactions', [])
    if file_interactions:
        file_parts = []
        for file_data in file_interactions[:3]:
            files = file_data.get('files', {})
            interacting_processes = file_data.get('interacting_processes', [])
            
            file_part = f"""
**Ê™îÊ°àË∑ØÂæë:** {files.get('file_path', 'Unknown')}
**‰∫§‰∫íÁ®ãÂ∫èÊï∏Èáè:** {len(interacting_processes)}"""
            file_parts.append(file_part)
        formatted_context['file_interaction_analysis'] = "\n".join(file_parts)
    else:
        formatted_context['file_interaction_analysis'] = "Êú™ÁôºÁèæÁï∞Â∏∏ÁöÑÊ™îÊ°àÁ≥ªÁµ±‰∫§‰∫í„ÄÇ"
    
    # 8. Á∂≤Ë∑ØÊãìÊí≤ÂàÜÊûê
    network_topology = context_data.get('network_topology', [])
    if network_topology:
        network_parts = []
        for net_data in network_topology[:3]:
            attacker = net_data.get('attacker', {})
            target = net_data.get('target', {})
            attack_sequence = net_data.get('attack_sequence', [])
            
            network_part = f"""
**ÊîªÊìä‰æÜÊ∫ê:** {attacker.get('address', 'Unknown')}
**ÁõÆÊ®ô‰∏ªÊ©ü:** {target.get('agent_name', 'Unknown')}
**ÊîªÊìäÂ∫èÂàó:** ÈÅéÂéª6Â∞èÊôÇÂÖß {len(attack_sequence)} Ê¨°Áõ∏ÈóúÊîªÊìä"""
            network_parts.append(network_part)
        formatted_context['network_topology_analysis'] = "\n".join(network_parts)
    else:
        formatted_context['network_topology_analysis'] = "Êú™ÁôºÁèæË§áÈõúÁöÑÁ∂≤Ë∑ØÊîªÊìäÊãìÊí≤„ÄÇ"
    
    # 9. Â®ÅËÑÖÂÖ®ÊôØÂàÜÊûê
    threat_landscape = context_data.get('threat_landscape', [])
    if threat_landscape:
        threat_parts = []
        threat_count = len(threat_landscape)
        if threat_count > 0:
            threat_part = f"**Á∂úÂêàÂ®ÅËÑÖË©ï‰º∞:** Ê™¢Ê∏¨Âà∞ {threat_count} ÂÄãÈ´òÁ¥öÂà•Â®ÅËÑÖÈóúËÅØ‰∫ã‰ª∂ÔºàÈÅéÂéª24Â∞èÊôÇÔºâ"
            threat_parts.append(threat_part)
        formatted_context['threat_landscape_analysis'] = "\n".join(threat_parts)
    else:
        formatted_context['threat_landscape_analysis'] = "Êï¥È´îÂ®ÅËÑÖÁí∞Â¢ÉÁõ∏Â∞çÁ©©ÂÆö„ÄÇ"
    
    # 10. ÂÇ≥Áµ±Ê™¢Á¥¢Ë£úÂÖÖÔºàÊ∑∑ÂêàÊ®°ÂºèÔºâ
    traditional_alerts = context_data.get('traditional_similar_alerts', [])
    traditional_metrics = context_data.get('traditional_metrics', [])
    traditional_logs = context_data.get('traditional_logs', [])
    
    if traditional_alerts or traditional_metrics or traditional_logs:
        supplement_parts = []
        if traditional_alerts:
            supplement_parts.append(f"**Áõ∏‰ººË≠¶Â†±Ë£úÂÖÖ:** {len(traditional_alerts)} ÂÄãÂêëÈáèÁõ∏‰ººË≠¶Â†±")
        if traditional_metrics:
            supplement_parts.append(f"**Á≥ªÁµ±ÊåáÊ®ôË£úÂÖÖ:** {len(traditional_metrics)} ÂÄãÁ≥ªÁµ±ÊÄßËÉΩË®òÈåÑ")
        if traditional_logs:
            supplement_parts.append(f"**Êó•Ë™åË£úÂÖÖ:** {len(traditional_logs)} ÂÄãÁ∂≤Ë∑Ø/SSHÊó•Ë™å")
        formatted_context['traditional_supplement'] = "\n".join(supplement_parts)
    else:
        formatted_context['traditional_supplement'] = "ÁÑ°ÈúÄÂÇ≥Áµ±Ê™¢Á¥¢Ë£úÂÖÖ„ÄÇ"
    
    return formatted_context

# ==================== Ê∑∑ÂêàÊ†ºÂºèÂåñÂáΩÊï∏ ====================

def format_hybrid_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    Ê∑∑Âêà‰∏ä‰∏ãÊñáÊ†ºÂºèÂåñÔºöËá™ÂãïÊ™¢Ê∏¨‰∏¶Ê†ºÂºèÂåñÂúñÂΩ¢ÊàñÂÇ≥Áµ±Ê™¢Á¥¢ÁµêÊûú
    
    Args:
        context_data: Ê™¢Á¥¢ÁµêÊûúË≥áÊñô
        
    Returns:
        Ê†ºÂºèÂåñÁöÑ‰∏ä‰∏ãÊñáÂ≠óÂÖ∏
    """
    # Ê™¢Ê∏¨ÊòØÂê¶ÁÇ∫ÂúñÂΩ¢Ê™¢Á¥¢ÁµêÊûú
    graph_indicators = ['attack_paths', 'lateral_movement', 'temporal_sequences', 
                       'ip_reputation', 'user_behavior', 'process_chains']
    
    has_graph_data = any(context_data.get(indicator) for indicator in graph_indicators)
    
    if has_graph_data:
        logger.info("üîó Formatting graph-native context for LLM analysis")
        return format_graph_context(context_data)
    else:
        logger.info("üìä Formatting traditional context for LLM analysis")
        return format_multi_source_context(context_data)