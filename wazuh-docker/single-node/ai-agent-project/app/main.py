import os
import logging
import traceback
import asyncio
from typing import List, Dict, Any, Optional
from fastapi import FastAPI
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# <--- æ–°å¢: åŒ¯å…¥æ–°çš„ LLM é¡åˆ¥ ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from opensearchpy import AsyncOpenSearch, AsyncHttpConnection

# --- æ–°å¢ Embedding æœå‹™ ---
from embedding_service import GeminiEmbeddingService

# --- åŸºç¤è¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½® ---
OPENSEARCH_URL = os.getenv("OPENSEARCH_URL", "https://wazuh.indexer:9200")
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER", "admin")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD", "SecretPassword")

# <--- ä¿®æ”¹: è®€å– LLM ä¾›æ‡‰å•†å’Œå°æ‡‰çš„ Keys ---
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "anthropic").lower() # é è¨­ç‚º anthropic
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# --- OpenSearch å®¢æˆ¶ç«¯ ---
client = AsyncOpenSearch(
    hosts=[OPENSEARCH_URL],
    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),
    use_ssl=True,
    verify_certs=False,
    ssl_show_warn=False,
    connection_class=AsyncHttpConnection
)

# <--- æ–°å¢: æ ¹æ“šç’°å¢ƒè®Šæ•¸é¸æ“‡ LLM çš„å‡½å¼ ---
def get_llm():
    """æ ¹æ“šç’°å¢ƒè®Šæ•¸ LLM_PROVIDER é¸æ“‡ä¸¦åˆå§‹åŒ– LLM"""
    logging.info(f"Selected LLM Provider: {LLM_PROVIDER}")
    
    if LLM_PROVIDER == 'gemini':
        if not GEMINI_API_KEY:
            raise ValueError("LLM_PROVIDER is 'gemini' but GEMINI_API_KEY is not set.")
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GEMINI_API_KEY)
    
    elif LLM_PROVIDER == 'anthropic':
        if not ANTHROPIC_API_KEY:
            raise ValueError("LLM_PROVIDER is 'anthropic' but ANTHROPIC_API_KEY is not set.")
        return ChatAnthropic(model="claude-3-haiku-20240307", anthropic_api_key=ANTHROPIC_API_KEY)
    
    else:
        raise ValueError(f"Unsupported LLM_PROVIDER: {LLM_PROVIDER}. Please choose 'gemini' or 'anthropic'.")

# --- æ–°å¢: æª¢ç´¢æ¨¡çµ„ (Retrieval Module) ---
class AlertRetrievalModule:
    """å°ˆé–€è² è²¬ç›¸ä¼¼è­¦å ±æª¢ç´¢çš„æ¨¡çµ„"""
    
    def __init__(self, opensearch_client: AsyncOpenSearch):
        self.client = opensearch_client
        logging.info("AlertRetrievalModule initialized")
    
    async def retrieve_similar_alerts(self, query_vector: List[float], k: int = 5) -> List[Dict[str, Any]]:
        """
        åŸ·è¡Œ k-NN æœå°‹ï¼Œæ‰¾å‡º N ç­†æœ€ç›¸ä¼¼çš„æ­·å²è­¦å ±
        
        Args:
            query_vector: æŸ¥è©¢å‘é‡
            k: è¿”å›ç›¸ä¼¼è­¦å ±çš„æ•¸é‡
            
        Returns:
            List of similar alerts with metadata and scores
        """
        try:
            vector_search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "knn": {
                                    "alert_vector": {
                                        "vector": query_vector,
                                        "k": k
                                    }
                                }
                            }
                        ],
                        "filter": [
                            # åªæª¢ç´¢å·²ç¶“æœ‰AIåˆ†æçš„æ­·å²è­¦å ±
                            {"exists": {"field": "ai_analysis"}}
                        ]
                    }
                },
                "_source": {
                    "includes": [
                        "rule.description", "rule.level", "rule.id",
                        "agent.name", "agent.ip", 
                        "timestamp", "ai_analysis.triage_report",
                        "location", "full_log"
                    ]
                }
            }
            
            response = await self.client.search(
                index="wazuh-alerts-*", 
                body=vector_search_body,
                size=k
            )
            
            similar_alerts = response['hits']['hits']
            logging.info(f"âœ… æª¢ç´¢æ¨¡çµ„æ‰¾åˆ° {len(similar_alerts)} ç­†ç›¸ä¼¼æ­·å²è­¦å ±")
            
            # è¨˜éŒ„æª¢ç´¢è©³æƒ…ç”¨æ–¼é™¤éŒ¯
            for i, alert in enumerate(similar_alerts):
                score = alert.get('_score', 0)
                rule_desc = alert['_source'].get('rule', {}).get('description', 'N/A')
                logging.info(f"  ç›¸ä¼¼è­¦å ± #{i+1}: åˆ†æ•¸={score:.4f}, è¦å‰‡={rule_desc[:50]}")
                
            return similar_alerts
            
        except Exception as e:
            logging.warning(f"âŒ æª¢ç´¢æ¨¡çµ„åŸ·è¡Œå¤±æ•—: {str(e)}")
            return []
    
    def format_historical_context(self, similar_alerts: List[Dict[str, Any]]) -> str:
        """
        æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ­·å²è­¦å ±ï¼Œæº–å‚™æ³¨å…¥åˆ° prompt ä¸­
        
        Args:
            similar_alerts: ç›¸ä¼¼è­¦å ±åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–å¾Œçš„æ­·å²è­¦å ±ä¸Šä¸‹æ–‡å­—ä¸²
        """
        if not similar_alerts:
            return "ç›®å‰æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„æ­·å²è­¦å ±å¯ä¾›åƒè€ƒã€‚"
        
        context_parts = ["ä»¥ä¸‹æ˜¯ç›¸é—œçš„æ­·å²è­¦å ±åˆ†æåƒè€ƒï¼š\n"]
        
        for i, alert in enumerate(similar_alerts, 1):
            source = alert['_source']
            rule = source.get('rule', {})
            agent = source.get('agent', {})
            ai_analysis = source.get('ai_analysis', {})
            score = alert.get('_score', 0)
            
            # æ ¼å¼åŒ–æ¯å€‹æ­·å²è­¦å ±
            context_parts.append(f"ã€æ­·å²è­¦å ± #{i}ã€‘(ç›¸ä¼¼åº¦: {score:.3f})")
            context_parts.append(f"è¦å‰‡: {rule.get('description', 'N/A')} (ç­‰ç´š: {rule.get('level', 'N/A')})")
            context_parts.append(f"ä¸»æ©Ÿ: {agent.get('name', 'N/A')} ({agent.get('ip', 'N/A')})")
            context_parts.append(f"æ™‚é–“: {source.get('timestamp', 'N/A')}")
            
            # åŒ…å«ä¹‹å‰çš„AIåˆ†æçµæœ
            if ai_analysis.get('triage_report'):
                triage_summary = ai_analysis['triage_report'][:200] + "..." if len(ai_analysis['triage_report']) > 200 else ai_analysis['triage_report']
                context_parts.append(f"ä¹‹å‰åˆ†æ: {triage_summary}")
            
            context_parts.append("---")
        
        formatted_context = "\n".join(context_parts)
        logging.info(f"âœ… æ ¼å¼åŒ–äº† {len(similar_alerts)} ç­†æ­·å²è­¦å ±ä¸Šä¸‹æ–‡ï¼Œç¸½é•·åº¦: {len(formatted_context)} å­—å…ƒ")
        
        return formatted_context

# --- LangChain å…ƒä»¶ ---
llm = get_llm()

# --- ä¿®æ”¹å¾Œçš„ Prompt Template (åŒ…å«æ­·å²è­¦å ±ä¸Šä¸‹æ–‡) ---
prompt_template = ChatPromptTemplate.from_template(
    """You are a senior security analyst with access to historical alert analysis. Your task is to triage a new Wazuh alert by leveraging both the current alert data and relevant historical context.

    **Relevant Historical Alerts:**
    {historical_context}

    **Current Wazuh Alert to Analyze:**
    {alert_summary}

    **Additional Context from the same host:**
    {context}

    **Your Enhanced Analysis Task:**
    1. **Event Summary**: Briefly describe what happened in this alert.
    2. **Historical Pattern Analysis**: Based on the historical alerts above, identify any patterns, trends, or recurring issues.
    3. **Risk Assessment**: Assess the potential risk level (Critical, High, Medium, Low, Informational) considering both current alert and historical context.
    4. **Contextual Insights**: Provide insights based on historical data (e.g., "This source IP has triggered 3 similar alerts in the past 24 hours").
    5. **Actionable Recommendation**: Provide a clear, specific recommendation for the next step based on the comprehensive analysis.

    **Your Enhanced Triage Report:**
    """
)

output_parser = StrOutputParser()
chain = prompt_template | llm | output_parser

# --- åˆå§‹åŒ– Embedding æœå‹™ ---
embedding_service = GeminiEmbeddingService()

# --- åˆå§‹åŒ–æª¢ç´¢æ¨¡çµ„ ---
retrieval_module = AlertRetrievalModule(client)

# --- æ¨¡çµ„åŒ–å‡½å¼ ---

async def query_new_alerts(max_alerts: int = 10) -> List[Dict[str, Any]]:
    """æŸ¥è©¢å°šæœªé€²è¡Œ AI åˆ†æçš„æ–°è­¦å ±"""
    try:
        response = await client.search(
            index="wazuh-alerts-*", 
            body={
                "query": {
                    "bool": {
                        "must_not": [{"exists": {"field": "ai_analysis"}}]
                    }
                }
            }, 
            size=max_alerts
        )
        alerts = response['hits']['hits']
        logging.info(f"Found {len(alerts)} new alerts to process")
        return alerts
    except Exception as e:
        logging.error(f"Error querying new alerts: {e}")
        raise

async def vectorize_alert(alert_source: Dict[str, Any]) -> List[float]:
    """å°‡è­¦å ±å…§å®¹è½‰æ›ç‚ºå‘é‡"""
    try:
        rule = alert_source.get('rule', {})
        agent = alert_source.get('agent', {})
        
        # æ§‹å»ºç”¨æ–¼å‘é‡åŒ–çš„è­¦å ±æ‘˜è¦
        alert_summary = f"Rule: {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')}) on Host: {agent.get('name', 'N/A')}"
        
        # ä½¿ç”¨ Gemini Embedding é€²è¡Œå‘é‡åŒ–
        alert_vector = await embedding_service.embed_query(alert_summary)
        logging.info(f"Successfully vectorized alert: {alert_summary[:50]}...")
        return alert_vector
    except Exception as e:
        logging.error(f"Error vectorizing alert: {e}")
        raise

async def find_similar_alerts(alert_vector: List[float], k: int = 5) -> List[Dict[str, Any]]:
    """ä½¿ç”¨å‘é‡æœå°‹æ‰¾å‡ºç›¸ä¼¼çš„æ­·å²è­¦å ± (èˆŠç‰ˆæœ¬ï¼Œå·²è¢«æª¢ç´¢æ¨¡çµ„å–ä»£)"""
    try:
        vector_search_body = {
            "query": {
                "knn": {
                    "alert_vector": {
                        "vector": alert_vector,
                        "k": k
                    }
                }
            }
        }
        
        similar_alerts = await client.search(
            index="wazuh-alerts-*", 
            body=vector_search_body,
            size=k
        )
        
        logging.info(f"Found {len(similar_alerts['hits']['hits'])} similar alerts")
        return similar_alerts['hits']['hits']
    except Exception as e:
        logging.warning(f"Vector search failed, using empty context: {str(e)}")
        return []

def build_context_from_similar_alerts(similar_alerts: List[Dict[str, Any]]) -> str:
    """å¾ç›¸ä¼¼è­¦å ±æ§‹å»ºä¸Šä¸‹æ–‡è³‡è¨Š (èˆŠç‰ˆæœ¬ï¼Œå·²è¢«æª¢ç´¢æ¨¡çµ„å–ä»£)"""
    if not similar_alerts:
        return "No additional context retrieved for this example."
    
    context = "ç›¸é—œæ­·å²è­¦å ±:\n"
    for similar_alert in similar_alerts:
        similar_rule = similar_alert['_source'].get('rule', {})
        context += f"- {similar_rule.get('description', 'N/A')} (Level: {similar_rule.get('level', 'N/A')})\n"
    
    return context

async def analyze_alert(alert_summary: str, historical_context: str, context: str) -> str:
    """ä½¿ç”¨ LLM åˆ†æè­¦å ± (æ›´æ–°ç‰ˆæœ¬ï¼ŒåŒ…å«æ­·å²ä¸Šä¸‹æ–‡)"""
    try:
        analysis_result = await chain.ainvoke({
            "alert_summary": alert_summary, 
            "historical_context": historical_context,
            "context": context
        })
        logging.info(f"AI Analysis completed: {analysis_result[:100]}...")
        return analysis_result
    except Exception as e:
        logging.error(f"Error analyzing alert: {e}")
        raise

async def update_alert_with_analysis_and_vector(
    alert_index: str, 
    alert_id: str, 
    alert_source: Dict[str, Any],
    analysis_result: str, 
    alert_vector: List[float]
) -> None:
    """æ›´æ–°è­¦å ±ï¼ŒåŠ å…¥ AI åˆ†æçµæœå’Œå‘é‡"""
    try:
        update_body = {
            "doc": {
                "ai_analysis": {
                    "triage_report": analysis_result, 
                    "provider": LLM_PROVIDER, 
                    "timestamp": alert_source.get('timestamp')
                },
                "alert_vector": alert_vector
            }
        }
        
        await client.update(index=alert_index, id=alert_id, body=update_body)
        logging.info(f"Successfully updated alert {alert_id} with AI analysis and vector")
    except Exception as e:
        logging.error(f"Error updating alert {alert_id}: {e}")
        raise

async def process_single_alert(alert: Dict[str, Any]) -> None:
    """è™•ç†å–®ä¸€è­¦å ±çš„å®Œæ•´æµç¨‹ (æ•´åˆæª¢ç´¢æ¨¡çµ„)"""
    alert_id = alert['_id']
    alert_index = alert['_index']
    alert_source = alert['_source']
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    
    alert_summary = f"Rule: {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')}) on Host: {agent.get('name', 'N/A')}"
    
    logging.info(f"ğŸ” Processing alert: {alert_id} - {alert_summary}")
    
    try:
        # æ­¥é©Ÿ 1: å‘é‡åŒ–è­¦å ±
        logging.info(f"ğŸ“Š Step 1: å‘é‡åŒ–è­¦å ± {alert_id}")
        alert_vector = await vectorize_alert(alert_source)
        
        # æ­¥é©Ÿ 2: ä½¿ç”¨æª¢ç´¢æ¨¡çµ„æœå°‹ç›¸ä¼¼è­¦å ±
        logging.info(f"ğŸ” Step 2: ä½¿ç”¨æª¢ç´¢æ¨¡çµ„æœå°‹ç›¸ä¼¼æ­·å²è­¦å ±")
        similar_alerts = await retrieval_module.retrieve_similar_alerts(alert_vector, k=5)
        
        # æ­¥é©Ÿ 3: æ ¼å¼åŒ–æ­·å²è­¦å ±ä¸Šä¸‹æ–‡
        logging.info(f"ğŸ“ Step 3: æ ¼å¼åŒ–æ­·å²è­¦å ±ä¸Šä¸‹æ–‡")
        historical_context = retrieval_module.format_historical_context(similar_alerts)
        
        # æ­¥é©Ÿ 4: æ§‹å»ºé¡å¤–ä¸Šä¸‹æ–‡ (åŒä¸»æ©Ÿç›¸é—œæ—¥èªŒ)
        context = "No additional host context available"  # å¯ä»¥åœ¨æœªä¾†æ“´å±•
        
        # æ­¥é©Ÿ 5: AI åˆ†æ (åŒ…å«æ­·å²ä¸Šä¸‹æ–‡)
        logging.info(f"ğŸ¤– Step 5: AI åˆ†æ (å«æ­·å²ä¸Šä¸‹æ–‡)")
        analysis_result = await analyze_alert(alert_summary, historical_context, context)
        
        # æ­¥é©Ÿ 6: æ›´æ–°è­¦å ±
        logging.info(f"ğŸ’¾ Step 6: æ›´æ–°è­¦å ±åˆ° OpenSearch")
        await update_alert_with_analysis_and_vector(
            alert_index, alert_id, alert_source, analysis_result, alert_vector
        )
        
        logging.info(f"âœ… Successfully processed alert {alert_id}")
        
    except Exception as e:
        logging.error(f"âŒ Error processing alert {alert_id}: {e}")
        raise

async def triage_new_alerts():
    """ä¸»è¦çš„ triage å·¥ä½œæµç¨‹ (æ•´åˆæª¢ç´¢æ¨¡çµ„)"""
    print("--- ğŸš€ TRIAGE JOB EXECUTING WITH RETRIEVAL MODULE ---")
    logging.info(f"Analyzing alerts with {LLM_PROVIDER} model and Gemini Embedding + Retrieval Module...")
    
    try:
        # æŸ¥è©¢æ–°è­¦å ±
        alerts = await query_new_alerts()
        
        if not alerts:
            print("--- No new alerts found. ---")
            logging.info("No new alerts found.")
            return
        
        print(f"--- ğŸ“Š Found {len(alerts)} new alerts to process with RAG ---")
        
        # è™•ç†æ¯å€‹è­¦å ±
        for i, alert in enumerate(alerts, 1):
            print(f"--- ğŸ”„ Processing alert {i}/{len(alerts)} ---")
            await process_single_alert(alert)
            
        print(f"--- âœ… Successfully processed {len(alerts)} alerts with enhanced RAG analysis ---")
            
    except Exception as e:
        print(f"!!!!!! A CRITICAL ERROR OCCURRED IN TRIAGE JOB !!!!!!")
        logging.error(f"An error occurred during triage: {e}", exc_info=True)
        traceback.print_exc()

# --- FastAPI æ‡‰ç”¨èˆ‡æ’ç¨‹ ---
app = FastAPI(title="Wazuh AI Triage Agent with RAG")
scheduler = AsyncIOScheduler()

@app.on_event("startup")
async def startup_event():
    logging.info("ğŸš€ AI Agent with RAG starting up...")
    # ç¢ºä¿ OpenSearch ç´¢å¼•æ¨¡æ¿å­˜åœ¨
    await ensure_index_template()
    scheduler.add_job(triage_new_alerts, 'interval', seconds=60, id='triage_job', misfire_grace_time=30)
    scheduler.start()
    logging.info("âœ… Scheduler started. Triage job with RAG scheduled.")

async def ensure_index_template():
    """ç¢ºä¿ wazuh-alerts ç´¢å¼•æ¨¡æ¿åŒ…å« alert_vector æ¬„ä½"""
    try:
        template_name = "wazuh-alerts-template"
        template_body = {
            "index_patterns": ["wazuh-alerts-*"],
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "index.knn": True  # å•Ÿç”¨ KNN æœå°‹
            },
            "mappings": {
                "properties": {
                    "alert_vector": {
                        "type": "knn_vector",
                        "dimension": embedding_service.dimension or 768,  # ä½¿ç”¨ embedding service çš„ç¶­åº¦
                        "method": {
                            "name": "hnsw",
                            "space_type": "cosinesimil",
                            "engine": "nmslib"
                        }
                    },
                    "ai_analysis": {
                        "properties": {
                            "triage_report": {"type": "text"},
                            "provider": {"type": "keyword"},
                            "timestamp": {"type": "date"}
                        }
                    }
                }
            }
        }
        
        # æª¢æŸ¥æ¨¡æ¿æ˜¯å¦å­˜åœ¨
        try:
            await client.indices.get_template(name=template_name)
            logging.info(f"Index template {template_name} already exists")
        except Exception:
            # æ¨¡æ¿ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°æ¨¡æ¿
            await client.indices.put_template(name=template_name, body=template_body)
            logging.info(f"Created index template: {template_name}")
            
    except Exception as e:
        logging.error(f"Error ensuring index template: {e}")

@app.get("/")
def read_root():
    return {
        "status": "AI Triage Agent with RAG is running", 
        "features": ["Vector Retrieval", "Historical Context", "Enhanced Analysis"],
        "scheduler_status": str(scheduler.get_jobs())
    }

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    try:
        # æª¢æŸ¥ OpenSearch é€£æ¥
        await client.info()
        return {
            "status": "healthy",
            "opensearch": "connected",
            "llm_provider": LLM_PROVIDER,
            "embedding_dimension": embedding_service.dimension or 768,
            "retrieval_module": "active"
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.on_event("shutdown")
def shutdown_event():
    scheduler.shutdown()
    logging.info("Scheduler shut down.")