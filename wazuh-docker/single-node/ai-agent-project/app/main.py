import os
import logging
import traceback
import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime, timedelta
from fastapi import FastAPI
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import uuid
import json
import re

# LangChain ç›¸é—œå¥—ä»¶å¼•å…¥
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# OpenSearch å®¢æˆ¶ç«¯
from opensearchpy import AsyncOpenSearch, AsyncHttpConnection

# Neo4j åœ–å½¢è³‡æ–™åº«å®¢æˆ¶ç«¯
try:
    from neo4j import AsyncGraphDatabase, AsyncDriver
    NEO4J_AVAILABLE = True
except ImportError:
    logger.warning("Neo4j driver not available. Graph persistence will be disabled.")
    NEO4J_AVAILABLE = False
    AsyncGraphDatabase = None
    AsyncDriver = None

# å¼•å…¥è‡ªå®šç¾©çš„åµŒå…¥æœå‹™æ¨¡çµ„
from embedding_service import GeminiEmbeddingService

# é…ç½®æ—¥èªŒç³»çµ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒè®Šæ•¸é…ç½®
OPENSEARCH_URL = os.getenv("OPENSEARCH_URL", "https://wazuh.indexer:9200")
OPENSEARCH_USER = os.getenv("OPENSEARCH_USER", "admin")
OPENSEARCH_PASSWORD = os.getenv("OPENSEARCH_PASSWORD", "SecretPassword")

# Neo4j åœ–å½¢è³‡æ–™åº«é…ç½®
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "wazuh-graph-2024")

# å¤§å‹èªè¨€æ¨¡å‹é…ç½®
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "anthropic").lower()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# åˆå§‹åŒ– OpenSearch éåŒæ­¥å®¢æˆ¶ç«¯
client = AsyncOpenSearch(
    hosts=[OPENSEARCH_URL],
    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),
    use_ssl=True,
    verify_certs=False,
    ssl_show_warn=False,
    connection_class=AsyncHttpConnection
)

# åˆå§‹åŒ– Neo4j åœ–å½¢è³‡æ–™åº«å®¢æˆ¶ç«¯
neo4j_driver = None
if NEO4J_AVAILABLE:
    try:
        neo4j_driver = AsyncGraphDatabase.driver(
            NEO4J_URI,
            auth=(NEO4J_USER, NEO4J_PASSWORD)
        )
        logger.info(f"Neo4j driver initialized: {NEO4J_URI}")
    except Exception as e:
        logger.warning(f"Failed to initialize Neo4j driver: {str(e)}")
        neo4j_driver = None
else:
    logger.warning("Neo4j driver not available - graph persistence disabled")

def get_llm():
    """
    æ ¹æ“šç’°å¢ƒé…ç½®åˆå§‹åŒ–å¤§å‹èªè¨€æ¨¡å‹
    
    æ”¯æ´çš„æä¾›å•†ï¼š
    - gemini: Google Gemini 1.5 Flash æ¨¡å‹
    - anthropic: Anthropic Claude 3 Haiku æ¨¡å‹
    
    Returns:
        ChatModel: é…ç½®å®Œæˆçš„èªè¨€æ¨¡å‹å¯¦ä¾‹
        
    Raises:
        ValueError: ç•¶æä¾›å•†ä¸æ”¯æ´æˆ– API é‡‘é‘°æœªè¨­å®šæ™‚
    """
    logger.info(f"æ­£åœ¨åˆå§‹åŒ– LLM æä¾›å•†: {LLM_PROVIDER}")
    
    if LLM_PROVIDER == 'gemini':
        if not GEMINI_API_KEY:
            raise ValueError("LLM_PROVIDER è¨­ç‚º 'gemini' ä½† GEMINI_API_KEY æœªè¨­å®š")
        return ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=GEMINI_API_KEY)
    
    elif LLM_PROVIDER == 'anthropic':
        if not ANTHROPIC_API_KEY:
            raise ValueError("LLM_PROVIDER è¨­ç‚º 'anthropic' ä½† ANTHROPIC_API_KEY æœªè¨­å®š")
        return ChatAnthropic(model="claude-3-haiku-20240307", anthropic_api_key=ANTHROPIC_API_KEY)
    
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„ LLM_PROVIDER: {LLM_PROVIDER}ã€‚è«‹é¸æ“‡ 'gemini' æˆ– 'anthropic'")

# åˆå§‹åŒ– LangChain çµ„ä»¶
llm = get_llm()

# Stage 4: GraphRAG prompt template for graph-native security analysis
graphrag_prompt_template = ChatPromptTemplate.from_template(
    """You are a senior security analyst with expertise in graph-based threat intelligence. Analyze the new Wazuh alert by interpreting the provided threat context graph.

    **Threat Context Graph (Simplified Cypher Path Notation):**
    {graph_context}

    **æ–° Wazuh è­¦å ±åˆ†æ:**
    {alert_summary}

    **ä½ çš„åˆ†æä»»å‹™:**
    1.  ç¸½çµæ–°äº‹ä»¶ã€‚
    2.  **è§£è®€å¨è„…åœ–**: æè¿°æ”»æ“Šè·¯å¾‘ã€é—œè¯å¯¦é«”ï¼Œä»¥åŠæ½›åœ¨çš„æ©«å‘ç§»å‹•è·¡è±¡ã€‚
    3.  åŸºæ–¼åœ–ä¸­æ­ç¤ºçš„æ”»æ“Šæ¨¡å¼è©•ä¼°é¢¨éšªç­‰ç´šã€‚
    4.  æä¾›åŸºæ–¼åœ–å½¢é—œè¯çš„ã€æ›´å…·é«”çš„æ‡‰å°å»ºè­°ã€‚

    **ä½ çš„æ·±åº¦æœƒè¨ºå ±å‘Š:**
    """
)

# Enhanced GraphRAG prompt template with comprehensive graph context
enhanced_graphrag_prompt_template = ChatPromptTemplate.from_template(
    """You are a senior cyber security analyst with expertise in graph-based threat hunting and advanced persistent threat (APT) analysis. Analyze the new Wazuh alert below using the comprehensive graph-native intelligence gathered from the security knowledge graph.

    **ğŸ”— Threat Context Graph (Simplified Cypher Path Notation):**
    {graph_context}

    **ğŸ”„ æ©«å‘ç§»å‹•æª¢æ¸¬ (Lateral Movement Detection):**
    {lateral_movement_analysis}

    **â° æ™‚é–“åºåˆ—é—œè¯ (Temporal Correlation):**
    {temporal_correlation}

    **ğŸŒ IP ä¿¡è­½åˆ†æ (IP Reputation Analysis):**
    {ip_reputation_analysis}

    **ğŸ‘¤ ä½¿ç”¨è€…è¡Œç‚ºåˆ†æ (User Behavior Analysis):**
    {user_behavior_analysis}

    **âš™ï¸ ç¨‹åºåŸ·è¡Œéˆåˆ†æ (Process Chain Analysis):**
    {process_chain_analysis}

    **ğŸ“ æª”æ¡ˆäº¤äº’åˆ†æ (File Interaction Analysis):**
    {file_interaction_analysis}

    **ğŸŒ ç¶²è·¯æ‹“æ’²åˆ†æ (Network Topology Analysis):**
    {network_topology_analysis}

    **âš ï¸ å¨è„…å…¨æ™¯åˆ†æ (Threat Landscape Analysis):**
    {threat_landscape_analysis}

    **ğŸ“Š å‚³çµ±æª¢ç´¢è£œå…… (Traditional Retrieval Supplement):**
    {traditional_supplement}

    **ğŸš¨ ç•¶å‰åˆ†æçš„æ–°è­¦å ±ï¼š**
    {alert_summary}

    **æ‚¨çš„åœ–å½¢åŒ–å¨è„…åˆ†æä»»å‹™ï¼š**
    1. **äº‹ä»¶æ‘˜è¦èˆ‡åˆ†é¡ï¼š** ç°¡è¦ç¸½çµæ–°äº‹ä»¶ï¼Œä¸¦æ ¹æ“šåœ–å½¢ä¸Šä¸‹æ–‡é€²è¡Œå¨è„…åˆ†é¡
    2. **æ”»æ“Šéˆé‡å»ºï¼š** åŸºæ–¼åœ–å½¢é—œè¯è³‡æ–™é‡å»ºå®Œæ•´çš„æ”»æ“Šæ™‚é–“ç·šå’Œè·¯å¾‘
    3. **æ©«å‘ç§»å‹•è©•ä¼°ï¼š** è©•ä¼°æ”»æ“Šè€…çš„æ©«å‘ç§»å‹•èƒ½åŠ›å’Œå·²æ»²é€çš„ç³»çµ±ç¯„åœ
    4. **å¨è„…è¡Œç‚ºè€…ç•«åƒï¼š** åŸºæ–¼æ”»æ“Šæ¨¡å¼ã€IPä¿¡è­½ã€æ™‚é–“æ¨¡å¼åˆ†æå¨è„…è¡Œç‚ºè€…ç‰¹å¾µ
    5. **é¢¨éšªç­‰ç´šè©•ä¼°ï¼š** ç¶œåˆæ‰€æœ‰åœ–å½¢æ™ºèƒ½ï¼Œè©•ä¼°é¢¨éšªç­‰ç´šï¼ˆCritical, High, Medium, Low, Informationalï¼‰
    6. **å½±éŸ¿ç¯„åœåˆ†æï¼š** ç¢ºå®šå—å½±éŸ¿çš„ç³»çµ±ã€ä½¿ç”¨è€…ã€æª”æ¡ˆå’Œç¶²è·¯è³‡æº
    7. **ç·©è§£å»ºè­°ï¼š** æä¾›åŸºæ–¼åœ–å½¢åˆ†æçš„ç²¾ç¢ºç·©è§£å’Œæ‡‰æ€¥éŸ¿æ‡‰å»ºè­°
    8. **æŒçºŒå¨è„…æŒ‡æ¨™ï¼š** è­˜åˆ¥éœ€è¦æŒçºŒç›£æ§çš„å¨è„…æŒ‡æ¨™ï¼ˆIOCs/IOAsï¼‰

    **æ‚¨çš„ GraphRAG å¨è„…åˆ†æå ±å‘Šï¼š**
    """
)

# Legacy prompt template for fallback scenarios
traditional_prompt_template = ChatPromptTemplate.from_template(
    """You are a senior security analyst with expertise in correlating security events with system performance data. Analyze the new Wazuh alert below using the provided multi-source contextual information.

**Historical Similar Alerts:**
{similar_alerts_context}

**Correlated System Metrics:**
{system_metrics_context}

**Process Information:**
{process_context}

**Network Data:**
{network_context}

**Additional Context:**
{additional_context}

**å¾…åˆ†æçš„æ–° Wazuh è­¦å ±ï¼š**
{alert_summary}

**Your Analysis Task:**
1. Briefly summarize the new event.
2. Correlate the alert with system performance data and other contextual information.
3. Assess its risk level (Critical, High, Medium, Low, Informational) considering all available context.
4. Identify any patterns or anomalies by cross-referencing different data sources.
5. Provide actionable recommendations based on the comprehensive analysis.

**Your Comprehensive Triage Report:**
"""
)

def get_analysis_chain(context_data: Dict[str, Any]):
    """
    æ ¹æ“šä¸Šä¸‹æ–‡è³‡æ–™é¡å‹é¸æ“‡é©ç•¶çš„åˆ†æéˆ
    """
    # æª¢æ¸¬æ˜¯å¦ç‚ºåœ–å½¢æª¢ç´¢çµæœ
    graph_indicators = ['attack_paths', 'lateral_movement', 'temporal_sequences']
    has_graph_data = any(context_data.get(indicator) for indicator in graph_indicators)
    
    if has_graph_data:
        logger.info("ğŸ”— Using Enhanced GraphRAG analysis chain with graph context")
        return enhanced_graphrag_prompt_template | llm | StrOutputParser()
    else:
        logger.info("ğŸ“Š Using traditional analysis chain")
        return traditional_prompt_template | llm | StrOutputParser()

# Remove legacy static chain - now using dynamic chain selection

# åˆå§‹åŒ–åµŒå…¥æœå‹™
embedding_service = GeminiEmbeddingService()

# === Stage 3: Enhanced Agentic Context Correlation Implementation ===

def determine_contextual_queries(alert: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Stage 3: Enhanced decision engine that determines what contextual information is needed
    based on the alert type and content using more sophisticated, human-like reasoning.
    
    Args:
        alert: The new alert document from OpenSearch
        
    Returns:
        List of query specifications for different types of contextual data
    """
    queries = []
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    data = alert_source.get('data', {})
    timestamp = alert_source.get('timestamp')
    
    rule_description = rule.get('description', '').lower()
    rule_groups = rule.get('groups', [])
    rule_level = rule.get('level', 0)
    host_name = agent.get('name', '')
    
    logger.info(f"ğŸ¤– AGENTIC DECISION ENGINE: Analyzing alert for contextual needs")
    logger.info(f"   Alert: {rule_description}")
    logger.info(f"   Level: {rule_level}, Host: {host_name}")
    logger.info(f"   Groups: {', '.join(rule_groups)}")
    
    # Default: Always perform k-NN search for similar historical alerts
    queries.append({
        'type': 'vector_similarity',
        'description': 'Similar historical alerts',
        'priority': 'high',
        'parameters': {
            'k': 7,  # Increased for better context
            'include_ai_analysis': True
        }
    })
    logger.info("âœ… DECISION: Adding vector similarity search (always required)")
    
    # Enhanced Resource monitoring correlation rules
    resource_keywords = [
        'high cpu usage', 'excessive ram consumption', 'memory usage', 'memory leak',
        'disk space', 'cpu utilization', 'system overload', 'performance', 
        'resource exhaustion', 'out of memory', 'cpu spike', 'high load'
    ]
    
    if any(keyword in rule_description for keyword in resource_keywords) or 'system' in rule_groups:
        logger.info("ğŸ” DECISION: Resource-related alert detected - correlating with system data")
        
        # Process information query
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Process information from same host',
            'priority': 'high',
            'parameters': {
                'keywords': ['process list', 'top processes', 'running processes', 'ps aux', 'htop'],
                'host': host_name,
                'time_window_minutes': 10,  # Wider window for resource issues
                'timestamp': timestamp
            }
        })
        
        # Memory usage correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Memory usage metrics',
            'priority': 'medium',
            'parameters': {
                'keywords': ['memory usage', 'ram utilization', 'swap usage', 'free memory'],
                'host': host_name,
                'time_window_minutes': 15,
                'timestamp': timestamp
            }
        })
        
        logger.info("   âœ… Added process and memory correlation queries")
    
    # Enhanced Security event correlation rules
    security_keywords = [
        'ssh brute-force', 'web attack', 'authentication failed', 'login attempt',
        'intrusion', 'malware', 'suspicious activity', 'unauthorized access',
        'privilege escalation', 'command injection', 'sql injection',
        'cross-site scripting', 'buffer overflow', 'trojan', 'backdoor'
    ]
    
    security_groups = ['authentication', 'attack', 'malware', 'intrusion_detection', 'web']
    
    if (any(keyword in rule_description for keyword in security_keywords) or 
        any(group in rule_groups for group in security_groups) or 
        rule_level >= 7):  # High-level alerts likely security-related
        
        logger.info("ğŸ›¡ï¸ DECISION: Security event detected - adding comprehensive correlation")
        
        # CPU metrics for detecting resource-intensive attacks
        queries.append({
            'type': 'keyword_time_range',
            'description': 'CPU metrics during security event',
            'priority': 'high',
            'parameters': {
                'keywords': ['cpu usage', 'cpu utilization', 'processor load', 'high cpu'],
                'host': host_name,
                'time_window_minutes': 2,  # Tight window for security correlation
                'timestamp': timestamp
            }
        })
        
        # Network activity correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Network activity during security event',
            'priority': 'high',
            'parameters': {
                'keywords': ['network traffic', 'network io', 'bandwidth', 'packets', 'connections'],
                'host': host_name,
                'time_window_minutes': 3,
                'timestamp': timestamp
            }
        })
        
        # User activity correlation
        queries.append({
            'type': 'keyword_time_range',
            'description': 'User activity correlation',
            'priority': 'medium',
            'parameters': {
                'keywords': ['user login', 'user activity', 'session', 'authentication'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        logger.info("   âœ… Added security event correlation queries (CPU, Network, User)")
    
    # SSH-specific enhanced correlation
    if 'ssh' in rule_description or 'sshd' in rule_description:
        logger.info("ğŸ”‘ DECISION: SSH-related alert - adding SSH-specific correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'SSH connection patterns',
            'priority': 'high',
            'parameters': {
                'keywords': ['ssh connection', 'port 22', 'sshd', 'ssh login', 'ssh session'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        # Look for brute force patterns
        if 'brute' in rule_description or 'failed' in rule_description:
            queries.append({
                'type': 'keyword_time_range',
                'description': 'SSH failure patterns',
                'priority': 'high',
                'parameters': {
                    'keywords': ['ssh failed', 'authentication failure', 'invalid user', 'connection refused'],
                    'host': host_name,
                    'time_window_minutes': 10,  # Wider window for brute force detection
                    'timestamp': timestamp
                }
            })
            logger.info("   âœ… Added SSH brute force correlation")
    
    # Web-related enhanced correlation
    web_indicators = ['web', 'http', 'apache', 'nginx', 'php', 'sql injection', 'xss']
    if any(indicator in rule_description for indicator in web_indicators):
        logger.info("ğŸŒ DECISION: Web-related alert - adding web server correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Web server performance',
            'priority': 'medium',
            'parameters': {
                'keywords': ['apache', 'nginx', 'web server', 'http requests', 'response time'],
                'host': host_name,
                'time_window_minutes': 3,
                'timestamp': timestamp
            }
        })
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'Web access logs',
            'priority': 'high',
            'parameters': {
                'keywords': ['access log', 'http status', 'user agent', 'request uri'],
                'host': host_name,
                'time_window_minutes': 2,
                'timestamp': timestamp
            }
        })
        
        logger.info("   âœ… Added web server correlation queries")
    
    # File system correlation for critical alerts
    if rule_level >= 10 or 'file' in rule_description:
        logger.info("ğŸ“ DECISION: High-level/file-related alert - adding filesystem correlation")
        
        queries.append({
            'type': 'keyword_time_range',
            'description': 'File system activity',
            'priority': 'medium',
            'parameters': {
                'keywords': ['file created', 'file modified', 'file deleted', 'disk usage', 'inode'],
                'host': host_name,
                'time_window_minutes': 5,
                'timestamp': timestamp
            }
        })
        
        logger.info("   âœ… Added filesystem correlation")
    
    # Summary logging
    total_queries = len(queries)
    high_priority = len([q for q in queries if q.get('priority') == 'high'])
    logger.info(f"ğŸ¯ AGENTIC DECISION COMPLETE: Generated {total_queries} contextual queries")
    logger.info(f"   High priority: {high_priority}, Total sources: {total_queries}")
    logger.info(f"   Query types: {', '.join(set(q['type'] for q in queries))}")
    
    return queries

async def execute_retrieval(queries: List[Dict[str, Any]], alert_vector: List[float]) -> Dict[str, Any]:
    """
    Stage 3: Enhanced retrieval function that executes multiple types of queries
    in parallel and aggregates results into a structured context object.
    
    Args:
        queries: List of query specifications from determine_contextual_queries
        alert_vector: Vector representation of the current alert
        
    Returns:
        Dictionary containing aggregated results from all queries
    """
    context_data = {
        'similar_alerts': [],
        'cpu_metrics': [],
        'network_logs': [],
        'process_data': [],
        'ssh_logs': [],
        'web_metrics': [],
        'user_activity': [],
        'memory_metrics': [],
        'filesystem_data': [],
        'additional_context': []
    }
    
    logger.info(f"ğŸ”„ EXECUTING RETRIEVAL: Processing {len(queries)} contextual queries in parallel")
    
    # Sort queries by priority for optimal execution order
    sorted_queries = sorted(queries, key=lambda x: {'high': 0, 'medium': 1, 'low': 2}.get(x.get('priority', 'medium'), 1))
    
    # Step 1: Collect all query tasks without awaiting them
    tasks = []
    for i, query in enumerate(sorted_queries, 1):
        query_type = query['type']
        description = query['description']
        priority = query.get('priority', 'medium')
        parameters = query['parameters']
        
        logger.info(f"   [{i}/{len(queries)}] ğŸ” {priority.upper()}: {description}")
        
        if query_type == 'vector_similarity':
            # K-NN vector search for similar alerts
            tasks.append(execute_vector_search(alert_vector, parameters))
        elif query_type == 'keyword_time_range':
            # Keyword and time-based search
            tasks.append(execute_keyword_time_search(parameters))
        else:
            # Handle unknown query types by adding a dummy coroutine that returns empty list
            async def dummy_query():
                logger.warning(f"Unknown query type: {query_type}")
                return []
            tasks.append(dummy_query())
    
    # Step 2: Execute all tasks in parallel using asyncio.gather
    all_results = []
    if tasks:
        try:
            logger.info(f"   ğŸš€ Executing {len(tasks)} queries in parallel...")
            all_results = await asyncio.gather(*tasks, return_exceptions=True)
            logger.info(f"   âœ… Parallel execution completed")
        except Exception as e:
            logger.error(f"   âŒ Parallel execution failed: {str(e)}")
            # Fallback to empty results if gather fails completely
            all_results = [[] for _ in tasks]
    
    # Step 3: Process results and categorize them based on query descriptions
    for i, query in enumerate(sorted_queries):
        if i >= len(all_results):
            continue
            
        result = all_results[i]
        query_type = query['type']
        description = query['description']
        
        # Handle exceptions in individual tasks
        if isinstance(result, Exception):
            logger.error(f"      âŒ Query failed: {description} with error {str(result)}")
            continue
        
        # Handle non-list results
        if not isinstance(result, list):
            logger.warning(f"      âš ï¸ Query returned non-list result: {description}")
            continue
        
        # Categorize results based on query type and description
        if query_type == 'vector_similarity':
            context_data['similar_alerts'].extend(result)
            logger.info(f"      âœ… Found {len(result)} similar alerts")
        elif query_type == 'keyword_time_range':
            # Enhanced categorization based on description keywords
            description_lower = description.lower()
            if 'cpu' in description_lower:
                context_data['cpu_metrics'].extend(result)
            elif 'network' in description_lower:
                context_data['network_logs'].extend(result)
            elif 'process' in description_lower:
                context_data['process_data'].extend(result)
            elif 'ssh' in description_lower:
                context_data['ssh_logs'].extend(result)
            elif 'web' in description_lower:
                context_data['web_metrics'].extend(result)
            elif 'user' in description_lower:
                context_data['user_activity'].extend(result)
            elif 'memory' in description_lower:
                context_data['memory_metrics'].extend(result)
            elif 'file' in description_lower:
                context_data['filesystem_data'].extend(result)
            else:
                context_data['additional_context'].extend(result)
            
            logger.info(f"      âœ… Found {len(result)} contextual records")
    
    # Enhanced retrieval summary
    total_results = sum(len(results) for results in context_data.values())
    logger.info(f"ğŸ“Š RETRIEVAL SUMMARY: {total_results} total contextual records (parallel execution)")
    for category, results in context_data.items():
        if results:
            logger.info(f"   {category}: {len(results)} records")
    
    return context_data

async def execute_vector_search(alert_vector: List[float], parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Execute k-NN vector similarity search for historical alerts.
    
    Args:
        alert_vector: Vector representation of the current alert
        parameters: Search parameters including k and filters
        
    Returns:
        List of similar alert documents
    """
    try:
        k = parameters.get('k', 5)
        include_ai_analysis = parameters.get('include_ai_analysis', True)
        
        # Build k-NN search query
        knn_search_body = {
            "size": k,
            "query": {
                "bool": {
                    "must": [
                        {
                            "knn": {
                                "alert_vector": {
                                    "vector": alert_vector,
                                    "k": k
                                }
                            }
                        }
                    ]
                }
            },
            "_source": ["rule", "agent", "ai_analysis", "timestamp", "data"]
        }
        
        # Add filter for alerts with AI analysis if requested
        if include_ai_analysis:
            if "filter" not in knn_search_body["query"]["bool"]:
                knn_search_body["query"]["bool"]["filter"] = []
            knn_search_body["query"]["bool"]["filter"].append(
                {"exists": {"field": "ai_analysis"}}
            )
        
        response = await client.search(
            index="wazuh-alerts-*",
            body=knn_search_body
        )
        
        similar_alerts = response.get('hits', {}).get('hits', [])
        return similar_alerts
        
    except Exception as e:
        logger.error(f"Vector search failed: {str(e)}")
        return []

async def execute_keyword_time_search(parameters: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Enhanced keyword and time-range search for system metrics and logs.
    
    Args:
        parameters: Search parameters including keywords, host, and time window
        
    Returns:
        List of matching documents
    """
    try:
        keywords = parameters.get('keywords', [])
        host = parameters.get('host', '')
        time_window_minutes = parameters.get('time_window_minutes', 5)
        timestamp = parameters.get('timestamp')
        
        if not timestamp:
            logger.warning("No timestamp provided for time-range search")
            return []
        
        # Parse timestamp and calculate time range
        if isinstance(timestamp, str):
            alert_time = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
        else:
            alert_time = datetime.utcnow()
        
        start_time = alert_time - timedelta(minutes=time_window_minutes)
        end_time = alert_time + timedelta(minutes=time_window_minutes)
        
        # Build enhanced keyword and time-range query
        search_body = {
            "size": 15,  # Increased for better context
            "query": {
                "bool": {
                    "should": [  # Using should for better matching flexibility
                        {
                            "multi_match": {
                                "query": " ".join(keywords),
                                "fields": ["rule.description^2", "data.*", "full_log", "location"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        {
                            "terms": {
                                "rule.description.keyword": keywords
                            }
                        }
                    ],
                    "filter": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_time.isoformat(),
                                    "lte": end_time.isoformat()
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "sort": [
                {"timestamp": {"order": "desc"}},
                {"_score": {"order": "desc"}}
            ]
        }
        
        # Add host filter if specified
        if host:
            search_body["query"]["bool"]["filter"].append({
                "term": {"agent.name.keyword": host}
            })
        
        response = await client.search(
            index="wazuh-alerts-*",
            body=search_body
        )
        
        results = response.get('hits', {}).get('hits', [])
        return results
        
    except Exception as e:
        logger.error(f"Keyword/time search failed: {str(e)}")
        return []

def format_multi_source_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    Stage 3: Enhanced formatting of multi-source context data for LLM consumption.
    
    Args:
        context_data: Aggregated context from execute_retrieval
        
    Returns:
        Dictionary with formatted context strings for each category
    """
    formatted_context = {}
    
    # Format similar alerts with enhanced details
    similar_alerts = context_data.get('similar_alerts', [])
    if similar_alerts:
        context_parts = []
        for i, alert in enumerate(similar_alerts, 1):
            source = alert.get('_source', {})
            rule = source.get('rule', {})
            agent = source.get('agent', {})
            ai_analysis = source.get('ai_analysis', {})
            
            # Extract risk level from previous analysis if available
            prev_analysis = ai_analysis.get('triage_report', '')
            risk_level = "Unknown"
            for level in ['Critical', 'High', 'Medium', 'Low']:
                if level.lower() in prev_analysis.lower():
                    risk_level = level
                    break
            
            context_part = f"""
{i}. **Alert:** {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')})
   **Host:** {agent.get('name', 'Unknown')} | **Time:** {source.get('timestamp', 'Unknown')}
   **Previous Risk Assessment:** {risk_level}
   **Similarity Score:** {alert.get('_score', 'N/A'):.3f}
   **Analysis Preview:** {prev_analysis[:200]}..."""
            context_parts.append(context_part)
        formatted_context['similar_alerts_context'] = "\n".join(context_parts)
    else:
        formatted_context['similar_alerts_context'] = "No similar historical alerts found."
    
    # Format system metrics with enhanced correlation info
    cpu_metrics = context_data.get('cpu_metrics', [])
    memory_metrics = context_data.get('memory_metrics', [])
    all_system_metrics = cpu_metrics + memory_metrics
    
    if all_system_metrics:
        metric_parts = []
        for metric in all_system_metrics[:10]:  # Limit for readability
            source = metric.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            metric_parts.append(f"- {timestamp}: {rule.get('description', 'System metric')}")
        formatted_context['system_metrics_context'] = "\n".join(metric_parts)
    else:
        formatted_context['system_metrics_context'] = "No correlated system metrics found."
    
    # Format process data
    process_data = context_data.get('process_data', [])
    if process_data:
        process_parts = []
        for proc in process_data[:8]:
            source = proc.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            process_parts.append(f"- {timestamp}: {rule.get('description', 'Process info')}")
        formatted_context['process_context'] = "\n".join(process_parts)
    else:
        formatted_context['process_context'] = "No process information found."
    
    # Format network data with enhanced details
    network_logs = context_data.get('network_logs', [])
    ssh_logs = context_data.get('ssh_logs', [])
    all_network_data = network_logs + ssh_logs
    
    if all_network_data:
        network_parts = []
        for net in all_network_data[:10]:
            source = net.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            data = source.get('data', {})
            
            # Extract relevant network details
            details = []
            if data.get('srcip'):
                details.append(f"SRC:{data['srcip']}")
            if data.get('dstip'):
                details.append(f"DST:{data['dstip']}")
            if data.get('srcport'):
                details.append(f"PORT:{data['srcport']}")
            
            detail_str = f" ({', '.join(details)})" if details else ""
            network_parts.append(f"- {timestamp}: {rule.get('description', 'Network activity')}{detail_str}")
        formatted_context['network_context'] = "\n".join(network_parts)
    else:
        formatted_context['network_context'] = "No correlated network data found."
    
    # Format additional context from various sources
    additional_sources = []
    for category in ['web_metrics', 'user_activity', 'filesystem_data', 'additional_context']:
        category_data = context_data.get(category, [])
        if category_data:
            additional_sources.extend(category_data[:5])  # Limit each category
    
    if additional_sources:
        additional_parts = []
        for item in additional_sources:
            source = item.get('_source', {})
            rule = source.get('rule', {})
            timestamp = source.get('timestamp', 'Unknown')
            additional_parts.append(f"- {timestamp}: {rule.get('description', 'Additional context')}")
        formatted_context['additional_context'] = "\n".join(additional_parts)
    else:
        formatted_context['additional_context'] = "No additional contextual data found."
    
    return formatted_context

async def query_new_alerts(limit: int = 10) -> List[Dict[str, Any]]:
    """Query OpenSearch for new unanalyzed alerts"""
    try:
        response = await client.search(
            index="wazuh-alerts-*",
            body={
                "query": {
                    "bool": {
                        "must_not": [{"exists": {"field": "ai_analysis"}}]
                    }
                },
                "sort": [{"timestamp": {"order": "desc"}}],
                "size": limit
            }
        )
        
        alerts = response.get('hits', {}).get('hits', [])
        logger.info(f"Found {len(alerts)} new alerts to process")
        return alerts
        
    except Exception as e:
        logger.error(f"Failed to query new alerts: {str(e)}")
        raise

async def process_single_alert(alert: Dict[str, Any]) -> None:
    """
    Stage 3: Enhanced single alert processing with agentic context correlation.
    
    Processing workflow:
    1. Fetch new alert
    2. Vectorize alert
    3. Decide: Call determine_contextual_queries to get required contextual queries
    4. Retrieve: Call execute_retrieval with query list to fetch all required data
    5. Format: Update context formatting to handle multi-source context
    6. Analyze: Send comprehensive context to LLM
    7. Update: Store results
    8. Graph Persistence: Extract entities and build relationships in graph database (NEW)
    """
    alert_id = alert['_id']
    alert_index = alert['_index']
    alert_source = alert['_source']
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    
    # Step 1: Prepare alert summary
    alert_summary = f"Rule: {rule.get('description', 'N/A')} (Level: {rule.get('level', 'N/A')}) on Host: {agent.get('name', 'N/A')}"
    logger.info(f"Processing alert {alert_id}: {alert_summary}")

    try:
        # Step 2: Vectorize new alert
        logger.info(f"ğŸ”® STEP 2: Vectorizing alert {alert_id}")
        alert_vector = await embedding_service.embed_alert_content(alert_source)
        logger.info(f"   âœ… Alert vectorized (dimension: {len(alert_vector)})")
        
        # Step 3: Decide - Determine graph queries for GraphRAG
        logger.info(f"ğŸ”— STEP 3: GRAPH-NATIVE DECISION - Determining Cypher queries for alert {alert_id}")
        graph_queries = determine_graph_queries(alert)
        
        # Step 4: Execute Graph-Native Retrieval
        logger.info(f"ğŸ“Š STEP 4: GRAPH-NATIVE RETRIEVAL - Executing {len(graph_queries)} Cypher queries for alert {alert_id}")
        context_data = await execute_hybrid_retrieval(alert)
        
        # Step 5: Format - Prepare graph-native context for LLM
        logger.info(f"ğŸ“‹ STEP 5: GRAPH CONTEXT FORMATTING - Preparing graph-native context for alert {alert_id}")
        formatted_context = format_hybrid_context(context_data)
        
        # Log context summary for verification
        total_context_items = sum(len(ctx.split('\n')) for ctx in formatted_context.values() if ctx and "No " not in ctx)
        logger.info(f"   ğŸ“Š Context summary: {total_context_items} total contextual items prepared")
        
        # Step 6: Analyze - Send comprehensive context to LLM using appropriate chain
        logger.info(f"ğŸ¤– STEP 6: GRAPHRAG ANALYSIS - Generating graph-native AI analysis for alert {alert_id}")
        analysis_chain = get_analysis_chain(context_data)
        analysis_result = await analysis_chain.ainvoke({
            "alert_summary": alert_summary,
            **formatted_context
        })
        
        # Extract risk level for logging
        risk_level = "Unknown"
        for level in ['Critical', 'High', 'Medium', 'Low', 'Informational']:
            if level.lower() in analysis_result.lower():
                risk_level = level
                break
        
        logger.info(f"   âœ… AI Analysis generated (Risk: {risk_level}): {analysis_result[:150]}...")
        
        # Step 7: Update - Store results in OpenSearch
        logger.info(f"ğŸ’¾ STEP 7: STORING RESULTS - Updating alert {alert_id} with GraphRAG analysis")
        
        # Enhanced metadata for Stage 4 GraphRAG
        context_metadata = {
            # Graph-native metrics
            "attack_paths_count": len(context_data.get('attack_paths', [])),
            "lateral_movement_count": len(context_data.get('lateral_movement', [])),
            "temporal_sequences_count": len(context_data.get('temporal_sequences', [])),
            "ip_reputation_count": len(context_data.get('ip_reputation', [])),
            "user_behavior_count": len(context_data.get('user_behavior', [])),
            "process_chains_count": len(context_data.get('process_chains', [])),
            "file_interactions_count": len(context_data.get('file_interactions', [])),
            "network_topology_count": len(context_data.get('network_topology', [])),
            "threat_landscape_count": len(context_data.get('threat_landscape', [])),
            "correlation_graph_count": len(context_data.get('correlation_graph', [])),
            
            # Traditional supplement metrics (when used)
            "traditional_similar_alerts_count": len(context_data.get('traditional_similar_alerts', [])),
            "traditional_metrics_count": len(context_data.get('traditional_metrics', [])),
            "traditional_logs_count": len(context_data.get('traditional_logs', [])),
            
            # Legacy compatibility
            "similar_alerts_count": len(context_data.get('similar_alerts', [])),
            "cpu_metrics_count": len(context_data.get('cpu_metrics', [])),
            "memory_metrics_count": len(context_data.get('memory_metrics', [])),
            "network_logs_count": len(context_data.get('network_logs', [])),
            "ssh_logs_count": len(context_data.get('ssh_logs', [])),
            "process_data_count": len(context_data.get('process_data', [])),
            "web_metrics_count": len(context_data.get('web_metrics', [])),
            "user_activity_count": len(context_data.get('user_activity', [])),
            "filesystem_data_count": len(context_data.get('filesystem_data', [])),
            "additional_context_count": len(context_data.get('additional_context', []))
        }
        
        update_body = {
            "doc": {
                "ai_analysis": {
                    "triage_report": analysis_result,
                    "provider": LLM_PROVIDER,
                    "timestamp": alert_source.get('timestamp'),
                    "context_sources": len(graph_queries),
                    "extracted_risk_level": risk_level,
                    "stage": "Stage 4 - GraphRAG Analysis",
                    "analysis_method": "Graph-Native Retrieval" if any(context_data.get(k) for k in ['attack_paths', 'lateral_movement']) else "Hybrid Retrieval",
                    **context_metadata
                },
                "alert_vector": alert_vector
            }
        }
        
        await client.update(index=alert_index, id=alert_id, body=update_body)
        
        logger.info(f"ğŸ‰ GRAPHRAG PROCESSING COMPLETE: Alert {alert_id} successfully updated")
        logger.info(f"   ğŸ“ˆ Graph-native correlation metadata stored for future analysis")
        
        # Step 8: Graph Persistence - Extract entities and build relationships (NEW)
        logger.info(f"ğŸ”— STEP 8: GRAPH PERSISTENCE - Building knowledge graph for alert {alert_id}")
        
        try:
            # Extract graph entities from alert and context
            graph_entities = await extract_graph_entities(alert, context_data, analysis_result)
            logger.info(f"   ğŸ” Extracted {len(graph_entities)} entities for graph database")
            
            # Build relationships between entities
            graph_relationships = await build_graph_relationships(graph_entities, alert, context_data)
            logger.info(f"   ğŸ”— Built {len(graph_relationships)} relationships for graph database")
            
            # Persist to graph database (Neo4j)
            graph_persistence_result = await persist_to_graph_database(graph_entities, graph_relationships, alert_id)
            
            if graph_persistence_result['success']:
                logger.info(f"   âœ… Graph persistence successful: {graph_persistence_result['nodes_created']} nodes, {graph_persistence_result['relationships_created']} relationships")
                
                # Update alert with graph metadata
                graph_metadata = {
                    "graph_entities_count": len(graph_entities),
                    "graph_relationships_count": len(graph_relationships),
                    "graph_persistence_timestamp": graph_persistence_result['timestamp'],
                    "graph_node_ids": graph_persistence_result.get('node_ids', [])
                }
                
                # Add graph metadata to the alert
                graph_update_body = {
                    "doc": {
                        "ai_analysis.graph_metadata": graph_metadata
                    }
                }
                
                await client.update(index=alert_index, id=alert_id, body=graph_update_body)
                logger.info(f"   ğŸ“Š Graph metadata added to alert {alert_id}")
                
            else:
                logger.warning(f"   âš ï¸ Graph persistence failed: {graph_persistence_result.get('error', 'Unknown error')}")
                
        except Exception as graph_error:
            logger.error(f"   âŒ Graph persistence error for alert {alert_id}: {str(graph_error)}")
            # Graph persistence failure should not break the main pipeline
            logger.info(f"   ğŸ”„ Main processing pipeline continues despite graph persistence failure")
        
    except Exception as e:
        logger.error(f"âŒ PROCESSING FAILED for alert {alert_id}: {str(e)}")
        logger.error(f"   Stack trace: {traceback.format_exc()}")
        raise

async def triage_new_alerts():
    """Main alert triage task with Stage 3 agentic context correlation"""
    print("ğŸš€ === STAGE 3 AGENTIC CONTEXT CORRELATION TRIAGE JOB EXECUTING ===")
    logger.info(f"ğŸ”¬ Analyzing alerts with {LLM_PROVIDER} model and enhanced agentic context correlation...")
    
    try:
        # Query new alerts
        alerts = await query_new_alerts(limit=10)
        
        if not alerts:
            print("ğŸ“­ --- No new alerts found ---")
            logger.info("No new alerts requiring agentic analysis")
            return
            
        logger.info(f"ğŸ¯ Found {len(alerts)} new alerts to process with agentic context correlation")
        
        # Process each alert with enhanced agentic workflow
        successful_processing = 0
        failed_processing = 0
        
        for i, alert in enumerate(alerts, 1):
            alert_id = alert['_id']
            rule_desc = alert.get('_source', {}).get('rule', {}).get('description', 'Unknown')
            
            try:
                logger.info(f"ğŸ”„ [{i}/{len(alerts)}] Processing alert: {alert_id}")
                logger.info(f"   Rule: {rule_desc}")
                
                await process_single_alert(alert)
                
                successful_processing += 1
                print(f"âœ… [{i}/{len(alerts)}] Successfully processed alert {alert_id}")
                logger.info(f"âœ… Alert {alert_id} processing completed successfully")
                
            except Exception as e:
                failed_processing += 1
                print(f"âŒ [{i}/{len(alerts)}] Failed to process alert {alert_id}: {str(e)}")
                logger.error(f"âŒ Alert {alert_id} processing failed: {str(e)}")
                continue
        
        # Summary logging
        print(f"ğŸ“Š === AGENTIC TRIAGE BATCH SUMMARY ===")
        print(f"   âœ… Successful: {successful_processing}")
        print(f"   âŒ Failed: {failed_processing}")
        print(f"   ğŸ“ˆ Success Rate: {(successful_processing/len(alerts)*100):.1f}%")
        
        logger.info(f"ğŸ¯ Agentic triage batch completed: {successful_processing}/{len(alerts)} successful")
            
    except Exception as e:
        print(f"ğŸ’¥ !!! CRITICAL ERROR IN AGENTIC TRIAGE JOB !!!")
        logger.error(f"Critical error during agentic triage: {e}", exc_info=True)
        traceback.print_exc()

# === FastAPI æ‡‰ç”¨ç¨‹å¼èˆ‡æ’ç¨‹å™¨ ===

app = FastAPI(title="Wazuh AI Triage Agent - Stage 3 Agentic Context Correlation")

scheduler = AsyncIOScheduler()

@app.on_event("startup")
async def startup_event():
    logging.info("AI Agent with Stage 3 Agentic Context Correlation starting up...")
    scheduler.add_job(triage_new_alerts, 'interval', seconds=60, id='agentic_triage_job', misfire_grace_time=30)
    scheduler.start()
    logging.info("Scheduler started. Agentic Context Correlation Triage job scheduled.")

@app.get("/")
def read_root():
    """æ ¹ç«¯é» - è¿”å›æœå‹™ç‹€æ…‹è³‡è¨Š"""
    return {
        "status": "AI Triage Agent with Agentic Context Correlation is running", 
        "scheduler_status": str(scheduler.get_jobs()),
        "stage": "Stage 3 - Agentic Context Correlation",
        "features": [
            "Dynamic contextual query generation",
            "Multi-source data retrieval",
            "Cross-referential analysis",
            "Enhanced decision engine"
        ]
    }

@app.get("/health")
async def health_check():
    """
    è©³ç´°å¥åº·æª¢æŸ¥ç«¯é»
    
    æä¾›å®Œæ•´çš„ç³»çµ±ç‹€æ…‹è³‡è¨Šï¼ŒåŒ…æ‹¬ï¼š
    - OpenSearch é€£ç·šç‹€æ…‹
    - åµŒå…¥æœå‹™å¯ç”¨æ€§
    - å‘é‡åŒ–çµ±è¨ˆè³‡æ–™
    - ç³»çµ±é…ç½®è³‡è¨Š
    
    Returns:
        Dict: è©³ç´°çš„å¥åº·æª¢æŸ¥å ±å‘Š
    """
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "3.0", # Updated version
        "stage": "Stage 3 - Agentic Context Correlation" # Updated stage
    }
    
    try:
        # æª¢æŸ¥ OpenSearch é€£ç·šç‹€æ…‹
        cluster_health = await client.cluster.health()
        health_status["opensearch"] = {
            "status": "connected",
            "cluster_name": cluster_health.get("cluster_name", "unknown"),
            "cluster_status": cluster_health.get("status", "unknown"),
            "number_of_nodes": cluster_health.get("number_of_nodes", 0)
        }
        
        # æª¢æŸ¥åµŒå…¥æœå‹™ç‹€æ…‹
        embedding_test = await embedding_service.test_connection()
        health_status["embedding_service"] = {
            "status": "working" if embedding_test else "failed",
            "model": embedding_service.model_name,
            "dimension": embedding_service.get_vector_dimension()
        }
        
        # æª¢æŸ¥å‘é‡åŒ–è­¦å ±çµ±è¨ˆ
        vectorized_count_response = await client.count(
            index="wazuh-alerts-*",
            body={"query": {"exists": {"field": "alert_vector"}}}
        )
        
        # æª¢æŸ¥ Stage 3 åˆ†æçµ±è¨ˆ
        stage3_analysis_response = await client.count(
            index="wazuh-alerts-*",
            body={"query": {"bool": {"must": [
                {"exists": {"field": "ai_analysis"}},
                {"term": {"ai_analysis.stage.keyword": "Stage 3 - Agentic Context Correlation"}}
            ]}}}
        )
        
        total_alerts_response = await client.count(index="wazuh-alerts-*")
        
        health_status["processing_stats"] = {
            "vectorized_alerts": vectorized_count_response.get("count", 0),
            "stage3_analyzed_alerts": stage3_analysis_response.get("count", 0),
            "total_alerts": total_alerts_response.get("count", 0),
            "vectorization_rate": round(
                (vectorized_count_response.get("count", 0) / max(total_alerts_response.get("count", 1), 1)) * 100, 2
            ),
            "stage3_analysis_rate": round(
                (stage3_analysis_response.get("count", 0) / max(total_alerts_response.get("count", 1), 1)) * 100, 2
            )
        }
        
        # LLM é…ç½®è³‡è¨Š
        health_status["llm_config"] = {
            "provider": LLM_PROVIDER,
            "model_configured": True
        }
        
        # æ’ç¨‹å™¨ç‹€æ…‹
        jobs = scheduler.get_jobs()
        health_status["scheduler"] = {
            "status": "running" if jobs else "no_jobs",
            "active_jobs": len(jobs),
            "next_run": str(jobs[0].next_run_time) if jobs else None
        }
        
    except Exception as e:
        health_status["status"] = "unhealthy"
        health_status["error"] = str(e)
        logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {str(e)}")
    
    return health_status

# ==================== åœ–å½¢åŒ–æŒä¹…å±¤å‡½æ•¸ (GraphRAG Stage 4 æº–å‚™) ====================

async def extract_graph_entities(alert: Dict[str, Any], context_data: Dict[str, Any], analysis_result: str) -> List[Dict[str, Any]]:
    """
    å¾è­¦å ±ã€ä¸Šä¸‹æ–‡è³‡æ–™å’Œåˆ†æçµæœä¸­æå–åœ–å½¢å¯¦é«”
    
    Args:
        alert: åŸå§‹è­¦å ±è³‡æ–™
        context_data: ä¸Šä¸‹æ–‡é—œè¯è³‡æ–™
        analysis_result: LLM åˆ†æçµæœ
    
    Returns:
        æå–çš„åœ–å½¢å¯¦é«”åˆ—è¡¨
    """
    entities = []
    alert_source = alert.get('_source', {})
    
    # 1. è­¦å ±å¯¦é«” (Alert Entity)
    alert_entity = {
        'type': 'Alert',
        'id': alert['_id'],
        'properties': {
            'alert_id': alert['_id'],
            'timestamp': alert_source.get('timestamp'),
            'rule_id': alert_source.get('rule', {}).get('id'),
            'rule_description': alert_source.get('rule', {}).get('description'),
            'rule_level': alert_source.get('rule', {}).get('level'),
            'rule_groups': alert_source.get('rule', {}).get('groups', []),
            'risk_level': _extract_risk_level_from_analysis(analysis_result),
            'triage_score': _calculate_triage_score(alert_source, analysis_result)
        }
    }
    entities.append(alert_entity)
    
    # 2. ä¸»æ©Ÿå¯¦é«” (Host Entity)
    agent = alert_source.get('agent', {})
    if agent.get('id') or agent.get('name'):
        host_entity = {
            'type': 'Host',
            'id': f"host_{agent.get('id', agent.get('name', 'unknown'))}",
            'properties': {
                'agent_id': agent.get('id'),
                'agent_name': agent.get('name'),
                'agent_ip': agent.get('ip'),
                'operating_system': _extract_os_info(alert_source)
            }
        }
        entities.append(host_entity)
    
    # 3. IP ä½å€å¯¦é«” (IP Address Entities)
    ip_addresses = _extract_ip_addresses(alert_source)
    for ip_info in ip_addresses:
        ip_entity = {
            'type': 'IPAddress',
            'id': f"ip_{ip_info['address']}",
            'properties': {
                'address': ip_info['address'],
                'type': ip_info['type'],  # source, destination, internal
                'geolocation': ip_info.get('geo'),
                'is_private': _is_private_ip(ip_info['address'])
            }
        }
        entities.append(ip_entity)
    
    # 4. ä½¿ç”¨è€…å¯¦é«” (User Entities)
    users = _extract_user_info(alert_source)
    for user_info in users:
        user_entity = {
            'type': 'User',
            'id': f"user_{user_info['name']}",
            'properties': {
                'username': user_info['name'],
                'user_type': user_info.get('type', 'unknown'),
                'authentication_method': user_info.get('auth_method')
            }
        }
        entities.append(user_entity)
    
    # 5. ç¨‹åºå¯¦é«” (Process Entities)
    processes = _extract_process_info(alert_source, context_data)
    for process_info in processes:
        process_entity = {
            'type': 'Process',
            'id': f"process_{process_info.get('pid', 'unknown')}_{process_info.get('name', 'unknown')}",
            'properties': {
                'process_name': process_info.get('name'),
                'process_id': process_info.get('pid'),
                'command_line': process_info.get('cmdline'),
                'parent_process': process_info.get('ppid'),
                'hash': process_info.get('hash')
            }
        }
        entities.append(process_entity)
    
    # 6. æª”æ¡ˆå¯¦é«” (File Entities)
    files = _extract_file_info(alert_source)
    for file_info in files:
        file_entity = {
            'type': 'File',
            'id': f"file_{hash(file_info['path'])}",
            'properties': {
                'file_path': file_info['path'],
                'file_name': file_info.get('name'),
                'file_size': file_info.get('size'),
                'file_hash': file_info.get('hash'),
                'file_permissions': file_info.get('permissions')
            }
        }
        entities.append(file_entity)
    
    # 7. å¨è„…å¯¦é«” (å¾åˆ†æçµæœæå–)
    threat_indicators = _extract_threat_indicators(analysis_result)
    for threat in threat_indicators:
        threat_entity = {
            'type': 'ThreatIndicator',
            'id': f"threat_{uuid.uuid4().hex[:8]}",
            'properties': {
                'indicator_type': threat['type'],
                'indicator_value': threat['value'],
                'confidence': threat.get('confidence', 0.5),
                'mitre_technique': threat.get('mitre_technique')
            }
        }
        entities.append(threat_entity)
    
    logger.info(f"Extracted {len(entities)} entities: {dict(zip(*zip(*[(e['type'], 1) for e in entities])))}")
    return entities

async def build_graph_relationships(entities: List[Dict[str, Any]], alert: Dict[str, Any], context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    æ ¹æ“šå¯¦é«”å’Œä¸Šä¸‹æ–‡è³‡æ–™å»ºç«‹åœ–å½¢é—œä¿‚
    
    Args:
        entities: æå–çš„å¯¦é«”åˆ—è¡¨
        alert: åŸå§‹è­¦å ±è³‡æ–™
        context_data: ä¸Šä¸‹æ–‡é—œè¯è³‡æ–™
    
    Returns:
        å¯¦é«”é–“çš„é—œä¿‚åˆ—è¡¨
    """
    relationships = []
    entity_by_id = {entity['id']: entity for entity in entities}
    entity_by_type = {}
    
    # æŒ‰é¡å‹çµ„ç¹”å¯¦é«”
    for entity in entities:
        entity_type = entity['type']
        if entity_type not in entity_by_type:
            entity_by_type[entity_type] = []
        entity_by_type[entity_type].append(entity)
    
    # 1. è­¦å ±è§¸ç™¼é—œä¿‚ (Alert -> Host)
    alert_entities = entity_by_type.get('Alert', [])
    host_entities = entity_by_type.get('Host', [])
    
    for alert_entity in alert_entities:
        for host_entity in host_entities:
            relationship = {
                'type': 'TRIGGERED_ON',
                'source_id': alert_entity['id'],
                'target_id': host_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'severity': alert.get('_source', {}).get('rule', {}).get('level')
                }
            }
            relationships.append(relationship)
    
    # 2. ä¾†æº IP é—œä¿‚ (Alert -> IPAddress)
    ip_entities = entity_by_type.get('IPAddress', [])
    for alert_entity in alert_entities:
        for ip_entity in ip_entities:
            if ip_entity['properties'].get('type') == 'source':
                relationship = {
                    'type': 'HAS_SOURCE_IP',
                    'source_id': alert_entity['id'],
                    'target_id': ip_entity['id'],
                    'properties': {
                        'timestamp': alert.get('_source', {}).get('timestamp')
                    }
                }
                relationships.append(relationship)
    
    # 3. ä½¿ç”¨è€…åƒèˆ‡é—œä¿‚ (Alert -> User)
    user_entities = entity_by_type.get('User', [])
    for alert_entity in alert_entities:
        for user_entity in user_entities:
            relationship = {
                'type': 'INVOLVES_USER',
                'source_id': alert_entity['id'],
                'target_id': user_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'action_type': _determine_user_action_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 4. ç¨‹åºåŸ·è¡Œé—œä¿‚ (Alert -> Process)
    process_entities = entity_by_type.get('Process', [])
    for alert_entity in alert_entities:
        for process_entity in process_entities:
            relationship = {
                'type': 'INVOLVES_PROCESS',
                'source_id': alert_entity['id'],
                'target_id': process_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp')
                }
            }
            relationships.append(relationship)
    
    # 5. æª”æ¡ˆå­˜å–é—œä¿‚ (Alert -> File)
    file_entities = entity_by_type.get('File', [])
    for alert_entity in alert_entities:
        for file_entity in file_entities:
            relationship = {
                'type': 'ACCESSES_FILE',
                'source_id': alert_entity['id'],
                'target_id': file_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'access_type': _determine_file_access_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 6. é¡ä¼¼è­¦å ±é—œä¿‚ (åŸºæ–¼ä¸Šä¸‹æ–‡è³‡æ–™)
    similar_alerts = context_data.get('similar_alerts', [])
    for similar_alert in similar_alerts[:5]:  # é™åˆ¶é—œä¿‚æ•¸é‡
        similar_alert_id = similar_alert.get('_id')
        if similar_alert_id:
            for alert_entity in alert_entities:
                relationship = {
                    'type': 'SIMILAR_TO',
                    'source_id': alert_entity['id'],
                    'target_id': f"alert_{similar_alert_id}",  # å‡è¨­è©²è­¦å ±å·²åœ¨åœ–ä¸­
                    'properties': {
                        'similarity_score': similar_alert.get('_score', 0.0),
                        'correlation_type': 'vector_similarity'
                    }
                }
                relationships.append(relationship)
    
    # 7. æ™‚é–“åºåˆ—é—œä¿‚ (Temporal Relationships)
    # æ ¹æ“šæ™‚é–“æˆ³å»ºç«‹ PRECEDES é—œä¿‚
    if len(alert_entities) > 1:
        sorted_alerts = sorted(alert_entities, key=lambda x: x['properties'].get('timestamp', ''))
        for i in range(len(sorted_alerts) - 1):
            relationship = {
                'type': 'PRECEDES',
                'source_id': sorted_alerts[i]['id'],
                'target_id': sorted_alerts[i + 1]['id'],
                'properties': {
                    'time_difference': _calculate_time_difference(
                        sorted_alerts[i]['properties'].get('timestamp'),
                        sorted_alerts[i + 1]['properties'].get('timestamp')
                    )
                }
            }
            relationships.append(relationship)
    
    logger.info(f"Built {len(relationships)} relationships")
    return relationships

async def persist_to_graph_database(entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]], alert_id: str) -> Dict[str, Any]:
    """
    å°‡å¯¦é«”å’Œé—œä¿‚æŒä¹…åŒ–åˆ° Neo4j åœ–å½¢è³‡æ–™åº«
    
    Args:
        entities: è¦å­˜å„²çš„å¯¦é«”åˆ—è¡¨
        relationships: è¦å­˜å„²çš„é—œä¿‚åˆ—è¡¨
        alert_id: è­¦å ± ID
    
    Returns:
        æŒä¹…åŒ–çµæœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œçµ±è¨ˆè³‡è¨Š
    """
    if not neo4j_driver:
        return {
            'success': False,
            'error': 'Neo4j driver not available',
            'nodes_created': 0,
            'relationships_created': 0
        }
    
    try:
        async with neo4j_driver.session() as session:
            # å­˜å„²ç¯€é»
            nodes_created = 0
            node_ids = []
            
            for entity in entities:
                # ä½¿ç”¨ MERGE ä¾†é¿å…é‡è¤‡ç¯€é»
                cypher_query = f"""
                MERGE (n:{entity['type']} {{id: $entity_id}})
                SET n += $properties
                RETURN n.id as node_id
                """
                
                result = await session.run(
                    cypher_query,
                    entity_id=entity['id'],
                    properties=entity['properties']
                )
                
                record = await result.single()
                if record:
                    node_ids.append(record['node_id'])
                    nodes_created += 1
            
            # å­˜å„²é—œä¿‚
            relationships_created = 0
            
            for relationship in relationships:
                # ä½¿ç”¨ MERGE ä¾†é¿å…é‡è¤‡é—œä¿‚
                cypher_query = f"""
                MATCH (source {{id: $source_id}})
                MATCH (target {{id: $target_id}})
                MERGE (source)-[r:{relationship['type']}]->(target)
                SET r += $properties
                RETURN r
                """
                
                result = await session.run(
                    cypher_query,
                    source_id=relationship['source_id'],
                    target_id=relationship['target_id'],
                    properties=relationship.get('properties', {})
                )
                
                if await result.peek():
                    relationships_created += 1
            
            # å»ºç«‹ç´¢å¼• (å¦‚æœä¸å­˜åœ¨)
            index_queries = [
                "CREATE INDEX alert_timestamp_idx IF NOT EXISTS FOR (a:Alert) ON (a.timestamp)",
                "CREATE INDEX host_agent_id_idx IF NOT EXISTS FOR (h:Host) ON (h.agent_id)",
                "CREATE INDEX ip_address_idx IF NOT EXISTS FOR (i:IPAddress) ON (i.address)",
                "CREATE INDEX user_name_idx IF NOT EXISTS FOR (u:User) ON (u.username)"
            ]
            
            for index_query in index_queries:
                await session.run(index_query)
            
            return {
                'success': True,
                'nodes_created': nodes_created,
                'relationships_created': relationships_created,
                'node_ids': node_ids,
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Graph persistence error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'nodes_created': 0,
            'relationships_created': 0
        }

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def _extract_risk_level_from_analysis(analysis_result: str) -> str:
    """å¾åˆ†æçµæœä¸­æå–é¢¨éšªç­‰ç´š"""
    risk_levels = ['Critical', 'High', 'Medium', 'Low', 'Informational']
    for level in risk_levels:
        if level.lower() in analysis_result.lower():
            return level
    return 'Unknown'

def _calculate_triage_score(alert_source: Dict, analysis_result: str) -> float:
    """è¨ˆç®—è­¦å ±åˆ†ç´šåˆ†æ•¸"""
    base_score = alert_source.get('rule', {}).get('level', 1) * 10
    
    # æ ¹æ“šåˆ†æçµæœèª¿æ•´åˆ†æ•¸
    if 'critical' in analysis_result.lower():
        return min(base_score * 1.5, 100)
    elif 'high' in analysis_result.lower():
        return min(base_score * 1.2, 100)
    elif 'low' in analysis_result.lower():
        return max(base_score * 0.8, 0)
    
    return base_score

def _extract_os_info(alert_source: Dict) -> str:
    """æå–ä½œæ¥­ç³»çµ±è³‡è¨Š"""
    agent = alert_source.get('agent', {})
    return agent.get('labels', {}).get('os', 'unknown')

def _extract_ip_addresses(alert_source: Dict) -> List[Dict]:
    """æå– IP ä½å€è³‡è¨Š"""
    ips = []
    data = alert_source.get('data', {})
    
    # ä¾†æº IP
    if data.get('srcip'):
        ips.append({
            'address': data['srcip'],
            'type': 'source',
            'geo': data.get('srcgeoip', {})
        })
    
    # ç›®çš„ IP
    if data.get('dstip'):
        ips.append({
            'address': data['dstip'],
            'type': 'destination',
            'geo': data.get('dstgeoip', {})
        })
    
    # Agent IP
    agent = alert_source.get('agent', {})
    if agent.get('ip'):
        ips.append({
            'address': agent['ip'],
            'type': 'internal'
        })
    
    return ips

def _is_private_ip(ip_address: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚ºç§æœ‰ IP ä½å€"""
    import ipaddress
    try:
        ip = ipaddress.ip_address(ip_address)
        return ip.is_private
    except:
        return False

def _extract_user_info(alert_source: Dict) -> List[Dict]:
    """æå–ä½¿ç”¨è€…è³‡è¨Š"""
    users = []
    data = alert_source.get('data', {})
    
    # ä¸»è¦ä½¿ç”¨è€…
    if data.get('user'):
        users.append({
            'name': data['user'],
            'type': 'primary'
        })
    
    # ä¾†æºä½¿ç”¨è€…
    if data.get('srcuser'):
        users.append({
            'name': data['srcuser'],
            'type': 'source'
        })
    
    return users

def _extract_process_info(alert_source: Dict, context_data: Dict) -> List[Dict]:
    """æå–ç¨‹åºè³‡è¨Š"""
    processes = []
    data = alert_source.get('data', {})
    
    # ä¾†è‡ªè­¦å ±çš„ç¨‹åºè³‡è¨Š
    if data.get('process'):
        processes.append({
            'name': data['process'].get('name'),
            'pid': data['process'].get('pid'),
            'cmdline': data['process'].get('cmdline'),
            'ppid': data['process'].get('ppid')
        })
    
    # ä¾†è‡ªä¸Šä¸‹æ–‡çš„ç¨‹åºè³‡è¨Š
    process_data = context_data.get('process_data', [])
    for proc in process_data[:5]:  # é™åˆ¶æ•¸é‡
        if isinstance(proc, dict) and proc.get('_source'):
            proc_source = proc['_source']
            processes.append({
                'name': proc_source.get('data', {}).get('process', {}).get('name'),
                'pid': proc_source.get('data', {}).get('process', {}).get('pid'),
                'cmdline': proc_source.get('data', {}).get('process', {}).get('cmdline')
            })
    
    return [p for p in processes if p.get('name')]  # éæ¿¾ç©ºçš„ç¨‹åº

def _extract_file_info(alert_source: Dict) -> List[Dict]:
    """æå–æª”æ¡ˆè³‡è¨Š"""
    files = []
    data = alert_source.get('data', {})
    
    # æª”æ¡ˆè·¯å¾‘
    if data.get('file'):
        files.append({
            'path': data['file'],
            'name': data['file'].split('/')[-1] if '/' in data['file'] else data['file']
        })
    
    # é¡å¤–çš„æª”æ¡ˆæ¬„ä½
    if data.get('path'):
        files.append({
            'path': data['path'],
            'name': data['path'].split('/')[-1] if '/' in data['path'] else data['path']
        })
    
    return files

def _extract_threat_indicators(analysis_result: str) -> List[Dict]:
    """å¾åˆ†æçµæœä¸­æå–å¨è„…æŒ‡æ¨™"""
    indicators = []
    
    # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼åŒ¹é…
    ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
    domain_pattern = r'\b[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*\b'
    
    # æå– IP ä½å€
    ips = re.findall(ip_pattern, analysis_result)
    for ip in ips[:3]:  # é™åˆ¶æ•¸é‡
        indicators.append({
            'type': 'ip_address',
            'value': ip,
            'confidence': 0.7
        })
    
    # æå–åŸŸåï¼ˆç°¡åŒ–ç‰ˆï¼‰
    words = analysis_result.split()
    for word in words:
        if '.' in word and len(word) > 4 and not word.startswith('.'):
            indicators.append({
                'type': 'domain',
                'value': word,
                'confidence': 0.5
            })
            if len(indicators) >= 5:  # é™åˆ¶æ•¸é‡
                break
    
    return indicators

def _determine_user_action_type(alert: Dict) -> str:
    """ç¢ºå®šä½¿ç”¨è€…å‹•ä½œé¡å‹"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'login' in rule_desc or 'authentication' in rule_desc:
        return 'authentication'
    elif 'ssh' in rule_desc:
        return 'remote_access'
    elif 'file' in rule_desc:
        return 'file_access'
    else:
        return 'unknown'

def _determine_file_access_type(alert: Dict) -> str:
    """ç¢ºå®šæª”æ¡ˆå­˜å–é¡å‹"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'write' in rule_desc or 'modify' in rule_desc:
        return 'write'
    elif 'read' in rule_desc:
        return 'read'
    elif 'delete' in rule_desc:
        return 'delete'
    else:
        return 'access'

def _calculate_time_difference(timestamp1: str, timestamp2: str) -> int:
    """è¨ˆç®—å…©å€‹æ™‚é–“æˆ³ä¹‹é–“çš„å·®ç•°ï¼ˆç§’ï¼‰"""
    try:
        from dateutil import parser
        dt1 = parser.parse(timestamp1)
        dt2 = parser.parse(timestamp2)
        return int(abs((dt2 - dt1).total_seconds()))
    except:
        return 0

@app.on_event("shutdown")
def shutdown_event():
    """æ‡‰ç”¨ç¨‹å¼é—œé–‰äº‹ä»¶è™•ç†å™¨"""
    scheduler.shutdown()
    if neo4j_driver:
        neo4j_driver.close()
        logger.info("Neo4j é€£æ¥å·²é—œé–‰")
    logger.info("æ’ç¨‹å™¨å·²é—œé–‰")

# ç§»é™¤é‡è¤‡çš„mainå‡½æ•¸å®šç¾©

# ==================== Graph-Native æª¢ç´¢å™¨ (Stage 4 Step 3) ====================

async def execute_graph_retrieval(cypher_queries: List[Dict[str, Any]], alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    Graph-Native æª¢ç´¢å™¨ï¼šåŸ·è¡Œ Cypher æŸ¥è©¢ä¾†æª¢ç´¢ç›¸é—œçš„åœ–å½¢å­ç¶²
    é€™æ˜¯ GraphRAG çš„æ ¸å¿ƒæª¢ç´¢å¼•æ“ï¼Œå–ä»£å‚³çµ±çš„å‘é‡èˆ‡é—œéµå­—æœå°‹
    
    Args:
        cypher_queries: å¾ Decision Engine ç”Ÿæˆçš„ Cypher æŸ¥è©¢ä»»å‹™åˆ—è¡¨
        alert: ç•¶å‰è­¦å ±è³‡æ–™
        
    Returns:
        Dictionary åŒ…å«æª¢ç´¢åˆ°çš„åœ–å½¢å­ç¶²å’Œçµæ§‹åŒ–ä¸Šä¸‹æ–‡
    """
    logger.info(f"ğŸ”— GRAPH-NATIVE RETRIEVAL: Processing {len(cypher_queries)} Cypher queries")
    
    context_data = {
        'attack_paths': [],           # æ”»æ“Šè·¯å¾‘å­åœ–
        'lateral_movement': [],       # æ©«å‘ç§»å‹•æ¨¡å¼
        'temporal_sequences': [],     # æ™‚é–“åºåˆ—é—œè¯
        'ip_reputation': [],          # IP ä¿¡è­½åœ–
        'user_behavior': [],          # ä½¿ç”¨è€…è¡Œç‚ºåœ–
        'process_chains': [],         # ç¨‹åºåŸ·è¡Œéˆ
        'file_interactions': [],      # æª”æ¡ˆäº¤äº’åœ–
        'network_topology': [],       # ç¶²è·¯æ‹“æ’²
        'threat_landscape': [],       # å¨è„…å…¨æ™¯
        'correlation_graph': []       # ç›¸é—œæ€§åœ–
    }
    
    if not neo4j_driver:
        logger.warning("Neo4j driver not available - falling back to traditional retrieval")
        # é™ç´šåˆ°å‚³çµ±æª¢ç´¢
        return await _fallback_to_traditional_retrieval(alert)
    
    # æ’åºæŸ¥è©¢ä»¥å„ªåŒ–åŸ·è¡Œé †åº
    sorted_queries = sorted(cypher_queries, key=lambda x: {
        'critical': 0, 'high': 1, 'medium': 2, 'low': 3
    }.get(x.get('priority', 'medium'), 2))
    
    alert_id = alert.get('_id')
    
    async with neo4j_driver.session() as session:
        for i, query_spec in enumerate(sorted_queries, 1):
            query_type = query_spec['type']
            description = query_spec['description']
            priority = query_spec.get('priority', 'medium')
            cypher_query = query_spec['cypher_query']
            parameters = query_spec.get('parameters', {})
            
            # æ³¨å…¥ç•¶å‰è­¦å ± ID åˆ°åƒæ•¸ä¸­
            parameters['alert_id'] = alert_id
            
            try:
                logger.info(f"   [{i}/{len(sorted_queries)}] ğŸ” {priority.upper()}: {description}")
                
                # åŸ·è¡Œ Cypher æŸ¥è©¢
                result = await session.run(cypher_query, parameters)
                records = await result.data()
                
                # æ ¹æ“šæŸ¥è©¢é¡å‹åˆ†é¡çµæœ
                await _categorize_graph_results(query_type, records, context_data)
                
                logger.info(f"      âœ… Graph query returned {len(records)} subgraph components")
                
            except Exception as e:
                logger.error(f"      âŒ Cypher query failed: {str(e)}")
                # è¨˜éŒ„å¤±æ•—çš„æŸ¥è©¢ä»¥ä¾¿å¾ŒçºŒåˆ†æ
                logger.error(f"      Query: {cypher_query[:200]}...")
                continue
    
    # ç”Ÿæˆæª¢ç´¢æ‘˜è¦
    total_components = sum(len(results) for results in context_data.values())
    logger.info(f"ğŸ“Š GRAPH RETRIEVAL SUMMARY: {total_components} total graph components")
    for category, results in context_data.items():
        if results:
            logger.info(f"   {category}: {len(results)} components")
    
    return context_data

async def _categorize_graph_results(query_type: str, records: List[Dict], context_data: Dict[str, Any]):
    """
    æ ¹æ“šæŸ¥è©¢é¡å‹å°‡åœ–å½¢çµæœåˆ†é¡åˆ°é©ç•¶çš„ä¸Šä¸‹æ–‡é¡åˆ¥ä¸­
    
    Args:
        query_type: æŸ¥è©¢é¡å‹ï¼ˆæ”»æ“Šè·¯å¾‘ã€æ©«å‘ç§»å‹•ç­‰ï¼‰
        records: Cypher æŸ¥è©¢è¿”å›çš„è¨˜éŒ„
        context_data: è¦æ›´æ–°çš„ä¸Šä¸‹æ–‡è³‡æ–™å­—å…¸
    """
    if query_type == 'attack_path_analysis':
        context_data['attack_paths'].extend(records)
    elif query_type == 'lateral_movement_detection':
        context_data['lateral_movement'].extend(records)
    elif query_type == 'temporal_correlation':
        context_data['temporal_sequences'].extend(records)
    elif query_type == 'ip_reputation_analysis':
        context_data['ip_reputation'].extend(records)
    elif query_type == 'user_behavior_analysis':
        context_data['user_behavior'].extend(records)
    elif query_type == 'process_chain_analysis':
        context_data['process_chains'].extend(records)
    elif query_type == 'file_interaction_analysis':
        context_data['file_interactions'].extend(records)
    elif query_type == 'network_topology_analysis':
        context_data['network_topology'].extend(records)
    elif query_type == 'threat_landscape_analysis':
        context_data['threat_landscape'].extend(records)
    else:
        # é è¨­åˆ†é¡
        context_data['correlation_graph'].extend(records)

async def _fallback_to_traditional_retrieval(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç•¶ Neo4j ä¸å¯ç”¨æ™‚ï¼Œé™ç´šåˆ°å‚³çµ±çš„å‘é‡å’Œé—œéµå­—æª¢ç´¢
    
    Args:
        alert: ç•¶å‰è­¦å ±è³‡æ–™
        
    Returns:
        å‚³çµ±æª¢ç´¢çš„çµæœ
    """
    logger.info("ğŸ”„ Falling back to traditional vector + keyword retrieval")
    
    # ç”Ÿæˆå‚³çµ±æª¢ç´¢æŸ¥è©¢
    traditional_queries = determine_contextual_queries(alert)
    
    # å‘é‡åŒ–è­¦å ±ï¼ˆå¦‚æœéœ€è¦ï¼‰
    alert_vector = []
    embedding_service = GeminiEmbeddingService()
    try:
        alert_text = _extract_alert_text_for_embedding(alert)
        alert_vector = await embedding_service.embed_text(alert_text)
    except Exception as e:
        logger.warning(f"Alert vectorization failed: {str(e)}")
    
    # åŸ·è¡Œå‚³çµ±æª¢ç´¢
    return await execute_retrieval(traditional_queries, alert_vector)

def _extract_alert_text_for_embedding(alert: Dict[str, Any]) -> str:
    """
    å¾è­¦å ±ä¸­æå–æ–‡æœ¬ç”¨æ–¼å‘é‡åŒ–
    """
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    
    text_parts = [
        rule.get('description', ''),
        ' '.join(rule.get('groups', [])),
        str(alert_source.get('data', {}))
    ]
    
    return ' '.join(filter(None, text_parts))

# ==================== Graph-Native æ±ºç­–å¼•æ“ ====================

def determine_graph_queries(alert: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Graph-Native æ±ºç­–å¼•æ“ï¼šæ ¹æ“šè­¦å ±å…§å®¹æ±ºå®šè¦åŸ·è¡Œçš„ Cypher æŸ¥è©¢
    å–ä»£åŸæœ‰çš„ determine_contextual_queriesï¼Œå°ˆæ³¨æ–¼åœ–å½¢æŸ¥è©¢ç­–ç•¥
    
    Args:
        alert: æ–°çš„è­¦å ±æ–‡æª”
        
    Returns:
        Cypher æŸ¥è©¢è¦æ ¼åˆ—è¡¨
    """
    queries = []
    alert_source = alert.get('_source', {})
    rule = alert_source.get('rule', {})
    agent = alert_source.get('agent', {})
    data = alert_source.get('data', {})
    timestamp = alert_source.get('timestamp')
    
    rule_description = rule.get('description', '').lower()
    rule_groups = rule.get('groups', [])
    rule_level = rule.get('level', 0)
    agent_id = agent.get('id', '')
    
    logger.info(f"ğŸ”— GRAPH-NATIVE DECISION ENGINE: Analyzing alert for graph queries")
    logger.info(f"   Alert: {rule_description}")
    logger.info(f"   Level: {rule_level}, Agent: {agent_id}")
    logger.info(f"   Groups: {', '.join(rule_groups)}")
    
    # 1. SSH æš´åŠ›ç ´è§£å ´æ™¯ - æ”»æ“Šä¾†æºå…¨è²Œåˆ†æ
    if 'ssh' in rule_description and ('brute' in rule_description or 'failed' in rule_description):
        logger.info("ğŸ”‘ DECISION: SSH brute force detected - analyzing attacker profile")
        
        queries.append({
            'type': 'attack_path_analysis',
            'description': 'SSH attacker complete activity profile',
            'priority': 'critical',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                CALL {
                    WITH attacker
                    MATCH (related_alert:Alert)-[:HAS_SOURCE_IP]->(attacker)
                    WHERE related_alert.timestamp > datetime() - duration({hours: 1})
                    MATCH (related_alert)-[r]->(entity)
                    WHERE type(r) <> 'MATCHED_RULE'
                    RETURN related_alert, r, entity
                }
                RETURN *
            ''',
            'parameters': {}
        })
        
        # æ©«å‘ç§»å‹•æª¢æ¸¬
        queries.append({
            'type': 'lateral_movement_detection',
            'description': 'Lateral movement patterns from attacker IP',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                MATCH (attacker)<-[:HAS_SOURCE_IP]-(other_alerts:Alert)-[:TRIGGERED_ON]->(hosts:Host)
                WITH attacker, collect(DISTINCT hosts) as target_hosts
                WHERE size(target_hosts) > 1
                MATCH path = (attacker)-[*1..3]-(hosts:Host)
                RETURN path, target_hosts, attacker
            ''',
            'parameters': {}
        })
    
    # 2. æƒ¡æ„è»Ÿé«”/ç¨‹åºåˆ†æå ´æ™¯
    malware_keywords = ['malware', 'trojan', 'virus', 'suspicious', 'backdoor', 'rootkit']
    if any(keyword in rule_description for keyword in malware_keywords):
        logger.info("ğŸ¦  DECISION: Malware detected - analyzing process execution chains")
        
        queries.append({
            'type': 'process_chain_analysis',
            'description': 'Malicious process execution chains',
            'priority': 'critical',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_PROCESS]->(process:Process)
                MATCH path = (process)-[:SPAWNED_BY*0..5]->(parent:Process)
                MATCH (parent)<-[:INVOLVES_PROCESS]-(related_alerts:Alert)
                WHERE related_alerts.timestamp > datetime() - duration({hours: 2})
                RETURN path, collect(related_alerts) as timeline
            ''',
            'parameters': {}
        })
        
        # æª”æ¡ˆç³»çµ±å½±éŸ¿åˆ†æ
        queries.append({
            'type': 'file_interaction_analysis',
            'description': 'File system impact analysis',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_PROCESS]->(process:Process)
                MATCH (process)-[:ACCESSED_FILE|MODIFIED_FILE|CREATED_FILE]->(files:File)
                MATCH (files)<-[r]-(other_processes:Process)<-[:INVOLVES_PROCESS]-(other_alerts:Alert)
                WHERE other_alerts.timestamp > alert.timestamp - duration({minutes: 30})
                RETURN files, collect(other_processes) as interacting_processes, 
                       collect(other_alerts) as related_alerts
            ''',
            'parameters': {}
        })
    
    # 3. ç¶²è·¯æ”»æ“Šå ´æ™¯ - Web æ”»æ“Šåˆ†æ
    web_keywords = ['web attack', 'sql injection', 'xss', 'command injection', 'http']
    if any(keyword in rule_description for keyword in web_keywords) or 'web' in rule_groups:
        logger.info("ğŸŒ DECISION: Web attack detected - analyzing network attack patterns")
        
        queries.append({
            'type': 'network_topology_analysis',
            'description': 'Web attack network topology',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(attacker:IPAddress)
                MATCH (alert)-[:TRIGGERED_ON]->(target:Host)
                MATCH (attacker)-[:CONNECTED_TO*1..3]-(related_ips:IPAddress)
                MATCH (related_ips)<-[:HAS_SOURCE_IP]-(attack_alerts:Alert)
                WHERE attack_alerts.timestamp > datetime() - duration({hours: 6})
                RETURN attacker, target, related_ips, collect(attack_alerts) as attack_sequence
            ''',
            'parameters': {}
        })
    
    # 4. ä½¿ç”¨è€…è¡Œç‚ºç•°å¸¸åˆ†æ
    auth_keywords = ['authentication', 'login', 'failed', 'privilege', 'escalation']
    if any(keyword in rule_description for keyword in auth_keywords) or 'authentication' in rule_groups:
        logger.info("ğŸ‘¤ DECISION: Authentication anomaly - analyzing user behavior patterns")
        
        queries.append({
            'type': 'user_behavior_analysis',
            'description': 'User behavior anomaly analysis',
            'priority': 'medium',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:INVOLVES_USER]->(user:User)
                MATCH (user)<-[:INVOLVES_USER]-(user_alerts:Alert)
                WHERE user_alerts.timestamp > datetime() - duration({days: 7})
                WITH user, collect(user_alerts) as user_history
                MATCH (user)<-[:INVOLVES_USER]-(recent_alerts:Alert)
                WHERE recent_alerts.timestamp > datetime() - duration({hours: 2})
                RETURN user, user_history, collect(recent_alerts) as recent_activity
            ''',
            'parameters': {}
        })
    
    # 5. æ™‚é–“åºåˆ—é—œè¯åˆ†æ (ç¸½æ˜¯åŸ·è¡Œ)
    queries.append({
        'type': 'temporal_correlation',
        'description': 'Temporal sequence analysis',
        'priority': 'medium',
        'cypher_query': '''
            MATCH (alert:Alert {id: $alert_id})-[:TRIGGERED_ON]->(host:Host)
            MATCH (host)<-[:TRIGGERED_ON]-(related_alerts:Alert)
            WHERE related_alerts.timestamp > alert.timestamp - duration({minutes: 30})
              AND related_alerts.timestamp < alert.timestamp + duration({minutes: 30})
              AND related_alerts.id <> alert.id
            WITH alert, related_alerts
            ORDER BY related_alerts.timestamp
            RETURN alert, collect(related_alerts) as temporal_sequence
        ''',
        'parameters': {}
    })
    
    # 6. IP ä¿¡è­½èˆ‡åœ°ç†ä½ç½®åˆ†æ (é‡å°å¤–éƒ¨ IP)
    if _has_external_ip(alert_source):
        logger.info("ğŸŒ DECISION: External IP detected - analyzing IP reputation")
        
        queries.append({
            'type': 'ip_reputation_analysis',
            'description': 'IP reputation and geolocation analysis',
            'priority': 'medium',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})-[:HAS_SOURCE_IP]->(ip:IPAddress)
                WHERE ip.is_private = false
                MATCH (ip)<-[:HAS_SOURCE_IP]-(historical_alerts:Alert)
                WHERE historical_alerts.timestamp > datetime() - duration({days: 30})
                WITH ip, collect(historical_alerts) as ip_history
                MATCH (ip)-[:GEOLOCATED_IN]->(geo:GeoLocation)
                RETURN ip, ip_history, geo
            ''',
            'parameters': {}
        })
    
    # 7. å¨è„…å…¨æ™¯åˆ†æ (é«˜ç´šåˆ¥è­¦å ±)
    if rule_level >= 8:
        logger.info("âš ï¸ DECISION: High-severity alert - comprehensive threat landscape analysis")
        
        queries.append({
            'type': 'threat_landscape_analysis',
            'description': 'Comprehensive threat landscape',
            'priority': 'high',
            'cypher_query': '''
                MATCH (alert:Alert {id: $alert_id})
                MATCH (alert)-[r1]->(entity1)
                MATCH (entity1)-[r2]->(entity2)
                MATCH (entity2)<-[r3]-(other_alerts:Alert)
                WHERE other_alerts.timestamp > datetime() - duration({hours: 24})
                  AND other_alerts.rule_level >= 6
                RETURN alert, entity1, entity2, other_alerts, r1, r2, r3
                LIMIT 50
            ''',
            'parameters': {}
        })
    
    logger.info(f"âœ… Generated {len(queries)} graph queries for alert analysis")
    return queries

def _has_external_ip(alert_source: Dict[str, Any]) -> bool:
    """
    æª¢æŸ¥è­¦å ±æ˜¯å¦åŒ…å«å¤–éƒ¨ IP åœ°å€
    """
    data = alert_source.get('data', {})
    
    # æª¢æŸ¥å¸¸è¦‹çš„ IP æ¬„ä½
    ip_fields = ['srcip', 'dstip', 'src_ip', 'dst_ip', 'remote_ip']
    
    for field in ip_fields:
        ip = data.get(field)
        if ip and not _is_private_ip(ip):
            return True
    
    return False

def _is_private_ip(ip_address: str) -> bool:
    """
    æª¢æŸ¥ IP åœ°å€æ˜¯å¦ç‚ºç§æœ‰åœ°å€
    """
    try:
        import ipaddress
        ip = ipaddress.ip_address(ip_address)
        return ip.is_private
    except:
        return False

# ==================== æ··åˆæª¢ç´¢æ•´åˆ ====================

async def execute_hybrid_retrieval(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ··åˆæª¢ç´¢ç³»çµ±ï¼šçµåˆåœ–å½¢æŸ¥è©¢å’Œå‚³çµ±æª¢ç´¢æ–¹æ³•
    ç‚º GraphRAG æä¾›æœ€ä½³çš„ä¸Šä¸‹æ–‡æª¢ç´¢ç­–ç•¥
    
    Args:
        alert: ç•¶å‰è­¦å ±è³‡æ–™
        
    Returns:
        çµåˆçš„æª¢ç´¢çµæœ
    """
    logger.info("ğŸ”—ğŸ” HYBRID RETRIEVAL: Combining graph and traditional methods")
    
    # 1. åŸ·è¡Œåœ–å½¢æŸ¥è©¢
    graph_queries = determine_graph_queries(alert)
    graph_context = await execute_graph_retrieval(graph_queries, alert)
    
    # 2. å¦‚æœåœ–å½¢æŸ¥è©¢çµæœä¸è¶³ï¼Œè£œå……å‚³çµ±æª¢ç´¢
    total_graph_results = sum(len(results) for results in graph_context.values())
    
    if total_graph_results < 10:  # è¨­å®šé–¾å€¼
        logger.info("ğŸ“Š Graph results insufficient - supplementing with traditional retrieval")
        
        # ç”Ÿæˆè£œå……æŸ¥è©¢
        traditional_queries = determine_contextual_queries(alert)
        
        # å‘é‡åŒ–è­¦å ±
        embedding_service = GeminiEmbeddingService()
        try:
            alert_text = _extract_alert_text_for_embedding(alert)
            alert_vector = await embedding_service.embed_text(alert_text)
            traditional_context = await execute_retrieval(traditional_queries, alert_vector)
            
            # åˆä½µçµæœ
            return _merge_retrieval_contexts(graph_context, traditional_context)
        except Exception as e:
            logger.warning(f"Traditional retrieval failed: {str(e)}")
            return graph_context
    
    return graph_context

def _merge_retrieval_contexts(graph_context: Dict[str, Any], traditional_context: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆä½µåœ–å½¢æª¢ç´¢å’Œå‚³çµ±æª¢ç´¢çš„çµæœ
    """
    merged_context = graph_context.copy()
    
    # æ·»åŠ å‚³çµ±æª¢ç´¢çš„çµæœä½œç‚ºè£œå……ä¸Šä¸‹æ–‡
    merged_context['traditional_similar_alerts'] = traditional_context.get('similar_alerts', [])
    merged_context['traditional_metrics'] = traditional_context.get('cpu_metrics', []) + \
                                          traditional_context.get('memory_metrics', [])
    merged_context['traditional_logs'] = traditional_context.get('network_logs', []) + \
                                       traditional_context.get('ssh_logs', [])
    
    return merged_context

def format_graph_context_cypher_notation(context_data: Dict[str, Any]) -> str:
    """
    å°‡åœ–å½¢ä¸Šä¸‹æ–‡è½‰æ›ç‚ºç°¡åŒ–çš„Cypherè·¯å¾‘è¨˜è™Ÿæ ¼å¼
    
    Args:
        context_data: åœ–å½¢æª¢ç´¢çš„çµæœæ•¸æ“š
        
    Returns:
        Cypherè·¯å¾‘è¨˜è™Ÿæ ¼å¼çš„å­—ç¬¦ä¸²
    """
    cypher_paths = []
    
    # 1. è™•ç†æ”»æ“Šè·¯å¾‘
    attack_paths = context_data.get('attack_paths', [])
    for path_data in attack_paths[:3]:  # é™åˆ¶æ•¸é‡ä»¥é¿å…éé•·
        attacker = path_data.get('attacker', {})
        related_alerts = path_data.get('related_alert', [])
        
        if attacker.get('address'):
            for alert in related_alerts[:5]:  # é™åˆ¶æ¯å€‹IPçš„è­¦å ±æ•¸é‡
                alert_desc = alert.get('rule', {}).get('description', 'Unknown')
                cypher_paths.append(
                    f"(IP:{attacker['address']}) -[TRIGGERED: {alert_desc[:30]}]-> (Alert:{alert.get('id', 'unknown')[:8]})"
                )
    
    # 2. è™•ç†æ©«å‘ç§»å‹•
    lateral_movement = context_data.get('lateral_movement', [])
    for movement_data in lateral_movement[:2]:
        attacker = movement_data.get('attacker', {})
        target_hosts = movement_data.get('target_hosts', [])
        
        if attacker.get('address') and target_hosts:
            for host in target_hosts[:3]:
                host_name = host.get('agent_name', 'unknown')
                cypher_paths.append(
                    f"(IP:{attacker['address']}) -[LATERAL_MOVE]-> (Host:{host_name})"
                )
    
    # 3. è™•ç†ç¨‹åºåŸ·è¡Œéˆ
    process_chains = context_data.get('process_chains', [])
    for process_data in process_chains[:2]:
        timeline = process_data.get('timeline', [])
        if len(timeline) > 1:
            for i in range(len(timeline) - 1):
                current_alert = timeline[i]
                next_alert = timeline[i + 1]
                current_process = current_alert.get('data', {}).get('process', {}).get('name', 'unknown')
                next_process = next_alert.get('data', {}).get('process', {}).get('name', 'unknown')
                cypher_paths.append(
                    f"(Process:{current_process}) -[SPAWNED]-> (Process:{next_process})"
                )
    
    # 4. è™•ç†IPä¿¡è­½
    ip_reputation = context_data.get('ip_reputation', [])
    for ip_data in ip_reputation[:2]:
        ip = ip_data.get('ip', {})
        ip_history = ip_data.get('ip_history', [])
        
        if ip.get('address') and ip_history:
            alert_count = len(ip_history)
            cypher_paths.append(
                f"(IP:{ip['address']}) -[REPUTATION: {alert_count} alerts in 30 days]-> (ThreatProfile:Suspicious)"
            )
    
    # 5. è™•ç†ä½¿ç”¨è€…è¡Œç‚º
    user_behavior = context_data.get('user_behavior', [])
    for user_data in user_behavior[:2]:
        user = user_data.get('user', {})
        recent_activity = user_data.get('recent_activity', [])
        
        if user.get('username') and recent_activity:
            activity_count = len(recent_activity)
            cypher_paths.append(
                f"(User:{user['username']}) -[RECENT_ACTIVITY: {activity_count} events]-> (BehaviorPattern:Anomalous)"
            )
    
    # 6. è™•ç†æª”æ¡ˆäº¤äº’
    file_interactions = context_data.get('file_interactions', [])
    for file_data in file_interactions[:2]:
        files = file_data.get('files', {})
        interacting_processes = file_data.get('interacting_processes', [])
        
        if files.get('file_path') and interacting_processes:
            file_path = files['file_path'].split('/')[-1]  # åªé¡¯ç¤ºæª”å
            process_count = len(interacting_processes)
            cypher_paths.append(
                f"(File:{file_path}) -[ACCESSED_BY: {process_count} processes]-> (SecurityEvent:Suspicious)"
            )
    
    # 7. è™•ç†æ™‚é–“åºåˆ—
    temporal_sequences = context_data.get('temporal_sequences', [])
    for seq_data in temporal_sequences[:1]:  # åªè™•ç†ä¸€å€‹ä¸»è¦åºåˆ—
        sequence = seq_data.get('temporal_sequence', [])
        if len(sequence) > 1:
            first_alert = sequence[0]
            last_alert = sequence[-1]
            time_span = len(sequence)
            cypher_paths.append(
                f"(Alert:{first_alert.get('id', 'unknown')[:8]}) -[TEMPORAL_SEQUENCE: {time_span} events in 30min]-> (Alert:{last_alert.get('id', 'unknown')[:8]})"
            )
    
    # å¦‚æœæ²’æœ‰è¶³å¤ çš„åœ–å½¢æ•¸æ“šï¼Œç”ŸæˆåŸºæ–¼å‚³çµ±æª¢ç´¢çš„è·¯å¾‘æ ¼å¼
    if not cypher_paths:
        cypher_paths = _generate_fallback_cypher_paths(context_data)
    
    return "\n".join(cypher_paths)

def _generate_fallback_cypher_paths(context_data: Dict[str, Any]) -> List[str]:
    """
    ç•¶åœ–å½¢æ•¸æ“šä¸è¶³æ™‚ï¼ŒåŸºæ–¼å‚³çµ±æª¢ç´¢çµæœç”ŸæˆCypherè·¯å¾‘æ ¼å¼
    
    Args:
        context_data: ä¸Šä¸‹æ–‡æ•¸æ“š
        
    Returns:
        Cypherè·¯å¾‘æ ¼å¼çš„åˆ—è¡¨
    """
    fallback_paths = []
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å‚³çµ±çš„ç›¸ä¼¼è­¦å ±
    traditional_alerts = context_data.get('traditional_similar_alerts', [])
    similar_alerts = context_data.get('similar_alerts', [])
    all_similar = traditional_alerts + similar_alerts
    
    if all_similar:
        for i, alert in enumerate(all_similar[:3], 1):
            alert_source = alert.get('_source', {})
            rule = alert_source.get('rule', {})
            agent = alert_source.get('agent', {})
            score = alert.get('_score', 0.0)
            
            fallback_paths.append(
                f"(Alert:Current) -[SIMILAR: {score:.2f}]-> (Alert:{rule.get('description', 'Unknown')[:25]}...)"
            )
            
            if agent.get('name'):
                fallback_paths.append(
                    f"(Host:{agent['name']}) -[EXPERIENCED]-> (Alert:{rule.get('description', 'Unknown')[:25]}...)"
                )
    
    # æª¢æŸ¥ç³»çµ±æŒ‡æ¨™
    cpu_metrics = context_data.get('cpu_metrics', [])
    memory_metrics = context_data.get('memory_metrics', [])
    
    if cpu_metrics or memory_metrics:
        fallback_paths.append(
            f"(Alert:Current) -[CORRELATED_WITH]-> (SystemMetrics:{len(cpu_metrics + memory_metrics)} events)"
        )
    
    # æª¢æŸ¥ç¶²è·¯æ—¥èªŒ
    network_logs = context_data.get('network_logs', [])
    ssh_logs = context_data.get('ssh_logs', [])
    
    if network_logs or ssh_logs:
        fallback_paths.append(
            f"(Alert:Current) -[NETWORK_CONTEXT]-> (NetworkActivity:{len(network_logs + ssh_logs)} events)"
        )
    
    # å¦‚æœé‚„æ˜¯æ²’æœ‰æ•¸æ“šï¼Œæä¾›åŸºæœ¬èªªæ˜
    if not fallback_paths:
        fallback_paths = [
            "åœ–å½¢æª¢ç´¢æœªç™¼ç¾æ˜é¡¯çš„å¨è„…é—œè¯æ¨¡å¼",
            "ç•¶å‰è­¦å ±ç‚ºç¨ç«‹äº‹ä»¶ï¼Œç„¡é¡¯è‘—çš„åœ–å½¢åŒ–é—œè¯",
            "å»ºè­°åŸºæ–¼è¦å‰‡ç­‰ç´šå’Œå…§å®¹é€²è¡Œå–®ä¸€äº‹ä»¶åˆ†æ"
        ]
    
    return fallback_paths

def format_graph_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    Graph-Native ä¸Šä¸‹æ–‡æ ¼å¼åŒ–ï¼šå°‡åœ–å½¢æª¢ç´¢çµæœæ ¼å¼åŒ–ç‚º LLM å¯ç†è§£çš„çµæ§‹åŒ–æ–‡æœ¬
    
    Args:
        context_data: å¾ execute_graph_retrieval ç²å¾—çš„åœ–å½¢ä¸Šä¸‹æ–‡è³‡æ–™
        
    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—å…¸ï¼Œæº–å‚™æä¾›çµ¦ LLM åˆ†æ
    """
    formatted_context = {}
    
    # æ·»åŠ Cypherè·¯å¾‘è¨˜è™Ÿæ ¼å¼
    formatted_context['graph_context'] = format_graph_context_cypher_notation(context_data)
    
    # 1. æ”»æ“Šè·¯å¾‘åˆ†æ
    attack_paths = context_data.get('attack_paths', [])
    if attack_paths:
        path_parts = []
        for i, path_data in enumerate(attack_paths[:5], 1):
            # è§£æåœ–å½¢è·¯å¾‘è³‡æ–™
            attacker = path_data.get('attacker', {})
            related_alerts = path_data.get('related_alert', [])
            entities = path_data.get('entity', [])
            
            path_part = f"""
{i}. **æ”»æ“Šä¾†æº:** {attacker.get('address', 'Unknown IP')}
   **ç›¸é—œè­¦å ±æ•¸é‡:** {len(related_alerts) if isinstance(related_alerts, list) else 1}
   **å½±éŸ¿å¯¦é«”:** {len(entities) if isinstance(entities, list) else 1} å€‹ç³»çµ±çµ„ä»¶
   **æ”»æ“Šæ™‚é–“ç¯„åœ:** éå»1å°æ™‚å…§çš„æŒçºŒæ´»å‹•"""
            path_parts.append(path_part)
        formatted_context['attack_path_analysis'] = "\n".join(path_parts)
    else:
        formatted_context['attack_path_analysis'] = "æœªç™¼ç¾æ˜ç¢ºçš„æ”»æ“Šè·¯å¾‘æ¨¡å¼ã€‚"
    
    # 2. æ©«å‘ç§»å‹•æª¢æ¸¬
    lateral_movement = context_data.get('lateral_movement', [])
    if lateral_movement:
        movement_parts = []
        for i, movement_data in enumerate(lateral_movement[:3], 1):
            attacker = movement_data.get('attacker', {})
            target_hosts = movement_data.get('target_hosts', [])
            
            movement_part = f"""
{i}. **æ©«å‘ç§»å‹•ä¾†æº:** {attacker.get('address', 'Unknown')}
   **ç›®æ¨™ä¸»æ©Ÿæ•¸é‡:** {len(target_hosts)} å°ä¸»æ©Ÿ
   **ç§»å‹•æ¨¡å¼:** å¤šä¸»æ©Ÿæ»²é€æª¢æ¸¬åˆ°"""
            movement_parts.append(movement_part)
        formatted_context['lateral_movement_analysis'] = "\n".join(movement_parts)
    else:
        formatted_context['lateral_movement_analysis'] = "æœªæª¢æ¸¬åˆ°æ©«å‘ç§»å‹•æ´»å‹•ã€‚"
    
    # 3. æ™‚é–“åºåˆ—é—œè¯
    temporal_sequences = context_data.get('temporal_sequences', [])
    if temporal_sequences:
        temporal_parts = []
        for seq_data in temporal_sequences[:3]:
            sequence = seq_data.get('temporal_sequence', [])
            if sequence:
                temporal_part = f"**æ™‚é–“åºåˆ—ç›¸é—œè­¦å ±:** {len(sequence)} å€‹ç›¸é—œäº‹ä»¶åœ¨Â±30åˆ†é˜æ™‚é–“çª—å£å…§"
                temporal_parts.append(temporal_part)
        formatted_context['temporal_correlation'] = "\n".join(temporal_parts)
    else:
        formatted_context['temporal_correlation'] = "æœªç™¼ç¾æ™‚é–“åºåˆ—ç›¸é—œäº‹ä»¶ã€‚"
    
    # 4. IP ä¿¡è­½åˆ†æ
    ip_reputation = context_data.get('ip_reputation', [])
    if ip_reputation:
        ip_parts = []
        for ip_data in ip_reputation[:3]:
            ip = ip_data.get('ip', {})
            ip_history = ip_data.get('ip_history', [])
            geo = ip_data.get('geo', {})
            
            ip_part = f"""
**IP åœ°å€:** {ip.get('address', 'Unknown')}
**æ­·å²æ´»å‹•:** éå»30å¤©å…§ {len(ip_history)} æ¬¡è­¦å ±è¨˜éŒ„
**åœ°ç†ä½ç½®:** {geo.get('country', 'Unknown')} - {geo.get('city', 'Unknown')}
**ç§æœ‰åœ°å€:** {'å¦' if not ip.get('is_private', True) else 'æ˜¯'}"""
            ip_parts.append(ip_part)
        formatted_context['ip_reputation_analysis'] = "\n".join(ip_parts)
    else:
        formatted_context['ip_reputation_analysis'] = "ç„¡å¤–éƒ¨IPä¿¡è­½è³‡æ–™å¯ä¾›åˆ†æã€‚"
    
    # 5. ä½¿ç”¨è€…è¡Œç‚ºåˆ†æ
    user_behavior = context_data.get('user_behavior', [])
    if user_behavior:
        user_parts = []
        for user_data in user_behavior[:3]:
            user = user_data.get('user', {})
            user_history = user_data.get('user_history', [])
            recent_activity = user_data.get('recent_activity', [])
            
            user_part = f"""
**ä½¿ç”¨è€…:** {user.get('username', 'Unknown')}
**æ­·å²è¡Œç‚º:** éå»7å¤©å…§ {len(user_history)} æ¬¡æ´»å‹•è¨˜éŒ„
**è¿‘æœŸç•°å¸¸:** éå»2å°æ™‚å…§ {len(recent_activity)} æ¬¡æ´»å‹•"""
            user_parts.append(user_part)
        formatted_context['user_behavior_analysis'] = "\n".join(user_parts)
    else:
        formatted_context['user_behavior_analysis'] = "æœªç™¼ç¾ç›¸é—œä½¿ç”¨è€…è¡Œç‚ºç•°å¸¸ã€‚"
    
    # 6. ç¨‹åºåŸ·è¡Œéˆåˆ†æ
    process_chains = context_data.get('process_chains', [])
    if process_chains:
        process_parts = []
        for process_data in process_chains[:3]:
            timeline = process_data.get('timeline', [])
            if timeline:
                process_part = f"**ç¨‹åºåŸ·è¡Œéˆ:** æª¢æ¸¬åˆ° {len(timeline)} å€‹ç›¸é—œç¨‹åºåŸ·è¡Œäº‹ä»¶"
                process_parts.append(process_part)
        formatted_context['process_chain_analysis'] = "\n".join(process_parts)
    else:
        formatted_context['process_chain_analysis'] = "æœªç™¼ç¾å¯ç–‘çš„ç¨‹åºåŸ·è¡Œéˆã€‚"
    
    # 7. æª”æ¡ˆäº¤äº’åˆ†æ
    file_interactions = context_data.get('file_interactions', [])
    if file_interactions:
        file_parts = []
        for file_data in file_interactions[:3]:
            files = file_data.get('files', {})
            interacting_processes = file_data.get('interacting_processes', [])
            
            file_part = f"""
**æª”æ¡ˆè·¯å¾‘:** {files.get('file_path', 'Unknown')}
**äº¤äº’ç¨‹åºæ•¸é‡:** {len(interacting_processes)}"""
            file_parts.append(file_part)
        formatted_context['file_interaction_analysis'] = "\n".join(file_parts)
    else:
        formatted_context['file_interaction_analysis'] = "æœªç™¼ç¾ç•°å¸¸çš„æª”æ¡ˆç³»çµ±äº¤äº’ã€‚"
    
    # 8. ç¶²è·¯æ‹“æ’²åˆ†æ
    network_topology = context_data.get('network_topology', [])
    if network_topology:
        network_parts = []
        for net_data in network_topology[:3]:
            attacker = net_data.get('attacker', {})
            target = net_data.get('target', {})
            attack_sequence = net_data.get('attack_sequence', [])
            
            network_part = f"""
**æ”»æ“Šä¾†æº:** {attacker.get('address', 'Unknown')}
**ç›®æ¨™ä¸»æ©Ÿ:** {target.get('agent_name', 'Unknown')}
**æ”»æ“Šåºåˆ—:** éå»6å°æ™‚å…§ {len(attack_sequence)} æ¬¡ç›¸é—œæ”»æ“Š"""
            network_parts.append(network_part)
        formatted_context['network_topology_analysis'] = "\n".join(network_parts)
    else:
        formatted_context['network_topology_analysis'] = "æœªç™¼ç¾è¤‡é›œçš„ç¶²è·¯æ”»æ“Šæ‹“æ’²ã€‚"
    
    # 9. å¨è„…å…¨æ™¯åˆ†æ
    threat_landscape = context_data.get('threat_landscape', [])
    if threat_landscape:
        threat_parts = []
        threat_count = len(threat_landscape)
        if threat_count > 0:
            threat_part = f"**ç¶œåˆå¨è„…è©•ä¼°:** æª¢æ¸¬åˆ° {threat_count} å€‹é«˜ç´šåˆ¥å¨è„…é—œè¯äº‹ä»¶ï¼ˆéå»24å°æ™‚ï¼‰"
            threat_parts.append(threat_part)
        formatted_context['threat_landscape_analysis'] = "\n".join(threat_parts)
    else:
        formatted_context['threat_landscape_analysis'] = "æ•´é«”å¨è„…ç’°å¢ƒç›¸å°ç©©å®šã€‚"
    
    # 10. å‚³çµ±æª¢ç´¢è£œå……ï¼ˆæ··åˆæ¨¡å¼ï¼‰
    traditional_alerts = context_data.get('traditional_similar_alerts', [])
    traditional_metrics = context_data.get('traditional_metrics', [])
    traditional_logs = context_data.get('traditional_logs', [])
    
    if traditional_alerts or traditional_metrics or traditional_logs:
        supplement_parts = []
        if traditional_alerts:
            supplement_parts.append(f"**ç›¸ä¼¼è­¦å ±è£œå……:** {len(traditional_alerts)} å€‹å‘é‡ç›¸ä¼¼è­¦å ±")
        if traditional_metrics:
            supplement_parts.append(f"**ç³»çµ±æŒ‡æ¨™è£œå……:** {len(traditional_metrics)} å€‹ç³»çµ±æ€§èƒ½è¨˜éŒ„")
        if traditional_logs:
            supplement_parts.append(f"**æ—¥èªŒè£œå……:** {len(traditional_logs)} å€‹ç¶²è·¯/SSHæ—¥èªŒ")
        formatted_context['traditional_supplement'] = "\n".join(supplement_parts)
    else:
        formatted_context['traditional_supplement'] = "ç„¡éœ€å‚³çµ±æª¢ç´¢è£œå……ã€‚"
    
    return formatted_context

# ==================== æ··åˆæ ¼å¼åŒ–å‡½æ•¸ ====================

def format_hybrid_context(context_data: Dict[str, Any]) -> Dict[str, str]:
    """
    æ··åˆä¸Šä¸‹æ–‡æ ¼å¼åŒ–ï¼šè‡ªå‹•æª¢æ¸¬ä¸¦æ ¼å¼åŒ–åœ–å½¢æˆ–å‚³çµ±æª¢ç´¢çµæœ
    
    Args:
        context_data: æª¢ç´¢çµæœè³‡æ–™
        
    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—å…¸
    """
    # æª¢æ¸¬æ˜¯å¦ç‚ºåœ–å½¢æª¢ç´¢çµæœ
    graph_indicators = ['attack_paths', 'lateral_movement', 'temporal_sequences', 
                       'ip_reputation', 'user_behavior', 'process_chains']
    
    has_graph_data = any(context_data.get(indicator) for indicator in graph_indicators)
    
    if has_graph_data:
        logger.info("ğŸ”— Formatting graph-native context for LLM analysis")
        return format_graph_context(context_data)
    else:
        logger.info("ğŸ“Š Formatting traditional context for LLM analysis")
        return format_multi_source_context(context_data)

# ==================== GraphRAG Context æ ¼å¼åŒ–ç¤ºä¾‹èˆ‡æ¸¬è©¦ ====================

def create_example_graph_context() -> str:
    """
    å‰µå»ºä¸€å€‹ç¤ºä¾‹åœ–å½¢ä¸Šä¸‹æ–‡ï¼Œå±•ç¤ºCypherè·¯å¾‘è¨˜è™Ÿæ ¼å¼
    é€™å€‹å‡½æ•¸å±•ç¤ºäº†GraphRAG prompt templateçš„é æœŸè¼¸å…¥æ ¼å¼
    
    Returns:
        æ ¼å¼åŒ–çš„Cypherè·¯å¾‘è¨˜è™Ÿå­—ç¬¦ä¸²
    """
    example_cypher_paths = [
        "(IP:192.168.1.100) -[FAILED_LOGIN: 50æ¬¡]-> (Host:web-01)",
        "(IP:192.168.1.100) -[FAILED_LOGIN: 25æ¬¡]-> (Host:db-01)", 
        "(IP:192.168.1.100) -[SUCCESSFUL_LOGIN]-> (Host:dev-server)",
        "(Host:dev-server) -[EXECUTED]-> (Process:mimikatz.exe)",
        "(Process:mimikatz.exe) -[ACCESSED]-> (File:sam.db)",
        "(User:admin) -[PRIVILEGE_ESCALATION]-> (Role:SYSTEM)",
        "(Alert:ssh_brute_01) -[TEMPORAL_SEQUENCE: 3 events in 30min]-> (Alert:privilege_esc_02)",
        "(IP:192.168.1.100) -[REPUTATION: 15 alerts in 30 days]-> (ThreatProfile:HighRisk)",
        "(File:malware.exe) -[ACCESSED_BY: 5 processes]-> (SecurityEvent:Suspicious)",
        "(User:alice) -[RECENT_ACTIVITY: 8 events]-> (BehaviorPattern:Anomalous)"
    ]
    
    return "\n".join(example_cypher_paths)

def demonstrate_enhanced_prompt_usage():
    """
    æ¼”ç¤ºå¢å¼·çš„GraphRAG prompt templateä½¿ç”¨æ–¹æ³•
    å±•ç¤ºå®Œæ•´çš„åœ–å½¢ä¸Šä¸‹æ–‡å¦‚ä½•è¢«æ³¨å…¥åˆ°promptä¸­
    """
    
    # å‰µå»ºç¤ºä¾‹ä¸Šä¸‹æ–‡æ•¸æ“š
    example_context = {
        'graph_context': create_example_graph_context(),
        'lateral_movement_analysis': """
        **æ©«å‘ç§»å‹•æª¢æ¸¬:** æª¢æ¸¬åˆ°æ”»æ“Šè€…å¾å–®ä¸€IPæ»²é€å¤šå€‹ä¸»æ©Ÿ
        - web-01: åˆå§‹å…¥ä¾µé»ï¼Œ50æ¬¡å¤±æ•—ç™»éŒ„
        - db-01: æ¬¡è¦ç›®æ¨™ï¼Œ25æ¬¡å¤±æ•—ç™»éŒ„  
        - dev-server: æˆåŠŸæ»²é€ï¼Œæ¬Šé™æå‡æª¢æ¸¬
        """,
        'temporal_correlation': """
        **æ™‚é–“åºåˆ—:** 3å€‹é—œè¯äº‹ä»¶åœ¨30åˆ†é˜å…§ç™¼ç”Ÿ
        - 18:30 SSHæš´åŠ›ç ´è§£é–‹å§‹
        - 18:45 æˆåŠŸç™»éŒ„dev-server
        - 18:55 æ¬Šé™æå‡å’Œæƒ¡æ„ç¨‹åºåŸ·è¡Œ
        """,
        'ip_reputation_analysis': """
        **IPä¿¡è­½:** 192.168.1.100 è¢«æ¨™è¨˜ç‚ºé«˜é¢¨éšª
        - éå»30å¤©å…§è§¸ç™¼15æ¬¡å®‰å…¨è­¦å ±
        - å¤šä¸»æ©Ÿæ”»æ“Šæ¨¡å¼
        """,
        'user_behavior_analysis': """
        **ä½¿ç”¨è€…è¡Œç‚º:** adminå’Œaliceè³¬æˆ¶ç•°å¸¸æ´»å‹•
        - admin: æ¬Šé™æå‡è‡³SYSTEMç­‰ç´š
        - alice: 2å°æ™‚å…§8æ¬¡ç•°å¸¸è¡Œç‚º
        """,
        'process_chain_analysis': """
        **ç¨‹åºåŸ·è¡Œéˆ:** æƒ¡æ„ç¨‹åºåŸ·è¡Œæª¢æ¸¬
        - mimikatz.exe: æ†‘è­‰ç«Šå–å·¥å…·
        - å­˜å–sam.db: å¯†ç¢¼å“ˆå¸Œæå–
        """,
        'file_interaction_analysis': """
        **æª”æ¡ˆäº¤äº’:** ç³»çµ±é—œéµæª”æ¡ˆè¢«å­˜å–
        - sam.db: å¯†ç¢¼è³‡æ–™åº«
        - malware.exe: 5å€‹ç¨‹åºå­˜å–æ­¤å¯ç–‘æª”æ¡ˆ
        """,
        'network_topology_analysis': """
        **ç¶²è·¯æ‹“æ’²:** å…§ç¶²æ©«å‘ç§»å‹•æ¨¡å¼
        - å–®ä¸€å¤–éƒ¨IPæ”»æ“Šå¤šå€‹å…§éƒ¨ä¸»æ©Ÿ
        - æˆåŠŸå»ºç«‹å…§ç¶²ç«‹è¶³é»
        """,
        'threat_landscape_analysis': """
        **å¨è„…å…¨æ™¯:** å…¸å‹APTæ”»æ“Šæ¨¡å¼
        - éšæ®µ1: åµå¯Ÿå’Œæš´åŠ›ç ´è§£
        - éšæ®µ2: æ¬Šé™æå‡
        - éšæ®µ3: æ©«å‘ç§»å‹•
        """,
        'traditional_supplement': """
        **å‚³çµ±æª¢ç´¢è£œå……:** 5å€‹å‘é‡ç›¸ä¼¼è­¦å ±æä¾›é¡å¤–ä¸Šä¸‹æ–‡
        """
    }
    
    # å‰µå»ºç¤ºä¾‹è­¦å ±æ‘˜è¦
    alert_summary = "SSH Brute Force Attack Detected on dev-server (Level: 7)"
    
    # å±•ç¤ºå¦‚ä½•ä½¿ç”¨enhanced_graphrag_prompt_template
    logger.info("ğŸ”— DEMONSTRATION: Enhanced GraphRAG Prompt Template Usage")
    logger.info("Graph Context Format:")
    logger.info(example_context['graph_context'])
    
    # é€™å±•ç¤ºäº†LLMå°‡æ¥æ”¶åˆ°çš„å®Œæ•´ä¸Šä¸‹æ–‡çµæ§‹
    full_prompt_context = {
        'alert_summary': alert_summary,
        **example_context
    }
    
    logger.info("âœ… Enhanced GraphRAG prompt ready with comprehensive graph context")
    return full_prompt_context

def validate_graph_context_format(graph_context: str) -> bool:
    """
    é©—è­‰åœ–å½¢ä¸Šä¸‹æ–‡æ ¼å¼æ˜¯å¦ç¬¦åˆCypherè·¯å¾‘è¨˜è™Ÿæ¨™æº–
    
    Args:
        graph_context: å¾…é©—è­‰çš„åœ–å½¢ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        
    Returns:
        æ ¼å¼æ˜¯å¦æœ‰æ•ˆ
    """
    lines = graph_context.strip().split('\n')
    valid_lines = 0
    
    for line in lines:
        # æª¢æŸ¥åŸºæœ¬çš„Cypherè·¯å¾‘æ ¼å¼: (Node) -[Relationship]-> (Node)
        if '(' in line and ')' in line and '-[' in line and ']-> (' in line:
            valid_lines += 1
        # æˆ–è€…æ˜¯èªªæ˜æ€§æ–‡å­—
        elif any(keyword in line for keyword in ['æœªç™¼ç¾', 'å»ºè­°', 'ç¨ç«‹äº‹ä»¶']):
            valid_lines += 1
    
    validity_ratio = valid_lines / len(lines) if lines else 0
    is_valid = validity_ratio >= 0.8  # è‡³å°‘80%çš„è¡Œæ‡‰è©²æ˜¯æœ‰æ•ˆæ ¼å¼
    
    logger.info(f"ğŸ“Š Graph context validation: {valid_lines}/{len(lines)} valid lines ({validity_ratio:.1%})")
    
    return is_valid

# ==================== Stage 4 Step 4 å®Œæˆç¢ºèª ====================

def stage4_step4_completion_summary():
    """
    Stage 4 Step 4 å®Œæˆç¸½çµï¼šå¢å¼·æç¤ºè©æ¨¡æ¿ä»¥å®¹ç´åœ–å½¢ä¸Šä¸‹æ–‡
    """
    logger.info("ğŸ‰ === STAGE 4 STEP 4 COMPLETION SUMMARY ===")
    logger.info("âœ… Enhanced prompt template with graph context capability")
    logger.info("âœ… Implemented Cypher path notation formatting")
    logger.info("âœ… Created fallback formatting for traditional retrieval")
    logger.info("âœ… Added validation and demonstration functions")
    logger.info("âœ… Integrated graph context into LLM analysis pipeline")
    
    completion_details = {
        "stage": "Stage 4 - GraphRAG Implementation",
        "step": "Step 4 - Enhanced Prompt Template",
        "features_implemented": [
            "enhanced_graphrag_prompt_template: å¢å¼·çš„GraphRAGæç¤ºè©æ¨¡æ¿",
            "format_graph_context_cypher_notation: Cypherè·¯å¾‘è¨˜è™Ÿæ ¼å¼åŒ–",
            "_generate_fallback_cypher_paths: å‚³çµ±æª¢ç´¢é™ç´šæ ¼å¼åŒ–",
            "create_example_graph_context: ç¤ºä¾‹åœ–å½¢ä¸Šä¸‹æ–‡ç”Ÿæˆ",
            "validate_graph_context_format: æ ¼å¼é©—è­‰åŠŸèƒ½",
            "demonstrate_enhanced_prompt_usage: ä½¿ç”¨ç¤ºä¾‹æ¼”ç¤º"
        ],
        "graph_context_format": "Simplified Cypher Path Notation",
        "example_format": "(IP:192.168.1.100) -[FAILED_LOGIN: 50æ¬¡]-> (Host:web-01)",
        "integration_points": [
            "get_analysis_chain: é¸æ“‡å¢å¼·çš„GraphRAGåˆ†æéˆ",
            "format_graph_context: æ•´åˆCypherè·¯å¾‘æ ¼å¼åŒ–",
            "process_single_alert: åœ–å½¢ä¸Šä¸‹æ–‡æ³¨å…¥åˆ†ææµç¨‹"
        ],
        "benefits_achieved": [
            "æ·±åº¦ä¸Šä¸‹æ–‡ï¼šå¾ç›¸ä¼¼äº‹ä»¶åˆ—è¡¨è®Šç‚ºæ”»æ“Šè·¯å¾‘åœ–",
            "é«˜æ•ˆæª¢ç´¢ï¼šåˆ©ç”¨Neo4jåœ–å½¢éæ­·èƒ½åŠ›",
            "æ“ºè„«ç‰ˆæœ¬ä¾è³´ï¼šç¾ä»£åŒ–è³‡æ–™åº«æ¶æ§‹", 
            "æ›´å¼·Agenticèƒ½åŠ›ï¼šè²¼è¿‘äººé¡åˆ†æå¸«æ€ç¶­æ¨¡å¼"
        ]
    }
    
    logger.info("ğŸ“Š Implementation Details:")
    for feature in completion_details["features_implemented"]:
        logger.info(f"   â€¢ {feature}")
    
    logger.info("ğŸ”— Graph Context Format Demonstrated:")
    logger.info(f"   â€¢ {completion_details['example_format']}")
    
    logger.info("ğŸ¯ Next Steps:")
    logger.info("   â€¢ Test enhanced prompt with real alert data")
    logger.info("   â€¢ Fine-tune Cypher path formatting based on LLM feedback")
    logger.info("   â€¢ Monitor GraphRAG analysis quality improvements")
    
    return completion_details

# åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚åŸ·è¡Œç¤ºä¾‹æ¼”ç¤º
if __name__ == "__main__":
    # æ¼”ç¤ºæ–°çš„GraphRAGåŠŸèƒ½
    stage4_step4_completion_summary()
    demonstrate_enhanced_prompt_usage()
    
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)