"""
åœ–å½¢æœå‹™æ¨¡çµ„
å°è£æ‰€æœ‰èˆ‡ Neo4j åœ–å½¢è³‡æ–™åº«çš„äº’å‹•
"""

import logging
import uuid
import re
from typing import List, Dict, Any
from datetime import datetime
from dateutil import parser

logger = logging.getLogger(__name__)

# æª¢æŸ¥ Neo4j æ˜¯å¦å¯ç”¨
try:
    from neo4j import AsyncGraphDatabase, AsyncDriver
    NEO4J_AVAILABLE = True
except ImportError:
    logger.warning("Neo4j é©…å‹•ç¨‹å¼ä¸å¯ç”¨ã€‚åœ–å½¢æŒä¹…åŒ–åŠŸèƒ½å°‡è¢«åœç”¨ã€‚")
    NEO4J_AVAILABLE = False
    AsyncGraphDatabase = None
    AsyncDriver = None

async def extract_graph_entities(alert: Dict[str, Any], context_data: Dict[str, Any], analysis_result: str) -> List[Dict[str, Any]]:
    """
    å¾žè­¦å ±ã€ä¸Šä¸‹æ–‡è³‡æ–™å’Œåˆ†æžçµæžœä¸­æå–åœ–å½¢å¯¦é«”
    
    Args:
        alert: åŽŸå§‹è­¦å ±è³‡æ–™
        context_data: ä¸Šä¸‹æ–‡é—œè¯è³‡æ–™
        analysis_result: LLM åˆ†æžçµæžœ
    
    Returns:
        æå–çš„åœ–å½¢å¯¦é«”åˆ—è¡¨
    """
    entities = []
    alert_source = alert.get('_source', {})
    
    # 1. è­¦å ±å¯¦é«” (Alert Entity)
    alert_entity = {
        'type': 'Alert',
        'id': alert['_id'],
        'properties': {
            'alert_id': alert['_id'],
            'timestamp': alert_source.get('timestamp'),
            'rule_id': alert_source.get('rule', {}).get('id'),
            'rule_description': alert_source.get('rule', {}).get('description'),
            'rule_level': alert_source.get('rule', {}).get('level'),
            'rule_groups': alert_source.get('rule', {}).get('groups', []),
            'risk_level': _extract_risk_level_from_analysis(analysis_result),
            'triage_score': _calculate_triage_score(alert_source, analysis_result)
        }
    }
    entities.append(alert_entity)
    
    # 2. ä¸»æ©Ÿå¯¦é«” (Host Entity)
    agent = alert_source.get('agent', {})
    if agent.get('id') or agent.get('name'):
        host_entity = {
            'type': 'Host',
            'id': f"host_{agent.get('id', agent.get('name', 'unknown'))}",
            'properties': {
                'agent_id': agent.get('id'),
                'agent_name': agent.get('name'),
                'agent_ip': agent.get('ip'),
                'operating_system': _extract_os_info(alert_source)
            }
        }
        entities.append(host_entity)
    
    # 3. IP ä½å€å¯¦é«” (IP Address Entities)
    ip_addresses = _extract_ip_addresses(alert_source)
    for ip_info in ip_addresses:
        ip_entity = {
            'type': 'IPAddress',
            'id': f"ip_{ip_info['address']}",
            'properties': {
                'address': ip_info['address'],
                'type': ip_info['type'],  # source, destination, internal
                'geolocation': ip_info.get('geo'),
                'is_private': _is_private_ip(ip_info['address'])
            }
        }
        entities.append(ip_entity)
    
    # 4. ä½¿ç”¨è€…å¯¦é«” (User Entities)
    users = _extract_user_info(alert_source)
    for user_info in users:
        user_entity = {
            'type': 'User',
            'id': f"user_{user_info['name']}",
            'properties': {
                'username': user_info['name'],
                'user_type': user_info.get('type', 'unknown'),
                'authentication_method': user_info.get('auth_method')
            }
        }
        entities.append(user_entity)
    
    # 5. ç¨‹åºå¯¦é«” (Process Entities)
    processes = _extract_process_info(alert_source, context_data)
    for process_info in processes:
        process_entity = {
            'type': 'Process',
            'id': f"process_{process_info.get('pid', 'unknown')}_{process_info.get('name', 'unknown')}",
            'properties': {
                'process_name': process_info.get('name'),
                'process_id': process_info.get('pid'),
                'command_line': process_info.get('cmdline'),
                'parent_process': process_info.get('ppid'),
                'hash': process_info.get('hash')
            }
        }
        entities.append(process_entity)
    
    # 6. æª”æ¡ˆå¯¦é«” (File Entities)
    files = _extract_file_info(alert_source)
    for file_info in files:
        file_entity = {
            'type': 'File',
            'id': f"file_{hash(file_info['path'])}",
            'properties': {
                'file_path': file_info['path'],
                'file_name': file_info.get('name'),
                'file_size': file_info.get('size'),
                'file_hash': file_info.get('hash'),
                'file_permissions': file_info.get('permissions')
            }
        }
        entities.append(file_entity)
    
    # 7. å¨è„…å¯¦é«” (å¾žåˆ†æžçµæžœæå–)
    threat_indicators = _extract_threat_indicators(analysis_result)
    for threat in threat_indicators:
        threat_entity = {
            'type': 'ThreatIndicator',
            'id': f"threat_{uuid.uuid4().hex[:8]}",
            'properties': {
                'indicator_type': threat['type'],
                'indicator_value': threat['value'],
                'confidence': threat.get('confidence', 0.5),
                'mitre_technique': threat.get('mitre_technique')
            }
        }
        entities.append(threat_entity)
    
    logger.info(f"Extracted {len(entities)} entities: {dict(zip(*zip(*[(e['type'], 1) for e in entities])))}")
    return entities

async def execute_graph_retrieval(cypher_queries: List[Dict[str, Any]], alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    Graph-Native æª¢ç´¢å™¨ï¼šåŸ·è¡Œ Cypher æŸ¥è©¢ä¾†æª¢ç´¢ç›¸é—œçš„åœ–å½¢å­ç¶²
    é€™æ˜¯ GraphRAG çš„æ ¸å¿ƒæª¢ç´¢å¼•æ“Žï¼Œå–ä»£å‚³çµ±çš„å‘é‡èˆ‡é—œéµå­—æœå°‹
    
    Args:
        cypher_queries: å¾ž Decision Engine ç”Ÿæˆçš„ Cypher æŸ¥è©¢ä»»å‹™åˆ—è¡¨
        alert: ç•¶å‰è­¦å ±è³‡æ–™
        
    Returns:
        Dictionary åŒ…å«æª¢ç´¢åˆ°çš„åœ–å½¢å­ç¶²å’Œçµæ§‹åŒ–ä¸Šä¸‹æ–‡
    """
    from ..services.neo4j_service import get_neo4j_driver
    from ..services.metrics import api_call_duration, api_errors_total
    import asyncio
    
    logger.info(f"ðŸ”— GRAPH-NATIVE RETRIEVAL: Processing {len(cypher_queries)} Cypher queries")
    
    context_data = {
        'attack_paths': [],           # æ”»æ“Šè·¯å¾‘å­åœ–
        'lateral_movement': [],       # æ©«å‘ç§»å‹•æ¨¡å¼
        'temporal_sequences': [],     # æ™‚é–“åºåˆ—é—œè¯
        'ip_reputation': [],          # IP ä¿¡è­½åœ–
        'user_behavior': [],          # ä½¿ç”¨è€…è¡Œç‚ºåœ–
        'process_chains': [],         # ç¨‹åºåŸ·è¡Œéˆ
        'file_interactions': [],      # æª”æ¡ˆäº¤äº’åœ–
        'network_topology': [],       # ç¶²è·¯æ‹“æ’²
        'threat_landscape': [],       # å¨è„…å…¨æ™¯
        'correlation_graph': []       # ç›¸é—œæ€§åœ–
    }
    
    neo4j_driver = get_neo4j_driver()
    if not neo4j_driver:
        logger.warning("Neo4j driver not available - falling back to traditional retrieval")
        # é™ç´šåˆ°å‚³çµ±æª¢ç´¢
        return await _fallback_to_traditional_retrieval(alert)
    
    # æŽ’åºæŸ¥è©¢ä»¥å„ªåŒ–åŸ·è¡Œé †åº
    sorted_queries = sorted(cypher_queries, key=lambda x: {
        'critical': 0, 'high': 1, 'medium': 2, 'low': 3
    }.get(x.get('priority', 'medium'), 2))
    
    alert_id = alert.get('_id', 'unknown')
    
    async with neo4j_driver.session() as session:
        # æ”¹ç‚ºä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æŸ¥è©¢
        async def execute_single_query(query_spec):
            """åŸ·è¡Œå–®å€‹ Cypher æŸ¥è©¢"""
            query_name = query_spec.get('name', 'unknown')
            purpose = query_spec.get('purpose', '')
            cypher_template = query_spec.get('cypher_template', '')
            parameters = query_spec.get('parameters', {})
            parameters['alert_id'] = alert_id
            
            try:
                neo4j_start = datetime.now()
                result = await session.run(cypher_template, parameters)
                records = await result.data()
                neo4j_duration = (datetime.now() - neo4j_start).total_seconds()
                api_call_duration.labels(stage='neo4j').observe(neo4j_duration)
                
                logger.info(f"   âœ… {query_name}: {purpose} - {len(records)} results ({neo4j_duration:.3f}s)")
                return {
                    'name': query_name,
                    'records': records,
                    'duration': neo4j_duration
                }
            except Exception as e:
                api_errors_total.labels(stage='neo4j').inc()
                logger.error(f"   âŒ {query_name} failed: {str(e)}")
                return {
                    'name': query_name,
                    'records': [],
                    'error': str(e)
                }
        
        # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æŸ¥è©¢
        logger.info(f"   ðŸš€ Executing {len(sorted_queries)} Cypher queries in parallel...")
        query_results = await asyncio.gather(
            *[execute_single_query(q) for q in sorted_queries],
            return_exceptions=False
        )
        
        # è™•ç†æŸ¥è©¢çµæžœ
        for result in query_results:
            if result and not result.get('error'):
                await _categorize_graph_results(
                    result['name'], 
                    result['records'], 
                    context_data
                )
    
    # ç”Ÿæˆæª¢ç´¢æ‘˜è¦
    total_components = sum(len(results) for results in context_data.values())
    logger.info(f"ðŸ“Š GRAPH RETRIEVAL SUMMARY: {total_components} total graph components")
    for category, results in context_data.items():
        if results:
            logger.info(f"   {category}: {len(results)} components")
    
    return context_data

async def _categorize_graph_results(query_name: str, records: List[Dict], context_data: Dict[str, Any]):
    """
    æ ¹æ“šæŸ¥è©¢é¡žåž‹å°‡åœ–å½¢çµæžœåˆ†é¡žåˆ°é©ç•¶çš„ä¸Šä¸‹æ–‡é¡žåˆ¥ä¸­
    
    Args:
        query_name: æŸ¥è©¢åç¨±ï¼ˆæ”»æ“Šè·¯å¾‘ã€æ©«å‘ç§»å‹•ç­‰ï¼‰
        records: Cypher æŸ¥è©¢è¿”å›žçš„è¨˜éŒ„
        context_data: è¦æ›´æ–°çš„ä¸Šä¸‹æ–‡è³‡æ–™å­—å…¸
    """
    # æ ¹æ“šæŸ¥è©¢åç¨±æ˜ å°„åˆ°ä¸Šä¸‹æ–‡é¡žåˆ¥
    category_mapping = {
        'lateral_movement_detection': 'lateral_movement',
        'ip_activity_pattern': 'ip_reputation',
        'host_attack_timeline': 'temporal_sequences',
        'attack_path_discovery': 'attack_paths',
        'user_behavior_analysis': 'user_behavior',
        'process_chain_analysis': 'process_chains',
        'file_access_pattern': 'file_interactions',
        'network_flow_analysis': 'network_topology',
        'threat_correlation': 'correlation_graph',
        'threat_actor_profile': 'threat_landscape'
    }
    
    category = category_mapping.get(query_name, 'correlation_graph')
    
    # å°‡è¨˜éŒ„æ·»åŠ åˆ°ç›¸æ‡‰é¡žåˆ¥
    if category in context_data:
        context_data[category].extend(records)
    else:
        logger.warning(f"Unknown query category: {query_name}")

async def _fallback_to_traditional_retrieval(alert: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç•¶ Neo4j ä¸å¯ç”¨æ™‚çš„é™ç´šæ–¹æ¡ˆ
    è¿”å›žç©ºçš„åœ–å½¢ä¸Šä¸‹æ–‡ï¼Œè®“ç³»çµ±ä½¿ç”¨å‚³çµ±æª¢ç´¢æ–¹æ³•
    
    Args:
        alert: è­¦å ±è³‡æ–™
        
    Returns:
        ç©ºçš„åœ–å½¢ä¸Šä¸‹æ–‡å­—å…¸
    """
    return {
        'attack_paths': [],
        'lateral_movement': [],
        'temporal_sequences': [],
        'ip_reputation': [],
        'user_behavior': [],
        'process_chains': [],
        'file_interactions': [],
        'network_topology': [],
        'threat_landscape': [],
        'correlation_graph': []
    }

async def build_graph_relationships(entities: List[Dict[str, Any]], alert: Dict[str, Any], context_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    æ ¹æ“šå¯¦é«”å’Œä¸Šä¸‹æ–‡è³‡æ–™å»ºç«‹åœ–å½¢é—œä¿‚
    
    Args:
        entities: æå–çš„å¯¦é«”åˆ—è¡¨
        alert: åŽŸå§‹è­¦å ±è³‡æ–™
        context_data: ä¸Šä¸‹æ–‡é—œè¯è³‡æ–™
    
    Returns:
        å¯¦é«”é–“çš„é—œä¿‚åˆ—è¡¨
    """
    relationships = []
    entity_by_id = {entity['id']: entity for entity in entities}
    entity_by_type = {}
    
    # æŒ‰é¡žåž‹çµ„ç¹”å¯¦é«”
    for entity in entities:
        entity_type = entity['type']
        if entity_type not in entity_by_type:
            entity_by_type[entity_type] = []
        entity_by_type[entity_type].append(entity)
    
    # 1. è­¦å ±è§¸ç™¼é—œä¿‚ (Alert -> Host)
    alert_entities = entity_by_type.get('Alert', [])
    host_entities = entity_by_type.get('Host', [])
    
    for alert_entity in alert_entities:
        for host_entity in host_entities:
            relationship = {
                'type': 'TRIGGERED_ON',
                'source_id': alert_entity['id'],
                'target_id': host_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'severity': alert.get('_source', {}).get('rule', {}).get('level')
                }
            }
            relationships.append(relationship)
    
    # 2. ä¾†æº IP é—œä¿‚ (Alert -> IPAddress)
    ip_entities = entity_by_type.get('IPAddress', [])
    for alert_entity in alert_entities:
        for ip_entity in ip_entities:
            if ip_entity['properties'].get('type') == 'source':
                relationship = {
                    'type': 'HAS_SOURCE_IP',
                    'source_id': alert_entity['id'],
                    'target_id': ip_entity['id'],
                    'properties': {
                        'timestamp': alert.get('_source', {}).get('timestamp')
                    }
                }
                relationships.append(relationship)
    
    # 3. ä½¿ç”¨è€…åƒèˆ‡é—œä¿‚ (Alert -> User)
    user_entities = entity_by_type.get('User', [])
    for alert_entity in alert_entities:
        for user_entity in user_entities:
            relationship = {
                'type': 'INVOLVES_USER',
                'source_id': alert_entity['id'],
                'target_id': user_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'action_type': _determine_user_action_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 4. ç¨‹åºåŸ·è¡Œé—œä¿‚ (Alert -> Process)
    process_entities = entity_by_type.get('Process', [])
    for alert_entity in alert_entities:
        for process_entity in process_entities:
            relationship = {
                'type': 'INVOLVES_PROCESS',
                'source_id': alert_entity['id'],
                'target_id': process_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp')
                }
            }
            relationships.append(relationship)
    
    # 5. æª”æ¡ˆå­˜å–é—œä¿‚ (Alert -> File)
    file_entities = entity_by_type.get('File', [])
    for alert_entity in alert_entities:
        for file_entity in file_entities:
            relationship = {
                'type': 'ACCESSES_FILE',
                'source_id': alert_entity['id'],
                'target_id': file_entity['id'],
                'properties': {
                    'timestamp': alert.get('_source', {}).get('timestamp'),
                    'access_type': _determine_file_access_type(alert)
                }
            }
            relationships.append(relationship)
    
    # 6. é¡žä¼¼è­¦å ±é—œä¿‚ (åŸºæ–¼ä¸Šä¸‹æ–‡è³‡æ–™)
    similar_alerts = context_data.get('similar_alerts', [])
    for similar_alert in similar_alerts[:5]:  # é™åˆ¶é—œä¿‚æ•¸é‡
        similar_alert_id = similar_alert.get('_id')
        if similar_alert_id:
            for alert_entity in alert_entities:
                relationship = {
                    'type': 'SIMILAR_TO',
                    'source_id': alert_entity['id'],
                    'target_id': f"alert_{similar_alert_id}",  # å‡è¨­è©²è­¦å ±å·²åœ¨åœ–ä¸­
                    'properties': {
                        'similarity_score': similar_alert.get('_score', 0.0),
                        'correlation_type': 'vector_similarity'
                    }
                }
                relationships.append(relationship)
    
    # 7. æ™‚é–“åºåˆ—é—œä¿‚ (Temporal Relationships)
    # æ ¹æ“šæ™‚é–“æˆ³å»ºç«‹ PRECEDES é—œä¿‚
    if len(alert_entities) > 1:
        sorted_alerts = sorted(alert_entities, key=lambda x: x['properties'].get('timestamp', ''))
        for i in range(len(sorted_alerts) - 1):
            relationship = {
                'type': 'PRECEDES',
                'source_id': sorted_alerts[i]['id'],
                'target_id': sorted_alerts[i + 1]['id'],
                'properties': {
                    'time_difference': _calculate_time_difference(
                        sorted_alerts[i]['properties'].get('timestamp'),
                        sorted_alerts[i + 1]['properties'].get('timestamp')
                    )
                }
            }
            relationships.append(relationship)
    
    logger.info(f"Built {len(relationships)} relationships")
    return relationships

async def persist_to_graph_database(entities: List[Dict[str, Any]], relationships: List[Dict[str, Any]], alert_id: str) -> Dict[str, Any]:
    """
    å°‡å¯¦é«”å’Œé—œä¿‚æŒä¹…åŒ–åˆ° Neo4j åœ–å½¢è³‡æ–™åº«
    
    Args:
        entities: è¦å­˜å„²çš„å¯¦é«”åˆ—è¡¨
        relationships: è¦å­˜å„²çš„é—œä¿‚åˆ—è¡¨
        alert_id: è­¦å ± ID
    
    Returns:
        æŒä¹…åŒ–çµæžœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œçµ±è¨ˆè³‡è¨Š
    """
    from ..services.neo4j_service import get_neo4j_driver
    
    neo4j_driver = get_neo4j_driver()
    if not neo4j_driver:
        return {
            'success': False,
            'error': 'Neo4j driver not available',
            'nodes_created': 0,
            'relationships_created': 0
        }
    
    try:
        async with neo4j_driver.session() as session:
            # å­˜å„²ç¯€é»ž
            nodes_created = 0
            node_ids = []
            
            for entity in entities:
                # ä½¿ç”¨ MERGE ä¾†é¿å…é‡è¤‡ç¯€é»ž
                cypher_query = f"""
                MERGE (n:{entity['type']} {{id: $entity_id}})
                SET n += $properties
                RETURN n.id as node_id
                """
                
                result = await session.run(
                    cypher_query,
                    entity_id=entity['id'],
                    properties=entity['properties']
                )
                
                record = await result.single()
                if record:
                    node_ids.append(record['node_id'])
                    nodes_created += 1
            
            # å­˜å„²é—œä¿‚
            relationships_created = 0
            
            for relationship in relationships:
                # ä½¿ç”¨ MERGE ä¾†é¿å…é‡è¤‡é—œä¿‚
                cypher_query = f"""
                MATCH (source {{id: $source_id}})
                MATCH (target {{id: $target_id}})
                MERGE (source)-[r:{relationship['type']}]->(target)
                SET r += $properties
                RETURN r
                """
                
                result = await session.run(
                    cypher_query,
                    source_id=relationship['source_id'],
                    target_id=relationship['target_id'],
                    properties=relationship.get('properties', {})
                )
                
                if await result.peek():
                    relationships_created += 1
            
            # å»ºç«‹ç´¢å¼• (å¦‚æžœä¸å­˜åœ¨)
            index_queries = [
                "CREATE INDEX alert_timestamp_idx IF NOT EXISTS FOR (a:Alert) ON (a.timestamp)",
                "CREATE INDEX host_agent_id_idx IF NOT EXISTS FOR (h:Host) ON (h.agent_id)",
                "CREATE INDEX ip_address_idx IF NOT EXISTS FOR (i:IPAddress) ON (i.address)",
                "CREATE INDEX user_name_idx IF NOT EXISTS FOR (u:User) ON (u.username)"
            ]
            
            for index_query in index_queries:
                await session.run(index_query)
            
            return {
                'success': True,
                'nodes_created': nodes_created,
                'relationships_created': relationships_created,
                'node_ids': node_ids,
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Graph persistence error: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'nodes_created': 0,
            'relationships_created': 0
        }

# ==================== è¼”åŠ©å‡½æ•¸ ====================

def _determine_user_action_type(alert: Dict) -> str:
    """ç¢ºå®šä½¿ç”¨è€…å‹•ä½œé¡žåž‹"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'login' in rule_desc or 'authentication' in rule_desc:
        return 'authentication'
    elif 'ssh' in rule_desc:
        return 'remote_access'
    elif 'file' in rule_desc:
        return 'file_access'
    else:
        return 'unknown'

def _determine_file_access_type(alert: Dict) -> str:
    """ç¢ºå®šæª”æ¡ˆå­˜å–é¡žåž‹"""
    rule_desc = alert.get('_source', {}).get('rule', {}).get('description', '').lower()
    
    if 'write' in rule_desc or 'modify' in rule_desc:
        return 'write'
    elif 'read' in rule_desc:
        return 'read'
    elif 'delete' in rule_desc:
        return 'delete'
    else:
        return 'access'

def _calculate_time_difference(timestamp1: str, timestamp2: str) -> int:
    """è¨ˆç®—å…©å€‹æ™‚é–“æˆ³ä¹‹é–“çš„å·®ç•°ï¼ˆç§’ï¼‰"""
    try:
        dt1 = parser.parse(timestamp1)
        dt2 = parser.parse(timestamp2)
        return int(abs((dt2 - dt1).total_seconds()))
    except:
        return 0