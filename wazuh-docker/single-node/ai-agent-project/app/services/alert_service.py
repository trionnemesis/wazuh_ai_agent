"""
è­¦å ±æœå‹™æ¨¡çµ„
è² è²¬è­¦å ±çš„æŸ¥è©¢ã€è™•ç†å’Œåˆ†ææµç¨‹
"""

import logging
import traceback
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import time

from ..services.opensearch_service import get_opensearch_client
from ..services.decision_service import determine_contextual_queries, determine_graph_queries
from ..services.retrieval_service import execute_retrieval, execute_hybrid_retrieval
from ..services.graph_service import extract_graph_entities, build_graph_relationships, persist_to_graph_database
from ..services.llm_service import get_analysis_chain
from ..services.metrics import (
    new_alerts_found_total, pending_alerts_gauge, alerts_processed_total,
    alert_processing_errors_total, record_processing_time, update_pending_alerts
)
from ..embedding_service import GeminiEmbeddingService

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–åµŒå…¥æœå‹™
embedding_service = GeminiEmbeddingService()

async def query_new_alerts(limit: int = 10) -> List[Dict[str, Any]]:
    """
    æŸ¥è©¢æ–°çš„ã€å°šæœªåˆ†æçš„è­¦å ±
    
    Args:
        limit: è¦æŸ¥è©¢çš„æœ€å¤§è­¦å ±æ•¸é‡
        
    Returns:
        List[Dict]: æ–°è­¦å ±åˆ—è¡¨
    """
    client = get_opensearch_client()
    
    query = {
        "query": {
            "bool": {
                "must_not": [
                    {"exists": {"field": "ai_analysis"}}
                ],
                "must": [
                    {"range": {"timestamp": {"gte": "now-24h"}}}
                ]
            }
        },
        "sort": [{"timestamp": {"order": "desc"}}],
        "size": limit
    }
    
    try:
        response = await client.search(index="wazuh-alerts-*", body=query)
        alerts = response['hits']['hits']
        logger.info(f"æŸ¥è©¢åˆ° {len(alerts)} å€‹æ–°è­¦å ±å¾…è™•ç†")
        return alerts
    except Exception as e:
        logger.error(f"æŸ¥è©¢æ–°è­¦å ±å¤±æ•—: {str(e)}")
        return []

async def triage_new_alerts():
    """ä¸»è¦çš„è­¦å ±åˆ†æµä»»å‹™ï¼Œä½¿ç”¨ Stage 4 GraphRAG é€²è¡Œåˆ†æ"""
    print("ğŸš€ === STAGE 4 GRAPHRAG ALERT TRIAGE JOB EXECUTING ===")
    logger.info("ğŸ”¬ Analyzing alerts with GraphRAG and enhanced threat intelligence...")
    
    try:
        # æŸ¥è©¢æ–°è­¦å ±
        alerts = await query_new_alerts(limit=10)
        
        # Prometheus ç›£æ§ - è¨˜éŒ„ç™¼ç¾çš„æ–°è­¦å ±æ•¸é‡
        new_alerts_found_total.inc(len(alerts))
        
        if not alerts:
            print("ğŸ“­ --- No new alerts found ---")
            logger.info("No new alerts requiring analysis")
            # è¨­ç½®å¾…è™•ç†è­¦å ±æ•¸é‡ç‚º 0
            update_pending_alerts(0)
            return
            
        logger.info(f"ğŸ¯ Found {len(alerts)} new alerts to process with GraphRAG")
        
        # è¨­ç½®å¾…è™•ç†è­¦å ±æ•¸é‡
        update_pending_alerts(len(alerts))
        
        # è™•ç†æ¯å€‹è­¦å ±
        successful_processing = 0
        failed_processing = 0
        
        for i, alert in enumerate(alerts, 1):
            alert_id = alert['_id']
            rule_desc = alert.get('_source', {}).get('rule', {}).get('description', 'Unknown')
            
            try:
                logger.info(f"ğŸ”„ [{i}/{len(alerts)}] Processing alert: {alert_id}")
                logger.info(f"   Rule: {rule_desc}")
                
                await process_single_alert(alert)
                
                successful_processing += 1
                alerts_processed_total.inc()
                print(f"âœ… [{i}/{len(alerts)}] Successfully processed alert {alert_id}")
                logger.info(f"âœ… Alert {alert_id} processing completed successfully")
                
            except Exception as e:
                failed_processing += 1
                alert_processing_errors_total.inc()
                print(f"âŒ [{i}/{len(alerts)}] Failed to process alert {alert_id}: {str(e)}")
                logger.error(f"âŒ Alert {alert_id} processing failed: {str(e)}")
                continue
        
        # ç¸½çµæ—¥èªŒ
        print(f"ğŸ“Š === GRAPHRAG TRIAGE BATCH SUMMARY ===")
        print(f"   âœ… Successful: {successful_processing}")
        print(f"   âŒ Failed: {failed_processing}")
        print(f"   ğŸ“ˆ Success Rate: {(successful_processing/len(alerts)*100):.1f}%")
        
        logger.info(f"ğŸ¯ GraphRAG triage batch completed: {successful_processing}/{len(alerts)} successful")
            
    except Exception as e:
        print(f"ğŸ’¥ !!! CRITICAL ERROR IN GRAPHRAG TRIAGE JOB !!!")
        logger.error(f"Critical error during GraphRAG triage: {e}", exc_info=True)
        traceback.print_exc()

async def process_single_alert(alert: Dict[str, Any]) -> None:
    """
    è™•ç†å–®å€‹è­¦å ±çš„å®Œæ•´æµç¨‹
    
    Args:
        alert: è­¦å ±è³‡æ–™
    """
    start_time = time.time()
    alert_id = alert['_id']
    source = alert['_source']
    
    try:
        # Step 1: å‘é‡åŒ–è­¦å ±
        logger.info(f"Step 1: Vectorizing alert {alert_id}")
        alert_vector = await embedding_service.embed_alert(source)
        
        if not alert_vector:
            raise ValueError("Failed to vectorize alert")
        
        # Step 2: æ±ºå®šæŸ¥è©¢ç­–ç•¥ (Agentic)
        logger.info(f"Step 2: Determining contextual queries")
        queries = await determine_contextual_queries(source)
        
        # Step 3: åŸ·è¡Œæ··åˆæª¢ç´¢ (GraphRAG + Traditional)
        logger.info(f"Step 3: Executing hybrid retrieval")
        context_data = await execute_hybrid_retrieval(source)
        
        # Step 4: LLM åˆ†æ
        logger.info(f"Step 4: Performing LLM analysis")
        analysis_chain = get_analysis_chain()
        analysis_result = await analysis_chain.ainvoke({
            "alert_summary": _create_alert_summary(source),
            **context_data
        })
        
        # Step 5: æå–åœ–å½¢å¯¦é«”å’Œé—œä¿‚
        logger.info(f"Step 5: Extracting graph entities")
        entities = await extract_graph_entities(source, context_data, analysis_result)
        relationships = await build_graph_relationships(entities, source, context_data)
        
        # Step 6: æŒä¹…åŒ–åˆ°åœ–å½¢è³‡æ–™åº«
        logger.info(f"Step 6: Persisting to graph database")
        graph_result = await persist_to_graph_database(entities, relationships, alert_id)
        
        # Step 7: æ›´æ–°è­¦å ±è¨˜éŒ„
        logger.info(f"Step 7: Updating alert record")
        await update_alert_with_analysis(alert_id, {
            "vector": alert_vector,
            "analysis": analysis_result,
            "graph_entities": len(entities),
            "graph_relationships": len(relationships),
            "timestamp": datetime.utcnow().isoformat()
        })
        
        # è¨˜éŒ„è™•ç†æ™‚é–“
        processing_time = time.time() - start_time
        record_processing_time(processing_time)
        
        logger.info(f"âœ… Alert {alert_id} processed successfully in {processing_time:.2f}s")
        
    except Exception as e:
        logger.error(f"Error processing alert {alert_id}: {str(e)}")
        raise

async def update_alert_with_analysis(alert_id: str, analysis_data: Dict[str, Any]) -> None:
    """
    æ›´æ–°è­¦å ±è¨˜éŒ„ï¼Œæ·»åŠ åˆ†æçµæœ
    
    Args:
        alert_id: è­¦å ± ID
        analysis_data: åˆ†æçµæœæ•¸æ“š
    """
    client = get_opensearch_client()
    
    try:
        # é¦–å…ˆç²å–è­¦å ±ç´¢å¼•
        get_response = await client.get(index="wazuh-alerts-*", id=alert_id)
        alert_index = get_response['_index']
        
        # æ›´æ–°æ–‡æª”
        update_body = {
            "doc": {
                "ai_analysis": analysis_data['analysis'],
                "alert_vector": analysis_data['vector'],
                "graph_metadata": {
                    "entities_count": analysis_data['graph_entities'],
                    "relationships_count": analysis_data['graph_relationships'],
                    "processed_at": analysis_data['timestamp']
                }
            }
        }
        
        await client.update(index=alert_index, id=alert_id, body=update_body)
        logger.info(f"Successfully updated alert {alert_id} with analysis results")
        
    except Exception as e:
        logger.error(f"Failed to update alert {alert_id}: {str(e)}")
        raise

def _create_alert_summary(alert: Dict[str, Any]) -> str:
    """å‰µå»ºè­¦å ±æ‘˜è¦"""
    return f"""
    Rule: {alert.get('rule', {}).get('description', 'N/A')}
    Level: {alert.get('rule', {}).get('level', 'N/A')}
    Agent: {alert.get('agent', {}).get('name', 'N/A')}
    Source IP: {alert.get('data', {}).get('srcip', 'N/A')}
    Destination IP: {alert.get('data', {}).get('dstip', 'N/A')}
    Event Type: {alert.get('decoder', {}).get('name', 'N/A')}
    Process: {alert.get('data', {}).get('process', 'N/A')}
    User: {alert.get('data', {}).get('srcuser', 'N/A')}
    Timestamp: {alert.get('timestamp', 'N/A')}
    """